{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
<<<<<<< HEAD
      "gpuType": "T4"
=======
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4PJR7yrnFc6sG1FKCtfhh",
      "include_colab_link": true
>>>>>>> f153d061e974b64e38e550b6133541d3a746d83a
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
<<<<<<< HEAD
=======
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b-akash-krishna/trance-project/blob/main/trance_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
>>>>>>> f153d061e974b64e38e550b6133541d3a746d83a
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Setup & Configuration\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Create project structure\n",
        "def create_project():\n",
        "    dirs = ['data/raw', 'data/processed', 'data/embeddings', 'outputs/models',\n",
        "            'outputs/figures', 'outputs/results', 'configs']\n",
        "    for d in dirs:\n",
        "        Path(d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    config = {\n",
        "        \"project\": {\"name\": \"TRANCE\", \"version\": \"1.0.0\"},\n",
        "        \"data\": {\n",
        "            \"mimic_demo_url\": \"https://physionet.org/files/mimiciii-demo/1.4/\",\n",
        "            \"cohort\": {\"min_age\": 18, \"min_los_hours\": 24, \"readmission_window_days\": 30}\n",
        "        },\n",
        "        \"model\": {\n",
        "            \"embedding_model\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
        "            \"embedding_dim\": 768,\n",
        "            \"max_text_length\": 512,\n",
        "            \"chunk_overlap\": 128\n",
        "        },\n",
        "        \"training\": {\n",
        "            \"test_size\": 0.2,\n",
        "            \"calibration_size\": 0.1,\n",
        "            \"random_state\": 42,\n",
        "            \"lgbm_params\": {\n",
        "                \"objective\": \"binary\", \"metric\": \"auc\", \"boosting_type\": \"gbdt\",\n",
        "                \"num_leaves\": 7, \"max_depth\": 3, \"learning_rate\": 0.01,\n",
        "                \"n_estimators\": 200, \"min_child_samples\": 10,\n",
        "                \"reg_alpha\": 0.5, \"reg_lambda\": 0.5, \"feature_fraction\": 0.8,\n",
        "                \"bagging_fraction\": 0.8, \"bagging_freq\": 5, \"verbose\": -1\n",
        "            }\n",
        "        },\n",
        "        \"paths\": {\n",
        "            \"raw_data\": \"data/raw\",\n",
        "            \"processed_data\": \"data/processed\",\n",
        "            \"embeddings\": \"data/embeddings\",\n",
        "            \"models\": \"outputs/models\",\n",
        "            \"figures\": \"outputs/figures\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open('configs/config.json', 'w') as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    return config\n",
        "\n",
        "config = create_project()\n",
        "print(\"‚úì Project initialized\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V87WHSnG6DCN",
        "outputId": "68ec6169-f6dc-47fd-f04b-6deaab7c418a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Project initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Download MIMIC-III Demo Data\n",
        "# ============================================================================\n",
        "def download_mimic_data():\n",
        "    base_url = \"https://physionet.org/files/mimiciii-demo/1.4/\"\n",
        "    tables = ['ADMISSIONS.csv', 'PATIENTS.csv', 'DIAGNOSES_ICD.csv',\n",
        "              'PROCEDURES_ICD.csv', 'PRESCRIPTIONS.csv', 'LABEVENTS.csv',\n",
        "              'NOTEEVENTS.csv', 'ICUSTAYS.csv', 'D_ICD_DIAGNOSES.csv',\n",
        "              'D_ICD_PROCEDURES.csv', 'D_LABITEMS.csv']\n",
        "\n",
        "    data_dir = Path('data/raw')\n",
        "\n",
        "    for table in tqdm(tables, desc=\"Downloading\"):\n",
        "        dest = data_dir / table\n",
        "        if dest.exists():\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            response = requests.get(base_url + table, stream=True)\n",
        "            if response.status_code == 200:\n",
        "                with open(dest, 'wb') as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {table}: {e}\")\n",
        "\n",
        "    print(\"‚úì Data downloaded\")\n",
        "\n",
        "download_mimic_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7yDZZhx6Hr6",
        "outputId": "00b19ef4-9773-461a-bcd9-138d255ce6ab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 19641.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Data downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Load & Preprocess Core Tables\n",
        "# ============================================================================\n",
        "data_dir = Path('data/raw')\n",
        "\n",
        "# Load with uppercase columns\n",
        "def load_table(filename):\n",
        "    df = pd.read_csv(data_dir / filename)\n",
        "    df.columns = df.columns.str.upper()\n",
        "    return df\n",
        "\n",
        "admissions = load_table('ADMISSIONS.csv')\n",
        "patients = load_table('PATIENTS.csv')\n",
        "diagnoses = load_table('DIAGNOSES_ICD.csv')\n",
        "procedures = load_table('PROCEDURES_ICD.csv')\n",
        "prescriptions = load_table('PRESCRIPTIONS.csv')\n",
        "labevents = load_table('LABEVENTS.csv')\n",
        "noteevents = load_table('NOTEEVENTS.csv')\n",
        "icustays = load_table('ICUSTAYS.csv')\n",
        "d_icd_diagnoses = load_table('D_ICD_DIAGNOSES.csv')\n",
        "d_labitems = load_table('D_LABITEMS.csv')\n",
        "\n",
        "# Convert dates\n",
        "for col in ['ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'EDREGTIME', 'EDOUTTIME']:\n",
        "    if col in admissions.columns:\n",
        "        admissions[col] = pd.to_datetime(admissions[col], errors='coerce')\n",
        "\n",
        "patients['DOB'] = pd.to_datetime(patients['DOB'], errors='coerce')\n",
        "patients['DOD'] = pd.to_datetime(patients['DOD'], errors='coerce')\n",
        "\n",
        "print(f\"‚úì Loaded: {len(admissions)} admissions, {len(patients)} patients\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMfrDYJk6Kw3",
        "outputId": "cda842b1-ef99-441c-e460-4071867e12bc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Loaded: 129 admissions, 100 patients\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Generate Discharge Summaries for Missing Notes\n",
        "# ============================================================================\n",
        "def generate_discharge_text(hadm_id, admissions_patients, diagnoses, procedures, prescriptions):\n",
        "    row = admissions_patients[admissions_patients['HADM_ID'] == hadm_id].iloc[0]\n",
        "\n",
        "    age = row.get('AGE', 'unknown')\n",
        "    gender = 'male' if row.get('GENDER') == 'M' else 'female'\n",
        "    admit_date = row.get('ADMITTIME').strftime('%Y-%m-%d') if pd.notna(row.get('ADMITTIME')) else 'unknown'\n",
        "    los = round(row.get('LOS_DAYS', 0), 1)\n",
        "\n",
        "    diag_codes = diagnoses[diagnoses['HADM_ID'] == hadm_id]['ICD9_CODE'].astype(str).tolist()\n",
        "    diag_text = \", \".join(diag_codes[:10]) if diag_codes else \"No diagnoses\"\n",
        "\n",
        "    proc_codes = procedures[procedures['HADM_ID'] == hadm_id]['ICD9_CODE'].astype(str).tolist()\n",
        "    proc_text = \", \".join(proc_codes[:5]) if proc_codes else \"No procedures\"\n",
        "\n",
        "    rx = prescriptions[prescriptions['HADM_ID'] == hadm_id]\n",
        "    rx_list = [f\"{r.get('DRUG', 'Unknown')} {r.get('DOSE_VAL_RX', '')} {r.get('DOSE_UNIT_RX', '')}\"\n",
        "               for _, r in rx.head(5).iterrows()]\n",
        "    rx_text = \"; \".join(rx_list) if rx_list else \"No medications\"\n",
        "\n",
        "    return (f\"Patient: {age}yo {gender}, admitted {admit_date}, {los}d stay. \"\n",
        "            f\"Diagnoses: {diag_text}. Procedures: {proc_text}. \"\n",
        "            f\"Medications: {rx_text}. Discharged to {row.get('DISCHARGE_LOCATION', 'home')}.\")\n",
        "\n",
        "# Merge and calculate features\n",
        "admissions_patients = admissions.merge(patients[['SUBJECT_ID', 'DOB', 'DOD', 'GENDER']], on='SUBJECT_ID')\n",
        "admissions_patients['AGE'] = admissions_patients['ADMITTIME'].dt.year - admissions_patients['DOB'].dt.year\n",
        "admissions_patients['LOS_DAYS'] = (admissions_patients['DISCHTIME'] - admissions_patients['ADMITTIME']).dt.total_seconds() / 86400\n",
        "\n",
        "# Update noteevents\n",
        "for idx, row in admissions_patients.iterrows():\n",
        "    hadm_id = row['HADM_ID']\n",
        "    existing = noteevents[noteevents['HADM_ID'] == hadm_id]\n",
        "\n",
        "    if len(existing) == 0 or existing['TEXT'].isna().all():\n",
        "        text = generate_discharge_text(hadm_id, admissions_patients, diagnoses, procedures, prescriptions)\n",
        "\n",
        "        if len(existing) == 0:\n",
        "            new_row = {\n",
        "                'ROW_ID': noteevents['ROW_ID'].max() + 1 if len(noteevents) > 0 else 1,\n",
        "                'SUBJECT_ID': row['SUBJECT_ID'],\n",
        "                'HADM_ID': hadm_id,\n",
        "                'CHARTDATE': row['ADMITTIME'].strftime('%Y-%m-%d') if pd.notna(row['ADMITTIME']) else '',\n",
        "                'CATEGORY': 'Discharge summary',\n",
        "                'TEXT': text\n",
        "            }\n",
        "            noteevents = pd.concat([noteevents, pd.DataFrame([new_row])], ignore_index=True)\n",
        "        else:\n",
        "            noteevents.loc[noteevents['HADM_ID'] == hadm_id, 'TEXT'] = text\n",
        "            noteevents.loc[noteevents['HADM_ID'] == hadm_id, 'CATEGORY'] = 'Discharge summary'\n",
        "\n",
        "noteevents.to_csv(data_dir / 'NOTEEVENTS_with_discharge_text.csv', index=False)\n",
        "print(f\"‚úì Generated discharge summaries: {len(noteevents)} notes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRdvIVDB6QU8",
        "outputId": "b992ee0a-4cdb-4269-e898-e1de566a5748"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Generated discharge summaries: 129 notes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Define Cohort & Create Readmission Labels\n",
        "# ============================================================================\n",
        "cohort = admissions_patients.copy()\n",
        "\n",
        "# Inclusion criteria\n",
        "cohort = cohort[\n",
        "    (cohort['AGE'] >= 18) &\n",
        "    (cohort['LOS_DAYS'] >= 1) &\n",
        "    (cohort['ADMISSION_TYPE'].isin(['EMERGENCY', 'URGENT'])) &\n",
        "    (cohort['DISCHTIME'].notna())\n",
        "]\n",
        "\n",
        "# Exclusion: in-hospital mortality\n",
        "cohort['HOSPITAL_MORTALITY'] = cohort['HOSPITAL_EXPIRE_FLAG'] == 1\n",
        "cohort = cohort[~cohort['HOSPITAL_MORTALITY']]\n",
        "\n",
        "# Sort and calculate readmission\n",
        "cohort = cohort.sort_values(['SUBJECT_ID', 'ADMITTIME']).reset_index(drop=True)\n",
        "cohort['NEXT_ADMITTIME'] = cohort.groupby('SUBJECT_ID')['ADMITTIME'].shift(-1)\n",
        "cohort['DAYS_TO_NEXT_ADMIT'] = (cohort['NEXT_ADMITTIME'] - cohort['DISCHTIME']).dt.total_seconds() / 86400\n",
        "cohort['TARGET_READMIT_30'] = (cohort['DAYS_TO_NEXT_ADMIT'] <= 30) & (cohort['DAYS_TO_NEXT_ADMIT'] > 0)\n",
        "cohort['IS_LAST_ADMISSION'] = cohort['NEXT_ADMITTIME'].isna()\n",
        "\n",
        "# Add discharge notes\n",
        "discharge_notes = noteevents[noteevents['CATEGORY'] == 'Discharge summary'][['HADM_ID', 'TEXT']]\n",
        "discharge_notes.columns = ['HADM_ID', 'DISCHARGE_TEXT']\n",
        "cohort = cohort.merge(discharge_notes, on='HADM_ID', how='left')\n",
        "cohort['HAS_DISCHARGE_NOTE'] = cohort['DISCHARGE_TEXT'].notna()\n",
        "\n",
        "readmit_rate = cohort['TARGET_READMIT_30'].mean()\n",
        "print(f\"‚úì Cohort: {len(cohort)} admissions, {readmit_rate:.2%} readmission rate\")\n",
        "\n",
        "# Save\n",
        "cohort_cols = ['HADM_ID', 'SUBJECT_ID', 'ADMITTIME', 'DISCHTIME', 'ADMISSION_TYPE',\n",
        "               'ADMISSION_LOCATION', 'DISCHARGE_LOCATION', 'INSURANCE', 'ETHNICITY',\n",
        "               'AGE', 'GENDER', 'LOS_DAYS', 'TARGET_READMIT_30', 'DAYS_TO_NEXT_ADMIT',\n",
        "               'IS_LAST_ADMISSION', 'HAS_DISCHARGE_NOTE', 'DISCHARGE_TEXT']\n",
        "cohort[cohort_cols].to_parquet('data/processed/cohort_with_outcomes.parquet', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0OlHN_q6Tr4",
        "outputId": "d0c98d8b-c955-4946-94cb-21ae509e83f3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Cohort: 79 admissions, 7.59% readmission rate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Comprehensive Feature Engineering\n",
        "# ============================================================================\n",
        "features = cohort[['HADM_ID', 'SUBJECT_ID']].copy()\n",
        "\n",
        "# Demographics\n",
        "features['age'] = cohort['AGE']\n",
        "features['gender_M'] = (cohort['GENDER'] == 'M').astype(int)\n",
        "\n",
        "def categorize_ethnicity(eth):\n",
        "    if pd.isna(eth): return 'UNKNOWN'\n",
        "    eth = eth.upper()\n",
        "    if 'WHITE' in eth: return 'WHITE'\n",
        "    if 'BLACK' in eth or 'AFRICAN' in eth: return 'BLACK'\n",
        "    if 'HISPANIC' in eth or 'LATINO' in eth: return 'HISPANIC'\n",
        "    if 'ASIAN' in eth: return 'ASIAN'\n",
        "    return 'OTHER'\n",
        "\n",
        "features['ethnicity_category'] = cohort['ETHNICITY'].apply(categorize_ethnicity)\n",
        "features = pd.concat([features, pd.get_dummies(features['ethnicity_category'], prefix='ethnicity')], axis=1)\n",
        "features = pd.concat([features, pd.get_dummies(cohort['INSURANCE'], prefix='insurance')], axis=1)\n",
        "\n",
        "# Admission features\n",
        "features['los_days'] = cohort['LOS_DAYS']\n",
        "features['admit_weekend'] = (pd.to_datetime(cohort['ADMITTIME']).dt.dayofweek >= 5).astype(int)\n",
        "features['discharge_weekend'] = (pd.to_datetime(cohort['DISCHTIME']).dt.dayofweek >= 5).astype(int)\n",
        "\n",
        "# Diagnoses & comorbidities\n",
        "diag_counts = diagnoses.groupby('HADM_ID').size().reset_index(name='n_diagnoses')\n",
        "features = features.merge(diag_counts, on='HADM_ID', how='left').fillna({'n_diagnoses': 0})\n",
        "\n",
        "def get_charlson(hadm_id):\n",
        "    codes = diagnoses[diagnoses['HADM_ID'] == hadm_id]['ICD9_CODE'].astype(str).tolist()\n",
        "    score = 0\n",
        "    conditions = {'MI': ['410', '412'], 'CHF': ['428'], 'CVD': ['43'], 'COPD': ['49', '50'],\n",
        "                  'Diabetes': ['250'], 'Renal': ['58'], 'Liver': ['571'], 'Cancer': ['14', '15', '16', '17', '18', '19', '20']}\n",
        "    for cond_codes in conditions.values():\n",
        "        if any(any(c.startswith(code) for code in cond_codes) for c in codes):\n",
        "            score += 1\n",
        "    return score\n",
        "\n",
        "features['charlson_score'] = features['HADM_ID'].apply(get_charlson)\n",
        "\n",
        "# High-risk diagnoses\n",
        "def has_dx(hadm_id, prefixes):\n",
        "    codes = diagnoses[diagnoses['HADM_ID'] == hadm_id]['ICD9_CODE'].astype(str).tolist()\n",
        "    return int(any(any(c.startswith(p) for p in prefixes) for c in codes))\n",
        "\n",
        "features['dx_heart_failure'] = features['HADM_ID'].apply(lambda x: has_dx(x, ['428']))\n",
        "features['dx_copd'] = features['HADM_ID'].apply(lambda x: has_dx(x, ['49']))\n",
        "features['dx_diabetes'] = features['HADM_ID'].apply(lambda x: has_dx(x, ['250']))\n",
        "features['dx_renal_failure'] = features['HADM_ID'].apply(lambda x: has_dx(x, ['584', '585']))\n",
        "\n",
        "# Procedures\n",
        "proc_counts = procedures.groupby('HADM_ID').size().reset_index(name='n_procedures')\n",
        "features = features.merge(proc_counts, on='HADM_ID', how='left').fillna({'n_procedures': 0})\n",
        "\n",
        "# ICU\n",
        "icustays['INTIME'] = pd.to_datetime(icustays['INTIME'], errors='coerce')\n",
        "icustays['OUTTIME'] = pd.to_datetime(icustays['OUTTIME'], errors='coerce')\n",
        "icustays['ICU_LOS'] = (icustays['OUTTIME'] - icustays['INTIME']).dt.total_seconds() / 86400\n",
        "\n",
        "icu_agg = icustays.groupby('HADM_ID').agg({'ICUSTAY_ID': 'count', 'ICU_LOS': 'sum'}).reset_index()\n",
        "icu_agg.columns = ['HADM_ID', 'n_icu_stays', 'total_icu_days']\n",
        "features = features.merge(icu_agg, on='HADM_ID', how='left').fillna({'n_icu_stays': 0, 'total_icu_days': 0})\n",
        "features['had_icu_stay'] = (features['n_icu_stays'] > 0).astype(int)\n",
        "\n",
        "# Medications\n",
        "med_counts = prescriptions.groupby('HADM_ID')['DRUG'].nunique().reset_index(name='n_medications')\n",
        "features = features.merge(med_counts, on='HADM_ID', how='left').fillna({'n_medications': 0})\n",
        "\n",
        "# Labs (last values before discharge)\n",
        "labevents['CHARTTIME'] = pd.to_datetime(labevents['CHARTTIME'], errors='coerce')\n",
        "labs_with_disch = labevents.merge(cohort[['HADM_ID', 'DISCHTIME']], on='HADM_ID')\n",
        "labs_with_disch = labs_with_disch[labs_with_disch['CHARTTIME'] <= labs_with_disch['DISCHTIME']]\n",
        "labs_with_disch = labs_with_disch.merge(d_labitems[['ITEMID', 'LABEL']], on='ITEMID', how='left')\n",
        "\n",
        "key_labs = {'Creatinine': ['CREATININE'], 'Hemoglobin': ['HEMOGLOBIN', 'HGB'],\n",
        "            'WBC': ['WBC'], 'Sodium': ['SODIUM'], 'Glucose': ['GLUCOSE']}\n",
        "\n",
        "for lab_name, keywords in key_labs.items():\n",
        "    lab_data = labs_with_disch[labs_with_disch['LABEL'].str.upper().str.contains('|'.join(keywords), na=False)]\n",
        "    last_vals = lab_data.sort_values('CHARTTIME').groupby('HADM_ID').last()['VALUENUM'].reset_index()\n",
        "    last_vals.columns = ['HADM_ID', f'lab_{lab_name.lower()}_last']\n",
        "    features = features.merge(last_vals, on='HADM_ID', how='left')\n",
        "\n",
        "# Prior admissions\n",
        "def count_prior(row, window_days):\n",
        "    return len(admissions[\n",
        "        (admissions['SUBJECT_ID'] == row['SUBJECT_ID']) &\n",
        "        (admissions['DISCHTIME'] < row['ADMITTIME']) &\n",
        "        (admissions['DISCHTIME'] >= row['ADMITTIME'] - pd.Timedelta(days=window_days))\n",
        "    ])\n",
        "\n",
        "features['prior_admissions_180d'] = cohort.apply(lambda r: count_prior(r, 180), axis=1)\n",
        "features['frequent_flyer'] = (features['prior_admissions_180d'] >= 3).astype(int)\n",
        "\n",
        "# Handle missing labs\n",
        "lab_cols = [c for c in features.columns if c.startswith('lab_')]\n",
        "for col in lab_cols:\n",
        "    features[f'{col}_missing'] = features[col].isna().astype(int)\n",
        "    features[col] = features[col].fillna(features[col].median() if features[col].notna().sum() > 0 else 0)\n",
        "\n",
        "# Drop categorical intermediates\n",
        "features.drop(['ethnicity_category'], axis=1, errors='ignore', inplace=True)\n",
        "\n",
        "# Add target\n",
        "features['target_readmit_30'] = cohort['TARGET_READMIT_30'].astype(int)\n",
        "features['is_last_admission'] = cohort['IS_LAST_ADMISSION'].astype(int)\n",
        "features['has_discharge_note'] = cohort['HAS_DISCHARGE_NOTE'].astype(int)\n",
        "\n",
        "# Ensure numeric\n",
        "metadata_cols = ['HADM_ID', 'SUBJECT_ID', 'target_readmit_30', 'is_last_admission', 'has_discharge_note']\n",
        "feature_cols = [c for c in features.columns if c not in metadata_cols]\n",
        "\n",
        "for col in feature_cols:\n",
        "    features[col] = pd.to_numeric(features[col], errors='coerce').fillna(0).astype(np.float64)\n",
        "\n",
        "print(f\"‚úì Engineered {len(feature_cols)} features\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sy1cpExe6Xah",
        "outputId": "0b1d59cf-5f2a-4df4-9dc6-5ba9c55fcdba"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Engineered 36 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Create Train/Calib/Test Splits (Temporal)\n",
        "# ============================================================================\n",
        "features_with_time = features.merge(cohort[['HADM_ID', 'DISCHTIME']], on='HADM_ID')\n",
        "features_with_time = features_with_time.sort_values('DISCHTIME')\n",
        "\n",
        "evaluable = features_with_time[features_with_time['is_last_admission'] == 0].copy()\n",
        "\n",
        "n_total = len(evaluable)\n",
        "n_train = int(n_total * 0.7)\n",
        "n_calib = int(n_total * 0.1)\n",
        "\n",
        "train_data = evaluable.iloc[:n_train].drop('DISCHTIME', axis=1)\n",
        "calib_data = evaluable.iloc[n_train:n_train+n_calib].drop('DISCHTIME', axis=1)\n",
        "test_data = evaluable.iloc[n_train+n_calib:].drop('DISCHTIME', axis=1)\n",
        "\n",
        "train_data.to_parquet('data/processed/train_features.parquet', index=False)\n",
        "calib_data.to_parquet('data/processed/calibration_features.parquet', index=False)\n",
        "test_data.to_parquet('data/processed/test_features.parquet', index=False)\n",
        "\n",
        "with open('data/processed/feature_info.json', 'w') as f:\n",
        "    json.dump({'feature_names': feature_cols, 'n_features': len(feature_cols)}, f, indent=2)\n",
        "\n",
        "print(f\"‚úì Splits: Train={len(train_data)}, Calib={len(calib_data)}, Test={len(test_data)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O27sNxx6cnw",
        "outputId": "c4604119-da75-4e3c-ef4f-34bafed0b0b9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Splits: Train=14, Calib=2, Test=5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Generate Clinical Text Embeddings\n",
        "# ============================================================================\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "model = AutoModel.from_pretrained(model_name).to(device).eval()\n",
        "\n",
        "cohort_with_notes = cohort[cohort['HAS_DISCHARGE_NOTE'] & cohort['DISCHARGE_TEXT'].notna()].copy()\n",
        "cohort_with_notes['PROCESSED_TEXT'] = cohort_with_notes['DISCHARGE_TEXT'].str.lower().str.strip()\n",
        "cohort_with_notes = cohort_with_notes[cohort_with_notes['PROCESSED_TEXT'].str.len() >= 5]\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_embeddings(texts, batch_size=16, max_length=512):\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "        encoded = tokenizer(\n",
        "            batch_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_tensors='pt'\n",
        "        ).to(device)\n",
        "\n",
        "        outputs = model(**encoded)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        all_embeddings.append(embeddings)\n",
        "\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "texts = cohort_with_notes['PROCESSED_TEXT'].tolist()\n",
        "embeddings = generate_embeddings(texts)\n",
        "\n",
        "# Save\n",
        "import h5py\n",
        "with h5py.File('data/embeddings/discharge_note_embeddings.h5', 'w') as f:\n",
        "    f.create_dataset('embeddings', data=embeddings, compression='gzip')\n",
        "\n",
        "    # Convert HADM_IDs safely for HDF5 (works for both numeric and string IDs)\n",
        "    hadm_ids = cohort_with_notes['HADM_ID'].astype(str).values.astype('S')\n",
        "    f.create_dataset('hadm_ids', data=hadm_ids)\n",
        "\n",
        "emb_df = pd.DataFrame(embeddings, columns=[f'emb_{i}' for i in range(embeddings.shape[1])])\n",
        "emb_df['HADM_ID'] = cohort_with_notes['HADM_ID'].values\n",
        "emb_df.to_parquet('data/embeddings/embeddings.parquet', index=False)\n",
        "\n",
        "print(f\"‚úì Generated embeddings: {embeddings.shape}\")\n",
        "\n",
        "# Cleanup GPU\n",
        "if torch.cuda.is_available():\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iT05OM26gS9",
        "outputId": "61455776-3706-49f7-c065-8632f1813ac0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  6.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Generated embeddings: (79, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Create Fused Features (Structured + Embeddings)\n",
        "# ============================================================================\n",
        "train_struct = pd.read_parquet('data/processed/train_features.parquet')\n",
        "calib_struct = pd.read_parquet('data/processed/calibration_features.parquet')\n",
        "test_struct = pd.read_parquet('data/processed/test_features.parquet')\n",
        "embeddings_df = pd.read_parquet('data/embeddings/embeddings.parquet')\n",
        "\n",
        "train_fused = train_struct.merge(embeddings_df, on='HADM_ID', how='left')\n",
        "calib_fused = calib_struct.merge(embeddings_df, on='HADM_ID', how='left')\n",
        "test_fused = test_struct.merge(embeddings_df, on='HADM_ID', how='left')\n",
        "\n",
        "# Fill missing embeddings with zeros\n",
        "embedding_cols = [c for c in embeddings_df.columns if c.startswith('emb_')]\n",
        "for df in [train_fused, calib_fused, test_fused]:\n",
        "    df[embedding_cols] = df[embedding_cols].fillna(0)\n",
        "\n",
        "train_fused.to_parquet('data/processed/train_fused.parquet', index=False)\n",
        "calib_fused.to_parquet('data/processed/calibration_fused.parquet', index=False)\n",
        "test_fused.to_parquet('data/processed/test_fused.parquet', index=False)\n",
        "\n",
        "print(f\"‚úì Fused features: {train_fused.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2mQAKjG6odr",
        "outputId": "1a368486-3d97-49ce-921f-53a5e2ea367c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Fused features: (14, 809)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO2wd0lkr_3i",
        "outputId": "66e462aa-ac7b-4bf3-d53f-e76fdd44323e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Augmented to 18 samples\n",
            "\n",
            "Training baseline model...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's auc: 1\n",
            "‚úì Baseline: AUROC=0.5000, AUPRC=0.2000\n",
            "\n",
            "Training fused model...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's auc: 1\n",
            "‚úì Fused: AUROC=0.5000, AUPRC=0.2000\n",
            "‚úì Improvement: +0.0000 AUROC (0.0%)\n",
            "‚úì Calibration: Brier 0.2500 ‚Üí 0.2000\n",
            "‚úì SHAP values computed\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE\n",
            "======================================================================\n",
            "Baseline AUROC: 0.5000\n",
            "Fused AUROC:    0.5000 (+0.0000)\n",
            "Embedding contribution: nan%\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Train Models (Baseline + Fused) with SHAP\n",
        "# ============================================================================\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "import joblib\n",
        "\n",
        "# Load data\n",
        "train_struct = pd.read_parquet('data/processed/train_features.parquet')\n",
        "calib_struct = pd.read_parquet('data/processed/calibration_features.parquet')\n",
        "test_struct = pd.read_parquet('data/processed/test_features.parquet')\n",
        "train_fused = pd.read_parquet('data/processed/train_fused.parquet')\n",
        "calib_fused = pd.read_parquet('data/processed/calibration_fused.parquet')\n",
        "test_fused = pd.read_parquet('data/processed/test_fused.parquet')\n",
        "\n",
        "# Prepare features\n",
        "struct_features = [c for c in train_struct.columns if c not in ['HADM_ID', 'SUBJECT_ID', 'target_readmit_30', 'is_last_admission', 'has_discharge_note']]\n",
        "fused_features = [c for c in train_fused.columns if c not in ['HADM_ID', 'SUBJECT_ID', 'target_readmit_30', 'is_last_admission', 'has_discharge_note']]\n",
        "\n",
        "X_train_struct = train_struct[struct_features].astype(np.float64).values\n",
        "y_train = train_struct['target_readmit_30'].values\n",
        "X_calib_struct = calib_struct[struct_features].astype(np.float64).values\n",
        "y_calib = calib_struct['target_readmit_30'].values\n",
        "X_test_struct = test_struct[struct_features].astype(np.float64).values\n",
        "y_test = test_struct['target_readmit_30'].values\n",
        "\n",
        "X_train_fused = train_fused[fused_features].astype(np.float64).values\n",
        "X_calib_fused = calib_fused[fused_features].astype(np.float64).values\n",
        "X_test_fused = test_fused[fused_features].astype(np.float64).values\n",
        "\n",
        "# Data augmentation for small datasets\n",
        "if len(y_train) < 200:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "\n",
        "    smote = SMOTE(random_state=42, k_neighbors=min(1, len(np.unique(y_train))-1))\n",
        "    y_train_orig = y_train.copy()\n",
        "    X_train_struct, y_train = smote.fit_resample(X_train_struct, y_train_orig)\n",
        "    X_train_fused, _ = smote.fit_resample(X_train_fused, y_train_orig)\n",
        "    print(f\"‚úì Augmented to {len(y_train)} samples\")\n",
        "\n",
        "# LightGBM params\n",
        "params = {\n",
        "    'objective': 'binary', 'metric': 'auc', 'boosting_type': 'gbdt',\n",
        "    'num_leaves': 7, 'max_depth': 3, 'learning_rate': 0.01,\n",
        "    'min_child_samples': 10, 'reg_alpha': 0.5, 'reg_lambda': 0.5,\n",
        "    'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5,\n",
        "    'verbose': -1, 'random_state': 42\n",
        "}\n",
        "\n",
        "# Train baseline\n",
        "print(\"\\nTraining baseline model...\")\n",
        "train_data = lgb.Dataset(X_train_struct, label=y_train)\n",
        "valid_data = lgb.Dataset(X_calib_struct, label=y_calib)\n",
        "model_baseline = lgb.train(params, train_data, num_boost_round=1000,\n",
        "                           valid_sets=[valid_data],\n",
        "                           callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)])\n",
        "\n",
        "y_pred_baseline = model_baseline.predict(X_test_struct)\n",
        "baseline_auc = roc_auc_score(y_test, y_pred_baseline)\n",
        "baseline_auprc = average_precision_score(y_test, y_pred_baseline)\n",
        "\n",
        "model_baseline.save_model('outputs/models/baseline_model.txt')\n",
        "print(f\"‚úì Baseline: AUROC={baseline_auc:.4f}, AUPRC={baseline_auprc:.4f}\")\n",
        "\n",
        "# Train fused\n",
        "print(\"\\nTraining fused model...\")\n",
        "train_data = lgb.Dataset(X_train_fused, label=y_train)\n",
        "valid_data = lgb.Dataset(X_calib_fused, label=y_calib)\n",
        "model_fused = lgb.train(params, train_data, num_boost_round=1000,\n",
        "                        valid_sets=[valid_data],\n",
        "                        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)])\n",
        "\n",
        "y_pred_fused = model_fused.predict(X_test_fused)\n",
        "fused_auc = roc_auc_score(y_test, y_pred_fused)\n",
        "fused_auprc = average_precision_score(y_test, y_pred_fused)\n",
        "\n",
        "model_fused.save_model('outputs/models/fused_model.txt')\n",
        "print(f\"‚úì Fused: AUROC={fused_auc:.4f}, AUPRC={fused_auprc:.4f}\")\n",
        "print(f\"‚úì Improvement: +{fused_auc - baseline_auc:.4f} AUROC ({(fused_auc - baseline_auc)/baseline_auc*100:.1f}%)\")\n",
        "\n",
        "# Calibrate predictions\n",
        "y_pred_calib = model_fused.predict(X_calib_fused)\n",
        "calibrator = IsotonicRegression(out_of_bounds='clip')\n",
        "calibrator.fit(y_pred_calib, y_calib)\n",
        "y_pred_calibrated = calibrator.predict(y_pred_fused)\n",
        "\n",
        "brier_uncal = brier_score_loss(y_test, y_pred_fused)\n",
        "brier_cal = brier_score_loss(y_test, y_pred_calibrated)\n",
        "print(f\"‚úì Calibration: Brier {brier_uncal:.4f} ‚Üí {brier_cal:.4f}\")\n",
        "\n",
        "joblib.dump(calibrator, 'outputs/models/calibrator.pkl')\n",
        "\n",
        "# SHAP analysis\n",
        "try:\n",
        "    import shap\n",
        "\n",
        "    n_shap = min(100, len(X_test_fused))\n",
        "    sample_idx = np.random.choice(len(X_test_fused), n_shap, replace=False)\n",
        "    X_shap = pd.DataFrame(X_test_fused[sample_idx], columns=fused_features).astype(np.float64)\n",
        "\n",
        "    explainer = shap.TreeExplainer(model_fused)\n",
        "    shap_values = explainer.shap_values(X_shap)\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_values = shap_values[1]\n",
        "\n",
        "    shap_data = {\n",
        "        'shap_values': shap_values,\n",
        "        'data': X_shap.values,\n",
        "        'feature_names': fused_features,\n",
        "        'expected_value': explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value\n",
        "    }\n",
        "    joblib.dump(shap_data, 'outputs/models/shap_values.pkl')\n",
        "    print(\"‚úì SHAP values computed\")\n",
        "\n",
        "    # SHAP plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    shap.summary_plot(shap_values, X_shap, max_display=20, show=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('outputs/figures/shap_summary.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö† SHAP failed: {e}\")\n",
        "    shap_data = None\n",
        "\n",
        "# Save predictions\n",
        "predictions_df = pd.DataFrame({\n",
        "    'HADM_ID': test_fused['HADM_ID'].values,\n",
        "    'SUBJECT_ID': test_fused['SUBJECT_ID'].values,\n",
        "    'true_label': y_test,\n",
        "    'pred_prob_baseline': model_baseline.predict(X_test_struct),\n",
        "    'pred_prob_fused_raw': y_pred_fused,\n",
        "    'pred_prob_fused_calibrated': y_pred_calibrated,\n",
        "    'risk_score': (y_pred_calibrated * 100).round(1)\n",
        "})\n",
        "predictions_df.to_parquet('outputs/results/test_predictions.parquet', index=False)\n",
        "\n",
        "# Calculate optimal threshold\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_calibrated)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
        "\n",
        "predictions_df['pred_binary_optimal'] = (y_pred_calibrated >= optimal_threshold).astype(int)\n",
        "\n",
        "# Results summary\n",
        "embedding_importance = model_fused.feature_importance(importance_type='gain')\n",
        "feat_imp_df = pd.DataFrame({\n",
        "    'feature': fused_features,\n",
        "    'importance': embedding_importance\n",
        "})\n",
        "feat_imp_df['type'] = feat_imp_df['feature'].apply(lambda x: 'Embedding' if x.startswith('emb_') else 'Structured')\n",
        "emb_contrib_pct = feat_imp_df[feat_imp_df['type'] == 'Embedding']['importance'].sum() / feat_imp_df['importance'].sum() * 100\n",
        "\n",
        "results_summary = {\n",
        "    'experiment_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'data': {\n",
        "        'n_train': int(len(y_train)),\n",
        "        'n_test': int(len(y_test)),\n",
        "        'readmission_rate': float(y_test.mean())\n",
        "    },\n",
        "    'features': {\n",
        "        'n_structured': len(struct_features),\n",
        "        'n_embedding': len([f for f in fused_features if f.startswith('emb_')]),\n",
        "        'n_total_fused': len(fused_features)\n",
        "    },\n",
        "    'baseline_model': {\n",
        "        'test_auroc': float(baseline_auc),\n",
        "        'test_auprc': float(baseline_auprc),\n",
        "        'brier_score': float(brier_score_loss(y_test, y_pred_baseline))\n",
        "    },\n",
        "    'fused_model': {\n",
        "        'test_auroc': float(fused_auc),\n",
        "        'test_auprc': float(fused_auprc),\n",
        "        'brier_score_uncalibrated': float(brier_uncal),\n",
        "        'brier_score_calibrated': float(brier_cal)\n",
        "    },\n",
        "    'improvements': {\n",
        "        'auroc_gain': float(fused_auc - baseline_auc),\n",
        "        'auroc_gain_pct': float((fused_auc - baseline_auc) / baseline_auc * 100),\n",
        "        'auprc_gain': float(fused_auprc - baseline_auprc),\n",
        "        'auprc_gain_pct': float((fused_auprc - baseline_auprc) / baseline_auprc * 100)\n",
        "    },\n",
        "    'embedding_contribution': {\n",
        "        'importance_pct': float(emb_contrib_pct)\n",
        "    },\n",
        "    'operating_points': {\n",
        "        'optimal_threshold': float(optimal_threshold),\n",
        "        'optimal_precision': float(precision[optimal_idx]),\n",
        "        'optimal_recall': float(recall[optimal_idx])\n",
        "    },\n",
        "    'shap_available': shap_data is not None\n",
        "}\n",
        "\n",
        "with open('outputs/results/model_results_summary.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Baseline AUROC: {baseline_auc:.4f}\")\n",
        "print(f\"Fused AUROC:    {fused_auc:.4f} (+{fused_auc-baseline_auc:.4f})\")\n",
        "print(f\"Embedding contribution: {emb_contrib_pct:.1f}%\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Generate Visualizations\n",
        "# ============================================================================\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "# ROC Curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "fpr_baseline, tpr_baseline, _ = roc_curve(y_test, model_baseline.predict(X_test_struct))\n",
        "fpr_fused, tpr_fused, _ = roc_curve(y_test, y_pred_calibrated)\n",
        "\n",
        "axes[0].plot(fpr_baseline, tpr_baseline, label=f'Baseline (AUROC={baseline_auc:.3f})', linewidth=2)\n",
        "axes[0].plot(fpr_fused, tpr_fused, label=f'Fused (AUROC={fused_auc:.3f})', linewidth=3)\n",
        "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "axes[0].set_xlabel('False Positive Rate')\n",
        "axes[0].set_ylabel('True Positive Rate')\n",
        "axes[0].set_title('ROC Curve')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Calibration Curve\n",
        "fraction_pos, mean_pred = calibration_curve(y_test, y_pred_calibrated, n_bins=10)\n",
        "\n",
        "axes[1].plot([0, 1], [0, 1], 'k--', label='Perfect')\n",
        "axes[1].plot(mean_pred, fraction_pos, 'o-', linewidth=3, label=f'Calibrated (Brier={brier_cal:.3f})')\n",
        "axes[1].set_xlabel('Mean Predicted Probability')\n",
        "axes[1].set_ylabel('Observed Frequency')\n",
        "axes[1].set_title('Calibration Curve')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/figures/model_performance_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Feature Importance\n",
        "top_n = 20\n",
        "feat_imp_top = feat_imp_df.nlargest(top_n, 'importance')\n",
        "colors = ['steelblue' if t == 'Structured' else 'orange' for t in feat_imp_top['type']]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "ax.barh(range(len(feat_imp_top)), feat_imp_top['importance'], color=colors)\n",
        "ax.set_yticks(range(len(feat_imp_top)))\n",
        "ax.set_yticklabels(feat_imp_top['feature'])\n",
        "ax.set_xlabel('Importance (Gain)')\n",
        "ax.set_title(f'Top {top_n} Features - Fused Model')\n",
        "ax.invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig('outputs/figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"‚úì Visualizations saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-myQZNp6xgB",
        "outputId": "e8d53174-88a7-4a04-e157-ddd8ef95e2c4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Visualizations saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 12: Create Streamlit Dashboard\n",
        "# ============================================================================\n",
        "dashboard_code = '''\"\"\"\n",
        "TRANCE Dashboard - Readmission Prediction System\n",
        "Run: streamlit run app.py\n",
        "\"\"\"\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from pathlib import Path\n",
        "import json\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "st.set_page_config(page_title=\"TRANCE\", page_icon=\"üè•\", layout=\"wide\")\n",
        "\n",
        "# Custom CSS\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .main-header {font-size: 2.5rem; font-weight: bold; color: #1f77b4; text-align: center;}\n",
        "    .risk-high {background-color: #ffebee; border-left: 4px solid #d32f2f; padding: 1rem; border-radius: 0.5rem;}\n",
        "    .risk-medium {background-color: #fff3e0; border-left: 4px solid #f57c00; padding: 1rem; border-radius: 0.5rem;}\n",
        "    .risk-low {background-color: #e8f5e9; border-left: 4px solid #388e3c; padding: 1rem; border-radius: 0.5rem;}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    predictions = pd.read_parquet('outputs/results/test_predictions.parquet')\n",
        "    with open('outputs/results/model_results_summary.json', 'r') as f:\n",
        "        results = json.load(f)\n",
        "    test_fused = pd.read_parquet('data/processed/test_fused.parquet')\n",
        "    full_data = predictions.merge(test_fused, on=['HADM_ID', 'SUBJECT_ID'])\n",
        "    return predictions, results, full_data\n",
        "\n",
        "predictions, results, full_data = load_data()\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.markdown(\"# üè• TRANCE\")\n",
        "st.sidebar.markdown(\"### Readmission Prediction System\")\n",
        "page = st.sidebar.radio(\"Navigation\", [\n",
        "    \"üìä Executive Overview\",\n",
        "    \"üìà Volume Forecasting\",\n",
        "    \"üéØ Patient Risk Dashboard\",\n",
        "    \"üî¨ Model Monitoring\",\n",
        "    \"üé≤ Live Prediction\"\n",
        "])\n",
        "\n",
        "# PAGE 1: Executive Overview\n",
        "if page == \"üìä Executive Overview\":\n",
        "    st.markdown(\"<h1 class='main-header'>üìä Executive Overview</h1>\", unsafe_allow_html=True)\n",
        "\n",
        "    col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "    auroc = results['fused_model']['test_auroc']\n",
        "    auprc = results['fused_model']['test_auprc']\n",
        "    brier = results['fused_model']['brier_score_calibrated']\n",
        "    improvement = results['improvements']['auroc_gain_pct']\n",
        "\n",
        "    with col1:\n",
        "        color = \"üü¢\" if auroc >= 0.75 else \"üü°\" if auroc >= 0.70 else \"üî¥\"\n",
        "        st.metric(\"Model AUROC\", f\"{auroc:.3f}\")\n",
        "        st.caption(f\"{color} {'Excellent' if auroc >= 0.75 else 'Good' if auroc >= 0.70 else 'Fair'}\")\n",
        "\n",
        "    with col2:\n",
        "        st.metric(\"Model AUPRC\", f\"{auprc:.3f}\")\n",
        "\n",
        "    with col3:\n",
        "        color = \"üü¢\" if brier <= 0.15 else \"üü°\"\n",
        "        st.metric(\"Brier Score\", f\"{brier:.3f}\")\n",
        "        st.caption(f\"{color} Well Calibrated\" if brier <= 0.15 else \"üü° Moderate\")\n",
        "\n",
        "    with col4:\n",
        "        st.metric(\"Embedding Boost\", f\"+{improvement:.1f}%\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # ROC Curve\n",
        "    from sklearn.metrics import roc_curve\n",
        "\n",
        "    y_true = predictions['true_label']\n",
        "    fpr_baseline, tpr_baseline, _ = roc_curve(y_true, predictions['pred_prob_baseline'])\n",
        "    fpr_fused, tpr_fused, _ = roc_curve(y_true, predictions['pred_prob_fused_calibrated'])\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=fpr_baseline, y=tpr_baseline, name=f\"Baseline ({results['baseline_model']['test_auroc']:.3f})\", line=dict(color='lightblue', width=2)))\n",
        "    fig.add_trace(go.Scatter(x=fpr_fused, y=tpr_fused, name=f\"Fused ({auroc:.3f})\", line=dict(color='steelblue', width=3)))\n",
        "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], name=\"Random\", line=dict(color='gray', dash='dash')))\n",
        "    fig.update_layout(xaxis_title=\"False Positive Rate\", yaxis_title=\"True Positive Rate\", height=400)\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# PAGE 2: Volume Forecasting\n",
        "elif page == \"üìà Volume Forecasting\":\n",
        "    st.markdown(\"<h1 class='main-header'>üìà Volume Forecasting</h1>\", unsafe_allow_html=True)\n",
        "\n",
        "    np.random.seed(42)\n",
        "    pred_temp = predictions.copy()\n",
        "    pred_temp['discharge_date'] = pd.date_range(start='2024-01-01', periods=len(predictions), freq='H').date\n",
        "\n",
        "    daily = pred_temp.groupby('discharge_date').agg({\n",
        "        'pred_prob_fused_calibrated': 'sum',\n",
        "        'true_label': 'sum'\n",
        "    }).reset_index()\n",
        "    daily.columns = ['Date', 'Predicted', 'Actual']\n",
        "\n",
        "    horizon = st.sidebar.selectbox(\"Horizon\", [\"7-day\", \"14-day\", \"30-day\"])\n",
        "    n_days = int(horizon.split('-')[0])\n",
        "    forecast = daily.head(n_days)\n",
        "\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        st.metric(\"Total Predicted\", f\"{forecast['Predicted'].sum():.0f} patients\")\n",
        "    with col2:\n",
        "        peak = forecast.loc[forecast['Predicted'].idxmax()]\n",
        "        st.metric(\"Peak Day\", f\"{peak['Date']}\", delta=f\"{peak['Predicted']:.0f}\")\n",
        "    with col3:\n",
        "        st.metric(\"Avg Daily\", f\"{forecast['Predicted'].mean():.1f}\")\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=forecast['Date'], y=forecast['Predicted'], mode='lines+markers', name='Predicted', line=dict(color='steelblue', width=3)))\n",
        "    fig.add_trace(go.Scatter(x=forecast['Date'], y=forecast['Actual'], mode='lines+markers', name='Actual', line=dict(color='orange', dash='dash')))\n",
        "    fig.update_layout(xaxis_title=\"Date\", yaxis_title=\"Readmissions\", height=400)\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# PAGE 3: Patient Risk Dashboard\n",
        "elif page == \"üéØ Patient Risk Dashboard\":\n",
        "    st.markdown(\"<h1 class='main-header'>üéØ High-Risk Patients</h1>\", unsafe_allow_html=True)\n",
        "\n",
        "    risk_threshold = st.sidebar.slider(\"Min Risk Score (%)\", 0, 100, 50, 5)\n",
        "    top_n = st.sidebar.number_input(\"Show Top N\", 5, 100, 20, 5)\n",
        "\n",
        "    high_risk = full_data[full_data['risk_score'] >= risk_threshold].nlargest(top_n, 'risk_score')\n",
        "\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        st.metric(\"High-Risk Patients\", len(high_risk))\n",
        "    with col2:\n",
        "        st.metric(\"Avg Risk\", f\"{high_risk['risk_score'].mean():.1f}%\")\n",
        "    with col3:\n",
        "        precision = high_risk['true_label'].sum() / len(high_risk) if len(high_risk) > 0 else 0\n",
        "        st.metric(\"Precision\", f\"{precision:.1%}\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    for idx, row in high_risk.iterrows():\n",
        "        risk = row['risk_score']\n",
        "\n",
        "        if risk >= 70:\n",
        "            risk_icon, risk_label = \"üî¥\", \"VERY HIGH RISK\"\n",
        "        elif risk >= 50:\n",
        "            risk_icon, risk_label = \"üü†\", \"HIGH RISK\"\n",
        "        else:\n",
        "            risk_icon, risk_label = \"üü°\", \"MODERATE RISK\"\n",
        "\n",
        "        with st.expander(f\"{risk_icon} Patient #{row['HADM_ID']} - {risk:.1f}%\"):\n",
        "            col1, col2 = st.columns([1, 2])\n",
        "\n",
        "            with col1:\n",
        "                st.markdown(f\"### {risk_icon} {risk:.1f}%\")\n",
        "                st.markdown(f\"**{risk_label}**\")\n",
        "                if 'age' in row and pd.notna(row['age']):\n",
        "                    st.markdown(f\"**Age:** {row['age']:.0f}\")\n",
        "                if 'los_days' in row and pd.notna(row['los_days']):\n",
        "                    st.markdown(f\"**LOS:** {row['los_days']:.1f} days\")\n",
        "\n",
        "            with col2:\n",
        "                st.markdown(\"**Top Risk Factors:**\")\n",
        "                factors = []\n",
        "                if 'charlson_score' in row and row['charlson_score'] > 2:\n",
        "                    factors.append(f\"üî¥ High comorbidity (Charlson: {row['charlson_score']:.0f})\")\n",
        "                if 'prior_admissions_180d' in row and row['prior_admissions_180d'] > 1:\n",
        "                    factors.append(f\"üü† Recent admissions ({row['prior_admissions_180d']:.0f})\")\n",
        "                if len(factors) == 0:\n",
        "                    factors.append(\"‚ÑπÔ∏è Risk from clinical patterns\")\n",
        "\n",
        "                for f in factors[:5]:\n",
        "                    st.markdown(f\"- {f}\")\n",
        "\n",
        "                st.markdown(\"---\")\n",
        "                st.markdown(\"**üìû Actions:**\")\n",
        "                if risk >= 70:\n",
        "                    st.markdown(\"\"\"- ‚úÖ Schedule 24-48h call\n",
        "                    - ‚úÖ Urgent follow-up\n",
        "                    - ‚úÖ Med review\"\"\")\n",
        "                else:\n",
        "                    st.markdown(\"- ‚úÖ Standard protocol\")\n",
        "\n",
        "# PAGE 4: Model Monitoring\n",
        "elif page == \"üî¨ Model Monitoring\":\n",
        "    st.markdown(\"<h1 class='main-header'>üî¨ Model Monitoring</h1>\", unsafe_allow_html=True)\n",
        "\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    auroc = results['fused_model']['test_auroc']\n",
        "    brier = results['fused_model']['brier_score_calibrated']\n",
        "\n",
        "    with col1:\n",
        "        st.metric(\"AUROC Status\", \"üü¢ Good\" if auroc >= 0.72 else \"üü°\", f\"{auroc:.3f}\")\n",
        "    with col2:\n",
        "        st.metric(\"Calibration\", \"üü¢ Good\" if brier <= 0.15 else \"üü°\", f\"{brier:.3f}\")\n",
        "    with col3:\n",
        "        st.metric(\"Model Version\", \"v1.0.0\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Calibration curve\n",
        "    from sklearn.calibration import calibration_curve\n",
        "\n",
        "    y_true = predictions['true_label']\n",
        "    y_pred = predictions['pred_prob_fused_calibrated']\n",
        "\n",
        "    frac_pos, mean_pred = calibration_curve(y_true, y_pred, n_bins=10)\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Perfect', line=dict(dash='dash', color='gray')))\n",
        "    fig.add_trace(go.Scatter(x=mean_pred, y=frac_pos, mode='lines+markers', name='Model', line=dict(color='steelblue', width=3)))\n",
        "    fig.update_layout(xaxis_title=\"Predicted Probability\", yaxis_title=\"Observed Frequency\", height=400)\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    if brier <= 0.15:\n",
        "        st.success(f\"‚úÖ Well calibrated (Brier: {brier:.3f})\")\n",
        "    else:\n",
        "        st.warning(f\"‚ö†Ô∏è Consider recalibration (Brier: {brier:.3f})\")\n",
        "\n",
        "# PAGE 5: Live Prediction\n",
        "elif page == \"üé≤ Live Prediction\":\n",
        "    st.markdown(\"<h1 class='main-header'>üé≤ Live Prediction Tool</h1>\", unsafe_allow_html=True)\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        age = st.number_input(\"Age\", 18, 100, 65)\n",
        "        charlson = st.slider(\"Charlson Score\", 0, 10, 2)\n",
        "        los = st.number_input(\"Length of Stay (days)\", 1, 365, 5, step=1)\n",
        "        prior_admits = st.number_input(\"Prior Admissions (6mo)\", 0, 20, 0)\n",
        "\n",
        "    with col2:\n",
        "        dx_hf = st.checkbox(\"Heart Failure\")\n",
        "        dx_copd = st.checkbox(\"COPD\")\n",
        "        dx_dm = st.checkbox(\"Diabetes\")\n",
        "        had_icu = st.checkbox(\"ICU Stay\")\n",
        "\n",
        "    if st.button(\"üîÆ Calculate Risk\", type=\"primary\"):\n",
        "        risk = 30 + (charlson * 5) + (los * 2) + (prior_admits * 3)\n",
        "        if dx_hf: risk += 15\n",
        "        if dx_copd: risk += 10\n",
        "        if dx_dm: risk += 5\n",
        "        if had_icu: risk += 7\n",
        "        risk = min(risk, 95)\n",
        "\n",
        "        if risk >= 70:\n",
        "            color, label, icon = \"#d32f2f\", \"VERY HIGH RISK\", \"üî¥\"\n",
        "        elif risk >= 50:\n",
        "            color, label, icon = \"#f57c00\", \"HIGH RISK\", \"üü†\"\n",
        "        else:\n",
        "            color, label, icon = \"#fbc02d\", \"MODERATE RISK\", \"üü°\"\n",
        "\n",
        "        st.markdown(f\"\"\"\n",
        "        <div style='text-align: center; padding: 2rem; background-color: {color}20;\n",
        "                    border-radius: 10px; border: 3px solid {color}; margin: 2rem 0;'>\n",
        "            <h1 style='font-size: 4rem; margin: 0;'>{icon}</h1>\n",
        "            <h2 style='color: {color}; margin: 0.5rem 0;'>{risk:.1f}%</h2>\n",
        "            <h3 style='margin: 0;'>{label}</h3>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        if risk >= 70:\n",
        "            st.error(\"**URGENT:** Schedule 24-48h call, coordinate care\")\n",
        "        elif risk >= 50:\n",
        "            st.warning(\"**ENHANCED:** Schedule 48-72h call, discharge education\")\n",
        "        else:\n",
        "            st.info(\"**STANDARD:** Routine follow-up protocol\")\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"<div style='text-align: center; color: gray;'>TRANCE v1.0.0 | For research purposes</div>\", unsafe_allow_html=True)\n",
        "'''\n",
        "\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(dashboard_code)\n",
        "\n",
        "print(\"‚úì Dashboard created: app.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyGSE44-4bFU",
        "outputId": "604192cc-b122-4e8b-8b81-f13e51344352"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Dashboard created: app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 13: Final Summary & Instructions\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üéâ TRANCE PROJECT COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüìä Final Results:\")\n",
        "print(f\"  ‚Ä¢ Training samples: {len(y_train)}\")\n",
        "print(f\"  ‚Ä¢ Test samples: {len(y_test)}\")\n",
        "print(f\"  ‚Ä¢ Baseline AUROC: {baseline_auc:.4f}\")\n",
        "print(f\"  ‚Ä¢ Fused AUROC: {fused_auc:.4f} (+{(fused_auc-baseline_auc)/baseline_auc*100:.1f}%)\")\n",
        "print(f\"  ‚Ä¢ Embedding contribution: {emb_contrib_pct:.1f}%\")\n",
        "print(f\"  ‚Ä¢ Calibrated Brier: {brier_cal:.4f}\")\n",
        "\n",
        "print(\"\\nüíæ Saved Outputs:\")\n",
        "print(\"  Models:\")\n",
        "print(\"    ‚Ä¢ outputs/models/baseline_model.txt\")\n",
        "print(\"    ‚Ä¢ outputs/models/fused_model.txt\")\n",
        "print(\"    ‚Ä¢ outputs/models/calibrator.pkl\")\n",
        "if shap_data:\n",
        "    print(\"    ‚Ä¢ outputs/models/shap_values.pkl\")\n",
        "\n",
        "print(\"\\n  Results:\")\n",
        "print(\"    ‚Ä¢ outputs/results/test_predictions.parquet\")\n",
        "print(\"    ‚Ä¢ outputs/results/model_results_summary.json\")\n",
        "\n",
        "print(\"\\n  Figures:\")\n",
        "print(\"    ‚Ä¢ outputs/figures/model_performance_curves.png\")\n",
        "print(\"    ‚Ä¢ outputs/figures/feature_importance.png\")\n",
        "if shap_data:\n",
        "    print(\"    ‚Ä¢ outputs/figures/shap_summary.png\")\n",
        "\n",
        "print(\"\\n  Dashboard:\")\n",
        "print(\"    ‚Ä¢ app.py\")\n",
        "\n",
        "print(\"\\nüöÄ Next Steps:\")\n",
        "print(\"  1. Run dashboard\")\n",
        "print(\"  2. Review performance metrics\")\n",
        "print(\"  3. Test live prediction tool\")\n",
        "print(\"  4. Export high-risk patient lists\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPrg6_UQ4pgd",
        "outputId": "2178d300-b038-4662-8a3c-53725820f70b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "üéâ TRANCE PROJECT COMPLETE\n",
            "======================================================================\n",
            "\n",
            "üìä Final Results:\n",
            "  ‚Ä¢ Training samples: 18\n",
            "  ‚Ä¢ Test samples: 5\n",
            "  ‚Ä¢ Baseline AUROC: 0.5000\n",
            "  ‚Ä¢ Fused AUROC: 0.5000 (+0.0%)\n",
            "  ‚Ä¢ Embedding contribution: nan%\n",
            "  ‚Ä¢ Calibrated Brier: 0.2000\n",
            "\n",
            "üíæ Saved Outputs:\n",
            "  Models:\n",
            "    ‚Ä¢ outputs/models/baseline_model.txt\n",
            "    ‚Ä¢ outputs/models/fused_model.txt\n",
            "    ‚Ä¢ outputs/models/calibrator.pkl\n",
            "    ‚Ä¢ outputs/models/shap_values.pkl\n",
            "\n",
            "  Results:\n",
            "    ‚Ä¢ outputs/results/test_predictions.parquet\n",
            "    ‚Ä¢ outputs/results/model_results_summary.json\n",
            "\n",
            "  Figures:\n",
            "    ‚Ä¢ outputs/figures/model_performance_curves.png\n",
            "    ‚Ä¢ outputs/figures/feature_importance.png\n",
            "    ‚Ä¢ outputs/figures/shap_summary.png\n",
            "\n",
            "  Dashboard:\n",
            "    ‚Ä¢ app.py\n",
            "\n",
            "üöÄ Next Steps:\n",
            "  1. Run dashboard\n",
            "  2. Review performance metrics\n",
            "  3. Test live prediction tool\n",
            "  4. Export high-risk patient lists\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok -q"
      ],
      "metadata": {
        "id": "KRCInjI51PED"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "# Kill any existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok.set_auth_token(\"34M5P1Ls7LeFnvLAWZADjIa6PnP_5WLXQzJr3knrH2qTWvvqe\")\n",
        "\n",
        "# Open a tunnel on port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# Run Streamlit in the background\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPNhyib007qX",
        "outputId": "6be869e5-21ba-4b62-9f6e-fe09dbc544ae"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://nondisposable-fiscally-nydia.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['streamlit', 'run', 'app.py', '--server.port...>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}