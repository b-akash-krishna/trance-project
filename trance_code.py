# -*- coding: utf-8 -*-
"""trance_embeddings

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EAJN-fgZztS1seCsD23kkQMkhLBaGUAh
"""

def create_project_structure():
    """Create the complete TRANCE project directory structure"""

    # Define project structure
    directories = [
        'data/raw',
        'data/processed',
        'data/embeddings',
        'notebooks',
        'src/data_processing',
        'src/models',
        'src/evaluation',
        'src/dashboard',
        'configs',
        'outputs/models',
        'outputs/figures',
        'outputs/results',
        'docs',
        'tests'
    ]

    # Create directories
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
        print(f"✓ Created: {directory}")

    return directories

create_project_structure()

def create_config():
  """Create initial configuration file"""
  config = {
      "project": {
          "name": "TRANCE",
          "version": "1.0.0",
          "description": "Temporal Readmission Analysis with Neural Clinical Embeddings"
      },
      "data": {
          "mimic_demo_url": "https://physionet.org/content/mimiciii-demo/1.4/",
          "cohort": {
              "min_age": 18,
              "min_los_hours": 24,
              "readmission_window_days": 30
          }
      },
      "model": {
          "embedding_model": "luqh/ClinicalT5-large",
          "embedding_dim": 768,
          "max_text_length": 512,
          "chunk_overlap": 128
      },
      "training": {
          "test_size": 0.2,
          "calibration_size": 0.1,
          "random_state": 42,
          "lgbm_params": {
              "objective": "binary",
              "metric": "auc",
              "boosting_type": "gbdt",
              "num_leaves": 31,
              "learning_rate": 0.05,
              "feature_fraction": 0.9
          }
      },
      "paths": {
          "raw_data": "data/raw",
          "processed_data": "data/processed",
          "embeddings": "data/embeddings",
          "models": "outputs/models",
          "figures": "outputs/figures"
      }
  }

  config_path = Path('configs/config.json')
  with open(config_path, 'w') as f:
      json.dump(config, f, indent=2)
  print(f"✓ Created: {config_path}")

  return config

create_config()

"""
Notebook: 01_data_download.ipynb
MIMIC-III Demo Dataset Download and Initial Exploration
"""
# Cell 1: Setup and Imports
import pandas as pd
import numpy as np
import os
from pathlib import Path
import requests
import gzip
import shutil
from tqdm import tqdm
import json

# Load config
with open('configs/config.json', 'r') as f:
    config = json.load(f)

print("✓ Imports complete")
print(f"Project: {config['project']['name']} v{config['project']['version']}")

# Cell 2: Download MIMIC-III Demo Files
"""
MIMIC-III Demo Dataset
- Publicly available (no PhysioNet credentials needed)
- Contains 100 patients
- Perfect for prototyping
"""

import requests
from tqdm import tqdm
from pathlib import Path

def download_file(url, destination):
    """Download a CSV file with progress bar and error handling"""
    try:
        response = requests.get(url, stream=True)
        if response.status_code != 200:
            print(f"⚠️ Failed to download {url} (HTTP {response.status_code})")
            return False

        total_size = int(response.headers.get('content-length', 0))
        with open(destination, 'wb') as f, tqdm(
            desc=destination.name,
            total=total_size if total_size > 0 else None,
            unit='iB',
            unit_scale=True,
            unit_divisor=1024,
        ) as pbar:
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
                    pbar.update(len(chunk))

        # Verify that file is not HTML (in case of bad URL)
        with open(destination, 'r', encoding='utf-8', errors='ignore') as f:
            start = f.read(15)
            if "<html>" in start.lower():
                print(f"⚠️ Skipping {destination.name}: HTML page instead of CSV")
                destination.unlink(missing_ok=True)
                return False

        return True

    except Exception as e:
        print(f"❌ Error downloading {url}: {e}")
        return False


# ✅ Correct PhysioNet base URL (uses /files/, not /content/)
base_url = "https://physionet.org/files/mimiciii-demo/1.4/"

# Key tables we need
tables = [
    'ADMISSIONS.csv',
    'PATIENTS.csv',
    'DIAGNOSES_ICD.csv',
    'PROCEDURES_ICD.csv',
    'PRESCRIPTIONS.csv',
    'LABEVENTS.csv',
    # 'CHARTEVENTS.csv',
    'NOTEEVENTS.csv',
    'ICUSTAYS.csv',
    'D_ICD_DIAGNOSES.csv',
    'D_ICD_PROCEDURES.csv',
    'D_LABITEMS.csv'
]

# Create data directory
data_dir = Path('data/raw')
data_dir.mkdir(parents=True, exist_ok=True)

print("⬇️ Downloading MIMIC-III Demo CSV files...\n")

for table in tables:
    url = base_url + table
    dest = data_dir / table

    if dest.exists():
        print(f"✓ {table} already exists")
        continue

    print(f"Downloading {table}...")
    success = download_file(url, dest)
    if success:
        print(f"✓ Downloaded {table}")
    else:
        print(f"❌ Failed to download {table}")

print("\n✅ All available CSV files downloaded successfully!")

# # Cell 3: Extract CSV files
# """
# Extract gzipped files
# """
# print("Extracting CSV files...")

# for table in tables:
#     gz_file = data_dir / table
#     csv_file = data_dir / table.replace('.gz', '')

#     if csv_file.exists():
#         print(f"✓ {csv_file.name} already exists")
#     else:
#         print(f"Extracting {table}...")
#         with gzip.open(gz_file, 'rb') as f_in:
#             with open(csv_file, 'wb') as f_out:
#                 shutil.copyfileobj(f_in, f_out)
#         print(f"✓ Extracted {csv_file.name}")

# print("\n✅ All files extracted!")

# Cell 4: Load and explore key tables
"""
Initial Data Exploration
"""
print("Loading key tables...")
from pathlib import Path

data_dir = Path("data/raw")  # if your notebook is in notebooks/

# Load tables
admissions = pd.read_csv(data_dir / 'ADMISSIONS.csv')
patients = pd.read_csv(data_dir / 'PATIENTS.csv')
diagnoses = pd.read_csv(data_dir / 'DIAGNOSES_ICD.csv')
notes = pd.read_csv(data_dir / 'NOTEEVENTS.csv')
labs = pd.read_csv(data_dir / 'LABEVENTS.csv')
prescriptions = pd.read_csv(data_dir / 'PRESCRIPTIONS.csv')

# Standardize column names to uppercase
admissions.columns = admissions.columns.str.upper()
patients.columns = patients.columns.str.upper()
diagnoses.columns = diagnoses.columns.str.upper()
notes.columns = notes.columns.str.upper()
labs.columns = labs.columns.str.upper()
prescriptions.columns = prescriptions.columns.str.upper()


print("✓ Tables loaded\n")

# Display table sizes
print("=" * 60)
print("MIMIC-III Demo Dataset Overview")
print("=" * 60)
print(f"Patients:        {len(patients):>10,} rows")
print(f"Admissions:      {len(admissions):>10,} rows")
print(f"Diagnoses:       {len(diagnoses):>10,} rows")
print(f"Clinical Notes:  {len(notes):>10,} rows")
print(f"Lab Events:      {len(labs):>10,} rows")
print(f"Prescriptions:   {len(prescriptions):>10,} rows")
print("=" * 60)

# Cell 5: Admissions Table Exploration
"""
Explore ADMISSIONS table - our primary table
"""
print("ADMISSIONS Table Sample:")
print(admissions.head())
print("\nColumns:", admissions.columns.tolist())
print("\nData Types:")
print(admissions.dtypes)
print("\nMissing Values:")
print(admissions.isnull().sum())

# admissions.columns = admissions.columns.str.upper()

admissions['ADMITTIME'] = pd.to_datetime(admissions['ADMITTIME'])
admissions['DISCHTIME'] = pd.to_datetime(admissions['DISCHTIME'])
admissions['DEATHTIME'] = pd.to_datetime(admissions['DEATHTIME'])

admissions['LOS_DAYS'] = (admissions['DISCHTIME'] - admissions['ADMITTIME']).dt.total_seconds() / 86400

print("\nAdmission Statistics:")
print(admissions[['LOS_DAYS']].describe())

# Cell 6: Patient Demographics
"""
Explore patient demographics
"""
# Merge patients with admissions for analysis
patient_admissions = admissions.merge(patients, on='SUBJECT_ID')

# Calculate age at admission
patient_admissions['AGE'] = patient_admissions['ADMITTIME'].dt.year - patient_admissions['DOB'].apply(lambda x: pd.to_datetime(x).year)

print("Patient Demographics:")
print("\nGender Distribution:")
print(patient_admissions['GENDER'].value_counts())

print("\nAge Statistics:")
print(patient_admissions['AGE'].describe())

print("\nEthnicity Distribution:")
print(patient_admissions['ETHNICITY'].value_counts().head(10))

print("\nInsurance Distribution:")
print(patient_admissions['INSURANCE'].value_counts())
import pandas as pd
import numpy as np
from pathlib import Path

data_dir = Path("data/raw")  # Adjust if your notebook is elsewhere

# Load tables
admissions = pd.read_csv(data_dir / 'ADMISSIONS.csv')
diagnoses = pd.read_csv(data_dir / 'DIAGNOSES_ICD.csv')
noteevents = pd.read_csv(data_dir / 'NOTEEVENTS.csv')
procedures = pd.read_csv(data_dir / 'PROCEDURES_ICD.csv')
patients = pd.read_csv(data_dir / 'PATIENTS.csv')
prescriptions = pd.read_csv(data_dir / 'PRESCRIPTIONS.csv')

# Capitalize column names
admissions.columns = admissions.columns.str.upper()
diagnoses.columns = diagnoses.columns.str.upper()
noteevents.columns = noteevents.columns.str.upper()
procedures.columns = procedures.columns.str.upper()
patients.columns = patients.columns.str.upper()
prescriptions.columns = prescriptions.columns.str.upper()

# Merge admissions with patient info
admissions_patients = admissions.merge(patients, on='SUBJECT_ID', suffixes=('_ADM', '_PAT'))

# Convert to datetime
admissions_patients['ADMITTIME'] = pd.to_datetime(admissions_patients['ADMITTIME'])
admissions_patients['DISCHTIME'] = pd.to_datetime(admissions_patients['DISCHTIME'])
admissions_patients['DOB'] = pd.to_datetime(admissions_patients['DOB'])

# Calculate age and LOS
admissions_patients['AGE'] = admissions_patients['ADMITTIME'].dt.year - admissions_patients['DOB'].dt.year
admissions_patients['LOS_DAYS'] = (admissions_patients['DISCHTIME'] - admissions_patients['ADMITTIME']).dt.total_seconds() / (24*3600)

# Updated discharge text generator
def generate_discharge_text_updated(hadm_id):
    row = admissions_patients[admissions_patients['HADM_ID'] == hadm_id].iloc[0]

    age = row.get('AGE', 'unknown age')
    gender = row.get('GENDER', 'unknown gender')
    if gender == 'M':
        gender = 'male'
    elif gender == 'F':
        gender = 'female'
    admit_date = row.get('ADMITTIME').strftime('%Y-%m-%d') if pd.notna(row.get('ADMITTIME')) else 'unknown date'
    los_days = round(row.get('LOS_DAYS', 0), 1)
    admission_type = row.get('ADMISSION_TYPE', 'unspecified')
    discharge_location = row.get('DISCHARGE_LOCATION', 'unknown')

    # Diagnoses
    patient_diag = diagnoses[diagnoses['HADM_ID'] == hadm_id]
    diag_text = ", ".join(patient_diag['ICD9_CODE'].astype(str).tolist()) or "No significant diagnoses recorded"

    # Procedures
    patient_proc = procedures[procedures['HADM_ID'] == hadm_id]
    proc_text = ", ".join(patient_proc['ICD9_CODE'].astype(str).tolist()) or "No procedures performed"

    # Prescriptions
    patient_rx = prescriptions[prescriptions['HADM_ID'] == hadm_id]
    rx_list = []
    for _, rx in patient_rx.iterrows():
        med = rx.get('DRUG', 'Unknown drug')
        dose = rx.get('DOSE_VAL_RX', '')
        unit = rx.get('DOSE_UNIT_RX', '')
        route = rx.get('ROUTE', '')
        parts = [str(part) for part in [dose, unit, route, med] if part]
        rx_list.append(" ".join(parts))
    rx_text = "; ".join(rx_list) if rx_list else "No medications prescribed"

    discharge_summary = (
        f"The patient is a {age}-year-old {gender} admitted on {admit_date} "
        f"via {admission_type}. During the {los_days}-day hospital stay, "
        f"the following diagnoses were recorded: {diag_text}. "
        f"Procedures performed include: {proc_text}. "
        f"At discharge, the patient was prescribed: {rx_text}. "
        f"The patient was discharged to {discharge_location} with appropriate follow-up instructions."
    )

    return discharge_summary

# Populate NOTEEVENTS
for idx, row in admissions_patients.iterrows():
    hadm_id = row['HADM_ID']

    # Find existing notes
    note_idx = noteevents[noteevents['HADM_ID'] == hadm_id].index

    if len(note_idx) == 0:
        new_row = {
            'ROW_ID': noteevents['ROW_ID'].max() + 1 if len(noteevents) > 0 else 1,
            'SUBJECT_ID': row['SUBJECT_ID'],
            'HADM_ID': hadm_id,
            'CHARTDATE': row['ADMITTIME'].strftime('%Y-%m-%d') if pd.notna(row['ADMITTIME']) else '',
            'CHARTTIME': row['ADMITTIME'].strftime('%H:%M:%S') if pd.notna(row['ADMITTIME']) else '',
            'STORETIME': row['ADMITTIME'] if pd.notna(row['ADMITTIME']) else '',
            'CATEGORY': 'Discharge summary',
            'DESCRIPTION': 'Synthetic discharge summary',
            'CGID': np.nan,
            'ISERROR': False,
            'TEXT': generate_discharge_text_updated(hadm_id)
        }
        noteevents = pd.concat([noteevents, pd.DataFrame([new_row])], ignore_index=True)
    else:
        # Update empty TEXT fields
        for i in note_idx:
            if pd.isna(noteevents.at[i, 'TEXT']) or noteevents.at[i, 'TEXT'].strip() == '':
                noteevents.at[i, 'TEXT'] = generate_discharge_text_updated(hadm_id)
                noteevents.at[i, 'CATEGORY'] = 'Discharge summary'
                noteevents.at[i, 'DESCRIPTION'] = 'Synthetic discharge summary'

# Save updated NOTEEVENTS
noteevents.to_csv(data_dir / 'NOTEEVENTS_with_discharge_text.csv', index=False)

# Preview
print(noteevents[['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'CATEGORY', 'DESCRIPTION', 'TEXT']].head(2).to_string())

# Cell 7: Clinical Notes Analysis
"""
Explore clinical notes - critical for embeddings
"""
print("Clinical Notes Analysis")
print("=" * 60)

print("\nNote Categories:")
print(noteevents['CATEGORY'].value_counts())

print("\nNote Descriptions:")
print(noteevents['DESCRIPTION'].value_counts().head(10))

# Filter discharge summaries
discharge_notes = noteevents[noteevents['CATEGORY'] == 'Discharge summary']
print(f"\nDischarge Summaries: {len(discharge_notes)} notes")

# Sample discharge summary
print("\nSample Discharge Summary:")
print("=" * 60)
if len(discharge_notes) > 0:
    sample = discharge_notes.iloc[0]
    print(f"Subject ID: {sample['SUBJECT_ID']}")
    print(f"Admission ID: {sample['HADM_ID']}")
    print(f"Chart Date: {sample['CHARTDATE']}")
    print(f"\nText (first 500 chars):")
    print(sample['TEXT'][:500] if pd.notna(sample['TEXT']) else "No text available")
    print("...")
# Cell 8: Readmission Analysis
"""
Identify readmissions in demo data
"""
print("Readmission Analysis")
print("=" * 60)

# Ensure datetime
admissions['ADMITTIME'] = pd.to_datetime(admissions['ADMITTIME'], errors='coerce')
admissions['DISCHTIME'] = pd.to_datetime(admissions['DISCHTIME'], errors='coerce')

# Drop rows where dates are missing
admissions_clean = admissions.dropna(subset=['ADMITTIME', 'DISCHTIME'])

# Sort by patient and admission time
admissions_sorted = admissions_clean.sort_values(['SUBJECT_ID', 'ADMITTIME'])

# Calculate time to next admission
admissions_sorted['NEXT_ADMIT'] = admissions_sorted.groupby('SUBJECT_ID')['ADMITTIME'].shift(-1)
admissions_sorted['DAYS_TO_NEXT_ADMIT'] = (
    (admissions_sorted['NEXT_ADMIT'] - admissions_sorted['DISCHTIME']).dt.total_seconds() / 86400
)

# Flag 30-day readmission
admissions_sorted['TARGET_READMIT_30'] = (admissions_sorted['DAYS_TO_NEXT_ADMIT'] <= 30) & (admissions_sorted['DAYS_TO_NEXT_ADMIT'] > 0)

print(f"Total admissions: {len(admissions_sorted)}")
print(f"Patients with multiple admissions: {admissions_sorted.groupby('SUBJECT_ID').size().gt(1).sum()}")
print(f"30-day readmissions: {admissions_sorted['TARGET_READMIT_30'].sum()} ({admissions_sorted['TARGET_READMIT_30'].mean()*100:.1f}%)")


# Cell 9: Diagnosis Analysis
"""
Analyze diagnosis patterns
"""
print("Diagnosis Analysis")
print("=" * 60)

# Load diagnosis descriptions
d_icd = pd.read_csv(data_dir / 'D_ICD_DIAGNOSES.csv')
d_icd.columns = d_icd.columns.str.upper()

# Merge with diagnoses
diagnoses_with_desc = diagnoses.merge(d_icd, on=['ICD9_CODE'], how='left')

print(f"Total diagnosis records: {len(diagnoses)}")
print(f"Unique ICD9 codes: {diagnoses['ICD9_CODE'].nunique()}")

print("\nTop 10 Most Common Diagnoses:")
top_diagnoses = diagnoses_with_desc.groupby(['ICD9_CODE', 'LONG_TITLE']).size().sort_values(ascending=False).head(10)
for (code, title), count in top_diagnoses.items():
    print(f"{code:>6} | {count:>3} | {title}")



"""
Notebook: 02_cohort_definition.ipynb
Define study cohort with inclusion/exclusion criteria and create readmission labels
"""

# Cell 1: Setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from datetime import timedelta

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)
sns.set_style('whitegrid')

# Load config
with open('configs/config.json', 'r') as f:
    config = json.load(f)

data_dir = Path('data/raw')
print("✓ Setup complete")

# Cell 2: Load Core Tables
"""
Load the essential tables for cohort definition
"""
print("Loading data tables...")

admissions = pd.read_csv(data_dir / 'ADMISSIONS.csv')
patients = pd.read_csv(data_dir / 'PATIENTS.csv')
notes = pd.read_csv(data_dir / 'NOTEEVENTS_with_discharge_text.csv')

# Standardize column names to uppercase
admissions.columns = admissions.columns.str.upper()
patients.columns = patients.columns.str.upper()
notes.columns = notes.columns.str.upper()

# Convert date columns
admissions['ADMITTIME'] = pd.to_datetime(admissions['ADMITTIME'])
admissions['DISCHTIME'] = pd.to_datetime(admissions['DISCHTIME'])
admissions['DEATHTIME'] = pd.to_datetime(admissions['DEATHTIME'])
admissions['EDREGTIME'] = pd.to_datetime(admissions['EDREGTIME'])
admissions['EDOUTTIME'] = pd.to_datetime(admissions['EDOUTTIME'])

patients['DOB'] = pd.to_datetime(patients['DOB'])
patients['DOD'] = pd.to_datetime(patients['DOD'])

print(f"✓ Loaded {len(admissions)} admissions")
print(f"✓ Loaded {len(patients)} patients")
print(f"✓ Loaded {len(notes)} clinical notes")

# Cell 3: Calculate Basic Features
"""
Add essential calculated fields
"""
print("Calculating admission features...")

# Ensure all columns are uppercase
admissions.columns = admissions.columns.str.upper()
patients.columns = patients.columns.str.upper()

# Merge patient info (DOB, DOD, GENDER)
admissions = admissions.merge(
    patients[['SUBJECT_ID', 'DOB', 'DOD', 'GENDER']],
    on='SUBJECT_ID',
    how='left'
)

# Convert datetime columns safely
for col in ['ADMITTIME', 'DISCHTIME', 'DEATHTIME', 'DOB']:
    admissions[col] = pd.to_datetime(admissions[col], errors='coerce')

# Calculate length of stay in days
admissions['LOS_DAYS'] = (admissions['DISCHTIME'] - admissions['ADMITTIME']).dt.total_seconds() / 86400

# Calculate age at admission safely
admissions['AGE'] = admissions['ADMITTIME'].dt.year - admissions['DOB'].dt.year
admissions['AGE'] -= ((admissions['ADMITTIME'].dt.month < admissions['DOB'].dt.month) |
                      ((admissions['ADMITTIME'].dt.month == admissions['DOB'].dt.month) &
                       (admissions['ADMITTIME'].dt.day < admissions['DOB'].dt.day)))
admissions['AGE'] = admissions['AGE'].clip(0, 120)  # clip unrealistic ages

# In-hospital mortality flag
admissions['HOSPITAL_MORTALITY'] = admissions['HOSPITAL_EXPIRE_FLAG'] == 1

# Summary
print("✓ Features calculated")
print(f"Age range: {admissions['AGE'].min():.1f} - {admissions['AGE'].max():.1f} years")
print(f"LOS range: {admissions['LOS_DAYS'].min():.1f} - {admissions['LOS_DAYS'].max():.1f} days")

# Cell 4: Inclusion Criteria
"""
Apply inclusion criteria to define eligible admissions
"""
print("\n" + "="*60)
print("APPLYING INCLUSION CRITERIA")
print("="*60)

# Start with all admissions
cohort = admissions.copy()
initial_count = len(cohort)
print(f"\nInitial admissions: {initial_count}")

# Criterion 1: Adult patients (Age >= 18)
min_age = config['data']['cohort']['min_age']
cohort = cohort[cohort['AGE'] >= min_age]
print(f"After age >= {min_age}: {len(cohort)} ({len(cohort)/initial_count*100:.1f}%)")

# Criterion 2: Minimum length of stay (>= 24 hours)
min_los_hours = config['data']['cohort']['min_los_hours']
min_los_days = min_los_hours / 24
cohort = cohort[cohort['LOS_DAYS'] >= min_los_days]
print(f"After LOS >= {min_los_hours}h: {len(cohort)} ({len(cohort)/initial_count*100:.1f}%)")

# Criterion 3: Non-elective admissions (emergency or urgent)
cohort = cohort[cohort['ADMISSION_TYPE'].isin(['EMERGENCY', 'URGENT'])]
print(f"After non-elective: {len(cohort)} ({len(cohort)/initial_count*100:.1f}%)")

# Criterion 4: Must have discharge time (exclude still hospitalized)
cohort = cohort[cohort['DISCHTIME'].notna()]
print(f"After has discharge: {len(cohort)} ({len(cohort)/initial_count*100:.1f}%)")

print(f"\n✓ Eligible admissions after inclusion: {len(cohort)}")


# Cell 5: Exclusion Criteria
"""
Apply exclusion criteria
"""
print("\n" + "="*60)
print("APPLYING EXCLUSION CRITERIA")
print("="*60)

before_exclusion = len(cohort)

# Exclusion 1: In-hospital mortality
excluded_mortality = cohort['HOSPITAL_MORTALITY'].sum()
cohort = cohort[~cohort['HOSPITAL_MORTALITY']]
print(f"Excluded in-hospital deaths: {excluded_mortality}")
print(f"Remaining: {len(cohort)} ({len(cohort)/before_exclusion*100:.1f}%)")

# Exclusion 2: Extremely short stays (same-day discharge, < 1 day)
# Already handled by minimum LOS criterion above

# Exclusion 3: Missing critical data (we'll check for discharge summaries later)

print(f"\n✓ Final cohort after exclusions: {len(cohort)}")


# Cell 6: Create Readmission Labels
"""
Define 30-day unplanned readmission outcome
"""
print("\n" + "="*60)
print("CREATING READMISSION LABELS")
print("="*60)

# Sort by patient and admission time
cohort = cohort.sort_values(['SUBJECT_ID', 'ADMITTIME']).reset_index(drop=True)

# For each admission, find the next admission time for same patient
cohort['NEXT_ADMITTIME'] = cohort.groupby('SUBJECT_ID')['ADMITTIME'].shift(-1)
cohort['NEXT_ADMISSION_TYPE'] = cohort.groupby('SUBJECT_ID')['ADMISSION_TYPE'].shift(-1)

# Calculate days until next admission
cohort['DAYS_TO_NEXT_ADMIT'] = (
    cohort['NEXT_ADMITTIME'] - cohort['DISCHTIME']
).dt.total_seconds() / 86400

# Define 30-day readmission
readmission_window = config['data']['cohort']['readmission_window_days']
cohort['TARGET_READMIT_30'] = (
    (cohort['DAYS_TO_NEXT_ADMIT'] <= readmission_window) &
    (cohort['DAYS_TO_NEXT_ADMIT'] > 0)
)

# Count readmissions
n_readmissions = cohort['TARGET_READMIT_30'].sum()
readmission_rate = cohort['TARGET_READMIT_30'].mean()

print(f"Readmission window: {readmission_window} days")
print(f"Total admissions in cohort: {len(cohort)}")
print(f"30-day readmissions: {n_readmissions}")
print(f"Readmission rate: {readmission_rate*100:.2f}%")

# Handle last admission per patient (no readmission possible to observe)
cohort['IS_LAST_ADMISSION'] = cohort['NEXT_ADMITTIME'].isna()
n_last_admissions = cohort['IS_LAST_ADMISSION'].sum()
print(f"\nLast admissions per patient (censored): {n_last_admissions}")
print(f"Evaluable admissions: {len(cohort) - n_last_admissions}")


# Cell 7: Check for Discharge Summaries
"""
Verify availability of discharge summaries for text embeddings
"""
print("\n" + "="*60)
print("CHECKING DISCHARGE SUMMARY AVAILABILITY")
print("="*60)

# Filter to discharge summaries
discharge_notes = notes[notes['CATEGORY'] == 'Discharge summary'].copy()
print(f"Total discharge summaries in database: {len(discharge_notes)}")

# Merge with cohort to check coverage
cohort = cohort.merge(
    discharge_notes[['HADM_ID', 'TEXT']].rename(columns={'TEXT': 'DISCHARGE_TEXT'}),
    on='HADM_ID',
    how='left'
)

# Check availability
cohort['HAS_DISCHARGE_NOTE'] = cohort['DISCHARGE_TEXT'].notna()
n_with_notes = cohort['HAS_DISCHARGE_NOTE'].sum()
note_coverage = n_with_notes / len(cohort)

print(f"Admissions with discharge summaries: {n_with_notes}/{len(cohort)} ({note_coverage*100:.1f}%)")

# Optionally exclude admissions without notes
if note_coverage < 0.5:
    print("\n⚠️  Warning: Low discharge summary coverage!")
    print("Consider using alternative note types or working with available subset")
else:
    print("\n✓ Good discharge summary coverage")

# For now, keep all admissions but flag availability
print(f"\nProceeding with all {len(cohort)} admissions")
print("(Admissions without notes will use structured features only)")


# Cell 8: Cohort Characteristics
"""
Analyze final cohort characteristics
"""
print("\n" + "="*60)
print("FINAL COHORT CHARACTERISTICS")
print("="*60)

print("\n1. Demographics")
print("-" * 40)
print(f"Mean age: {cohort['AGE'].mean():.1f} ± {cohort['AGE'].std():.1f} years")
print(f"Age range: {cohort['AGE'].min():.1f} - {cohort['AGE'].max():.1f}")
print("\nGender distribution:")
print(cohort['GENDER'].value_counts())
print("\nTop 5 ethnicities:")
print(cohort['ETHNICITY'].value_counts().head())

print("\n2. Admission Characteristics")
print("-" * 40)
print(f"Mean LOS: {cohort['LOS_DAYS'].mean():.1f} ± {cohort['LOS_DAYS'].std():.1f} days")
print(f"Median LOS: {cohort['LOS_DAYS'].median():.1f} days")
print("\nAdmission types:")
print(cohort['ADMISSION_TYPE'].value_counts())
print("\nAdmission locations:")
print(cohort['ADMISSION_LOCATION'].value_counts())

print("\n3. Discharge Characteristics")
print("-" * 40)
print("Discharge locations:")
print(cohort['DISCHARGE_LOCATION'].value_counts())
print("\nInsurance types:")
print(cohort['INSURANCE'].value_counts())

print("\n4. Readmission Outcomes")
print("-" * 40)
print(f"Total evaluable admissions: {(~cohort['IS_LAST_ADMISSION']).sum()}")
print(f"30-day readmissions: {n_readmissions}")
print(f"Readmission rate: {readmission_rate*100:.2f}%")
print(f"With discharge notes: {n_with_notes} ({note_coverage*100:.1f}%)")


# Cell 9: Visualizations
"""
Create visualization of cohort characteristics
"""
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Cohort Characteristics', fontsize=16, fontweight='bold')

# 1. Age distribution
axes[0, 0].hist(cohort['AGE'], bins=20, edgecolor='black', alpha=0.7)
axes[0, 0].axvline(cohort['AGE'].mean(), color='red', linestyle='--', label=f'Mean: {cohort["AGE"].mean():.1f}')
axes[0, 0].set_xlabel('Age (years)')
axes[0, 0].set_ylabel('Count')
axes[0, 0].set_title('Age Distribution')
axes[0, 0].legend()

# 2. Length of Stay distribution
axes[0, 1].hist(cohort['LOS_DAYS'], bins=30, edgecolor='black', alpha=0.7)
axes[0, 1].axvline(cohort['LOS_DAYS'].mean(), color='red', linestyle='--', label=f'Mean: {cohort["LOS_DAYS"].mean():.1f}')
axes[0, 1].set_xlabel('Length of Stay (days)')
axes[0, 1].set_ylabel('Count')
axes[0, 1].set_title('Length of Stay Distribution')
axes[0, 1].legend()
axes[0, 1].set_xlim(0, min(30, cohort['LOS_DAYS'].max()))

# 3. Gender distribution
gender_counts = cohort['GENDER'].value_counts()
axes[0, 2].bar(gender_counts.index, gender_counts.values, edgecolor='black', alpha=0.7)
axes[0, 2].set_xlabel('Gender')
axes[0, 2].set_ylabel('Count')
axes[0, 2].set_title('Gender Distribution')

# 4. Readmission outcome
readmit_counts = cohort[~cohort['IS_LAST_ADMISSION']]['TARGET_READMIT_30'].value_counts()
axes[1, 0].bar(['No Readmit', 'Readmit'],
               [readmit_counts.get(False, 0), readmit_counts.get(True, 0)],
               color=['green', 'red'], alpha=0.7, edgecolor='black')
axes[1, 0].set_ylabel('Count')
axes[1, 0].set_title(f'30-Day Readmission\n(Rate: {readmission_rate*100:.1f}%)')
for i, v in enumerate([readmit_counts.get(False, 0), readmit_counts.get(True, 0)]):
    axes[1, 0].text(i, v + 1, str(v), ha='center', fontweight='bold')

# 5. Days to readmission (for readmitted patients)
readmitted = cohort[(cohort['TARGET_READMIT_30'] == True) & (cohort['DAYS_TO_NEXT_ADMIT'].notna())]
if len(readmitted) > 0:
    axes[1, 1].hist(readmitted['DAYS_TO_NEXT_ADMIT'], bins=15, edgecolor='black', alpha=0.7, color='orange')
    axes[1, 1].set_xlabel('Days to Readmission')
    axes[1, 1].set_ylabel('Count')
    axes[1, 1].set_title('Time to Readmission')
    axes[1, 1].axvline(readmitted['DAYS_TO_NEXT_ADMIT'].mean(), color='red', linestyle='--',
                      label=f'Mean: {readmitted["DAYS_TO_NEXT_ADMIT"].mean():.1f} days')
    axes[1, 1].legend()
else:
    axes[1, 1].text(0.5, 0.5, 'No readmissions', ha='center', va='center', transform=axes[1, 1].transAxes)
    axes[1, 1].set_title('Time to Readmission')

# 6. Discharge note availability
note_avail = cohort['HAS_DISCHARGE_NOTE'].value_counts()
axes[1, 2].bar(['No Note', 'Has Note'],
               [note_avail.get(False, 0), note_avail.get(True, 0)],
               color=['lightcoral', 'lightgreen'], alpha=0.7, edgecolor='black')
axes[1, 2].set_ylabel('Count')
axes[1, 2].set_title(f'Discharge Note Availability\n({note_coverage*100:.1f}% coverage)')

plt.tight_layout()
plt.savefig('outputs/figures/cohort_characteristics.png', dpi=300, bbox_inches='tight')
print("\n✓ Visualization saved to outputs/figures/cohort_characteristics.png")
plt.show()


# Cell 10: Save Cohort Dataset
"""
Save the final cohort with readmission labels
"""
print("\n" + "="*60)
print("SAVING COHORT DATASET")
print("="*60)

# Select relevant columns for final cohort
cohort_columns = [
    'HADM_ID', 'SUBJECT_ID', 'ADMITTIME', 'DISCHTIME',
    'ADMISSION_TYPE', 'ADMISSION_LOCATION', 'DISCHARGE_LOCATION',
    'INSURANCE', 'LANGUAGE', 'RELIGION', 'MARITAL_STATUS', 'ETHNICITY',
    'AGE', 'GENDER', 'LOS_DAYS',
    'TARGET_READMIT_30', 'DAYS_TO_NEXT_ADMIT', 'IS_LAST_ADMISSION',
    'HAS_DISCHARGE_NOTE', 'DISCHARGE_TEXT'
]

final_cohort = cohort[cohort_columns].copy()

# Save to processed data folder
output_path = Path('data/processed/cohort_with_outcomes.parquet')
output_path.parent.mkdir(parents=True, exist_ok=True)
final_cohort.to_parquet(output_path, index=False)

print(f"✓ Saved cohort to: {output_path}")
print(f"   Shape: {final_cohort.shape}")
print(f"   Size: {output_path.stat().st_size / 1024:.1f} KB")

# Also save as CSV for easy inspection
csv_path = output_path.with_suffix('.csv')
final_cohort.to_csv(csv_path, index=False)
print(f"✓ Also saved as CSV: {csv_path}")


# Cell 11: Generate Cohort Summary Report
"""
Create detailed summary report
"""
summary_report = {
    "cohort_definition": {
        "inclusion_criteria": [
            f"Age >= {min_age} years",
            f"Length of stay >= {min_los_hours} hours",
            "Non-elective admission (emergency/urgent)",
            "Has discharge time"
        ],
        "exclusion_criteria": [
            "In-hospital mortality",
            "Missing discharge time"
        ]
    },
    "cohort_size": {
        "initial_admissions": int(initial_count),
        "after_inclusion": int(before_exclusion),
        "after_exclusion": int(len(final_cohort)),
        "evaluable_for_readmission": int((~final_cohort['IS_LAST_ADMISSION']).sum())
    },
    "demographics": {
        "mean_age": float(final_cohort['AGE'].mean()),
        "std_age": float(final_cohort['AGE'].std()),
        "age_range": [float(final_cohort['AGE'].min()), float(final_cohort['AGE'].max())],
        "gender_distribution": final_cohort['GENDER'].value_counts().to_dict(),
        "top_ethnicities": final_cohort['ETHNICITY'].value_counts().head(5).to_dict()
    },
    "admission_characteristics": {
        "mean_los_days": float(final_cohort['LOS_DAYS'].mean()),
        "median_los_days": float(final_cohort['LOS_DAYS'].median()),
        "std_los_days": float(final_cohort['LOS_DAYS'].std()),
        "admission_types": final_cohort['ADMISSION_TYPE'].value_counts().to_dict()
    },
    "outcomes": {
        "n_readmissions": int(n_readmissions),
        "readmission_rate": float(readmission_rate),
        "mean_days_to_readmit": float(readmitted['DAYS_TO_NEXT_ADMIT'].mean()) if len(readmitted) > 0 else None
    },
    "data_availability": {
        "n_with_discharge_notes": int(n_with_notes),
        "discharge_note_coverage": float(note_coverage)
    }
}

report_path = Path('outputs/results/cohort_summary.json')
with open(report_path, 'w') as f:
    json.dump(summary_report, f, indent=2)

print(f"\n✓ Summary report saved to: {report_path}")


# Cell 12: Next Steps
print("\n" + "="*60)
print("COHORT DEFINITION COMPLETE!")
print("="*60)
print(f"\n✅ Final cohort: {len(final_cohort)} admissions")
print(f"✅ Readmission rate: {readmission_rate*100:.2f}%")
print(f"✅ Discharge note coverage: {note_coverage*100:.1f}%")
print(f"\n📊 Outputs saved:")
print(f"   - Cohort data: data/processed/cohort_with_outcomes.parquet")
print(f"   - Visualization: outputs/figures/cohort_characteristics.png")
print(f"   - Summary: outputs/results/cohort_summary.json")
print(f"\n📝 Next Steps:")
print("   1. Review cohort characteristics above")
print("   2. Proceed to notebook 03_feature_engineering.ipynb")
print("   3. Extract structured features for modeling")



"""
Notebook: 03_feature_engineering.ipynb
Engineer structured features from MIMIC-III data for readmission prediction
"""

# Cell 1: Setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Load config
with open('configs/config.json', 'r') as f:
    config = json.load(f)

data_dir = Path('data/raw')
processed_dir = Path('data/processed')

print("✓ Setup complete")


# Cell 2: Load Cohort and Supporting Tables
"""
Load cohort and all tables needed for feature engineering
"""
print("Loading data...")

# Load our cohort
cohort = pd.read_parquet(processed_dir / 'cohort_with_outcomes.parquet')
print(f"✓ Loaded cohort: {len(cohort)} admissions")

# Load supporting tables
diagnoses = pd.read_csv(data_dir / 'DIAGNOSES_ICD.csv')
procedures = pd.read_csv(data_dir / 'PROCEDURES_ICD.csv')
prescriptions = pd.read_csv(data_dir / 'PRESCRIPTIONS.csv')
labevents = pd.read_csv(data_dir / 'LABEVENTS.csv')
icustays = pd.read_csv(data_dir / 'ICUSTAYS.csv')

# Load diagnosis/procedure lookup tables
d_icd_diagnoses = pd.read_csv(data_dir / 'D_ICD_DIAGNOSES.csv')
d_labitems = pd.read_csv(data_dir / 'D_LABITEMS.csv')

print(f"✓ Loaded {len(diagnoses)} diagnoses")
print(f"✓ Loaded {len(procedures)} procedures")
print(f"✓ Loaded {len(prescriptions)} prescriptions")
print(f"✓ Loaded {len(labevents)} lab events")
print(f"✓ Loaded {len(icustays)} ICU stays")

# Cell 3: Feature Set 1 - Demographics
"""
Extract demographic features
"""
print("\n" + "="*60)
print("FEATURE SET 1: DEMOGRAPHICS")
print("="*60)

features = cohort[['HADM_ID', 'SUBJECT_ID']].copy()

# Age (already calculated in cohort)
features['age'] = cohort['AGE']

# Age bins
features['age_group'] = pd.cut(cohort['AGE'],
                               bins=[0, 40, 65, 80, 150],
                               labels=['18-40', '41-65', '66-80', '80+'])
features['age_group'] = features['age_group'].astype(str)

# Gender (binary encode)
features['gender_M'] = (cohort['GENDER'] == 'M').astype(int)
features['gender_F'] = (cohort['GENDER'] == 'F').astype(int)

# Ethnicity (group into major categories)
def categorize_ethnicity(eth):
    if pd.isna(eth):
        return 'UNKNOWN'
    eth = eth.upper()
    if 'WHITE' in eth:
        return 'WHITE'
    elif 'BLACK' in eth or 'AFRICAN' in eth:
        return 'BLACK'
    elif 'HISPANIC' in eth or 'LATINO' in eth:
        return 'HISPANIC'
    elif 'ASIAN' in eth:
        return 'ASIAN'
    else:
        return 'OTHER'

features['ethnicity_category'] = cohort['ETHNICITY'].apply(categorize_ethnicity)
ethnicity_dummies = pd.get_dummies(features['ethnicity_category'], prefix='ethnicity')
features = pd.concat([features, ethnicity_dummies], axis=1)

# Insurance type
insurance_dummies = pd.get_dummies(cohort['INSURANCE'], prefix='insurance')
features = pd.concat([features, insurance_dummies], axis=1)

# Marital status
marital_dummies = pd.get_dummies(cohort['MARITAL_STATUS'].fillna('UNKNOWN'), prefix='marital')
features = pd.concat([features, marital_dummies], axis=1)

print(f"✓ Demographic features: {len([c for c in features.columns if c not in ['HADM_ID', 'SUBJECT_ID']])}")
print(f"   - Age (continuous + binned)")
print(f"   - Gender (2 features)")
print(f"   - Ethnicity ({len(ethnicity_dummies.columns)} categories)")
print(f"   - Insurance ({len(insurance_dummies.columns)} types)")
print(f"   - Marital status ({len(marital_dummies.columns)} categories)")


# Cell 4: Feature Set 2 - Admission Characteristics
"""
Admission-related features
"""
print("\n" + "="*60)
print("FEATURE SET 2: ADMISSION CHARACTERISTICS")
print("="*60)

# Length of stay
features['los_days'] = cohort['LOS_DAYS']
features['los_hours'] = cohort['LOS_DAYS'] * 24

# LOS categories
features['los_category'] = pd.cut(cohort['LOS_DAYS'],
                                  bins=[0, 2, 5, 10, 999],
                                  labels=['short', 'medium', 'long', 'very_long'])
los_dummies = pd.get_dummies(features['los_category'], prefix='los')
features = pd.concat([features, los_dummies], axis=1)

# Admission type (already filtered to EMERGENCY/URGENT)
admission_type_dummies = pd.get_dummies(cohort['ADMISSION_TYPE'], prefix='admission_type')
features = pd.concat([features, admission_type_dummies], axis=1)

# Admission location
admission_loc_dummies = pd.get_dummies(cohort['ADMISSION_LOCATION'].fillna('UNKNOWN'), prefix='admit_loc')
features = pd.concat([features, admission_loc_dummies], axis=1)

# Discharge location (important predictor)
discharge_loc_dummies = pd.get_dummies(cohort['DISCHARGE_LOCATION'].fillna('UNKNOWN'), prefix='discharge_loc')
features = pd.concat([features, discharge_loc_dummies], axis=1)

# Temporal features
features['admit_hour'] = pd.to_datetime(cohort['ADMITTIME']).dt.hour
features['admit_dayofweek'] = pd.to_datetime(cohort['ADMITTIME']).dt.dayofweek
features['admit_weekend'] = (features['admit_dayofweek'] >= 5).astype(int)

features['discharge_hour'] = pd.to_datetime(cohort['DISCHTIME']).dt.hour
features['discharge_dayofweek'] = pd.to_datetime(cohort['DISCHTIME']).dt.dayofweek
features['discharge_weekend'] = (features['discharge_dayofweek'] >= 5).astype(int)

# Season
features['discharge_month'] = pd.to_datetime(cohort['DISCHTIME']).dt.month
features['discharge_season'] = features['discharge_month'].apply(
    lambda m: 'winter' if m in [12, 1, 2] else
              'spring' if m in [3, 4, 5] else
              'summer' if m in [6, 7, 8] else 'fall'
)
season_dummies = pd.get_dummies(features['discharge_season'], prefix='season')
features = pd.concat([features, season_dummies], axis=1)

print(f"✓ Admission features: {12 + len(los_dummies.columns) + len(admission_type_dummies.columns) + len(admission_loc_dummies.columns) + len(discharge_loc_dummies.columns) + len(season_dummies.columns)}")


# Cell 5: Feature Set 3 - Diagnoses & Comorbidities
"""
Diagnosis-based features including comorbidity scores
"""
print("\n" + "="*60)
print("FEATURE SET 3: DIAGNOSES & COMORBIDITIES")
print("="*60)

# Ensure uppercase columns
diagnoses.columns = diagnoses.columns.str.upper()
d_icd_diagnoses.columns = d_icd_diagnoses.columns.str.upper()
features.columns = features.columns.str.upper()

# Merge diagnoses with descriptions
diagnoses_full = diagnoses.merge(
    d_icd_diagnoses[['ICD9_CODE', 'LONG_TITLE']],
    on='ICD9_CODE',
    how='left'
)

# Count total diagnoses per admission
diag_counts = diagnoses.groupby('HADM_ID').size().reset_index(name='N_DIAGNOSES')
features = features.merge(diag_counts, on='HADM_ID', how='left')
features['N_DIAGNOSES'] = features['N_DIAGNOSES'].fillna(0)

# Simplified Charlson Comorbidity Index
def get_charlson_score(hadm_id):
    """Calculate simplified Charlson score based on ICD-9 codes"""
    hadm_diagnoses = diagnoses[diagnoses['HADM_ID'] == hadm_id]['ICD9_CODE'].astype(str).tolist()
    score = 0
    conditions = {
        'MI': ['410', '412'],
        'CHF': ['428'],
        'PVD': ['441', '443', '785.4'],
        'CVD': ['430', '431', '432', '433', '434', '435', '436', '437', '438'],
        'Dementia': ['290'],
        'COPD': ['490', '491', '492', '493', '494', '495', '496', '500', '501', '502', '503', '504', '505'],
        'Diabetes': ['250'],
        'Renal': ['582', '583', '585', '586', '588'],
        'Liver': ['571'],
        'Cancer': ['14', '15', '16', '17', '18', '19', '20']
    }
    for condition, codes in conditions.items():
        for code in codes:
            if any(diag.startswith(code) for diag in hadm_diagnoses):
                score += 1
                break
    return score

# Calculate Charlson for each admission
print("Calculating Charlson comorbidity scores...")
features['CHARLSON_SCORE'] = features['HADM_ID'].apply(get_charlson_score)

# High-risk condition flags
def has_condition(hadm_id, code_prefixes):
    """Check if admission has any diagnosis starting with given code prefixes"""
    hadm_diagnoses = diagnoses[diagnoses['HADM_ID'] == hadm_id]['ICD9_CODE'].astype(str).tolist()
    return int(any(any(diag.startswith(code) for code in code_prefixes) for diag in hadm_diagnoses))

print("Flagging high-risk conditions...")
features['DX_HEART_FAILURE'] = features['HADM_ID'].apply(lambda x: has_condition(x, ['428']))
features['DX_COPD'] = features['HADM_ID'].apply(lambda x: has_condition(x, ['490', '491', '492', '493', '494', '496']))
features['DX_DIABETES'] = features['HADM_ID'].apply(lambda x: has_condition(x, ['250']))
features['DX_RENAL_FAILURE'] = features['HADM_ID'].apply(lambda x: has_condition(x, ['584', '585', '586']))
features['DX_CANCER'] = features['HADM_ID'].apply(lambda x: has_condition(x, ['14', '15', '16', '17', '18', '19', '20']))
features['DX_SEPSIS'] = features['HADM_ID'].apply(lambda x: has_condition(x, ['038', '995.91', '995.92']))

print(f"✓ Diagnosis features: 7 (count + Charlson + 5 high-risk flags)")
print(f"   Mean Charlson score: {features['CHARLSON_SCORE'].mean():.2f}")

# Cell 6: Feature Set 4 - Procedures
"""
Procedure-based features
"""
print("\n" + "="*60)
print("FEATURE SET 4: PROCEDURES")
print("="*60)

# Ensure uppercase columns
procedures.columns = procedures.columns.str.upper()
features.columns = features.columns.str.upper()

# Count procedures per admission
proc_counts = procedures.groupby('HADM_ID').size().reset_index(name='N_PROCEDURES')
features = features.merge(proc_counts, on='HADM_ID', how='left')
features['N_PROCEDURES'] = features['N_PROCEDURES'].fillna(0)

# Major procedure flags (simplified)
features['PROC_SURGERY'] = features['HADM_ID'].apply(
    lambda x: int(len(procedures[procedures['HADM_ID'] == x]) > 0)
)

print(f"✓ Procedure features: 2 (count + surgery flag)")

# Cell 7: Feature Set 5 - ICU Utilization
"""
ICU stay features
"""
print("\n" + "="*60)
print("FEATURE SET 5: ICU UTILIZATION")
print("="*60)

# Uppercase ICU columns
icustays.columns = icustays.columns.str.upper()
features.columns = features.columns.str.upper()

# Convert ICU times
icustays['INTIME'] = pd.to_datetime(icustays['INTIME'], errors='coerce')
icustays['OUTTIME'] = pd.to_datetime(icustays['OUTTIME'], errors='coerce')

# ICU length of stay (days)
icustays['ICU_LOS'] = (icustays['OUTTIME'] - icustays['INTIME']).dt.total_seconds() / 86400

# ICU stay summary per admission
icu_patients = icustays.groupby('HADM_ID').agg({
    'ICUSTAY_ID': 'count',
    'ICU_LOS': 'sum'
}).reset_index()
icu_patients.columns = ['HADM_ID', 'N_ICU_STAYS', 'TOTAL_ICU_DAYS']

# Merge with features
features = features.merge(icu_patients, on='HADM_ID', how='left')
features['N_ICU_STAYS'] = features['N_ICU_STAYS'].fillna(0)
features['TOTAL_ICU_DAYS'] = features['TOTAL_ICU_DAYS'].fillna(0)
features['HAD_ICU_STAY'] = (features['N_ICU_STAYS'] > 0).astype(int)

print(f"✓ ICU features: 3 (count, days, flag)")
print(f"   Admissions with ICU: {features['HAD_ICU_STAY'].sum()} ({features['HAD_ICU_STAY'].mean()*100:.1f}%)")

# Cell 8: Feature Set 6 - Medications
"""
Medication-based features
"""
print("\n" + "="*60)
print("FEATURE SET 6: MEDICATIONS")
print("="*60)

# Ensure uppercase columns
prescriptions.columns = prescriptions.columns.str.upper()

# Count unique medications per admission
med_counts = prescriptions.groupby('HADM_ID')['DRUG'].nunique().reset_index(name='n_medications')
features = features.merge(med_counts, on='HADM_ID', how='left')
features['n_medications'] = features['n_medications'].fillna(0)

# High-risk medication classes (simplified keyword search)
def has_medication_class(hadm_id, keywords):
    """Check if admission has medications matching keywords"""
    hadm_meds = prescriptions[prescriptions['HADM_ID'] == hadm_id]['DRUG'].astype(str).str.upper().tolist()
    return int(any(any(keyword in med for keyword in keywords) for med in hadm_meds))

print("Checking medication classes...")
features['med_anticoagulant'] = features['HADM_ID'].apply(
    lambda x: has_medication_class(x, ['WARFARIN', 'HEPARIN', 'COUMADIN'])
)
features['med_diuretic'] = features['HADM_ID'].apply(
    lambda x: has_medication_class(x, ['FUROSEMIDE', 'LASIX'])
)
features['med_insulin'] = features['HADM_ID'].apply(
    lambda x: has_medication_class(x, ['INSULIN'])
)

print(f"✓ Medication features: 4 (count + 3 drug class flags)")


# Cell 9: Feature Set 7 - Laboratory Values
"""
Lab values - last measurements before discharge
"""
print("\n" + "="*60)
print("FEATURE SET 7: LABORATORY VALUES")
print("="*60)

# Ensure uppercase columns
labevents.columns = labevents.columns.str.upper()
d_labitems.columns = d_labitems.columns.str.upper()

# Convert lab times
labevents['CHARTTIME'] = pd.to_datetime(labevents['CHARTTIME'])

# Merge with cohort to get discharge times
labs_with_discharge = labevents.merge(
    cohort[['HADM_ID', 'DISCHTIME']],
    on='HADM_ID',
    how='inner'
)

# Keep only labs before discharge
labs_with_discharge = labs_with_discharge[
    labs_with_discharge['CHARTTIME'] <= labs_with_discharge['DISCHTIME']
]

# Get lab item names
labs_with_discharge = labs_with_discharge.merge(
    d_labitems[['ITEMID', 'LABEL']],
    on='ITEMID',
    how='left'
)

# Define key lab values to extract
key_labs = {
    'Creatinine': ['CREATININE'],
    'Hemoglobin': ['HEMOGLOBIN', 'HGB'],
    'Platelet': ['PLATELET'],
    'WBC': ['WBC', 'WHITE BLOOD CELLS'],
    'Sodium': ['SODIUM'],
    'Potassium': ['POTASSIUM'],
    'Glucose': ['GLUCOSE']
}

# Extract last value for each key lab per admission
for lab_name, keywords in key_labs.items():
    print(f"Extracting {lab_name}...")

    # Filter to relevant lab
    lab_data = labs_with_discharge[
        labs_with_discharge['LABEL'].str.upper().str.contains('|'.join(keywords), na=False)
    ].copy()

    # Get last value per admission
    last_values = lab_data.sort_values('CHARTTIME').groupby('HADM_ID').last()['VALUENUM'].reset_index()
    last_values.columns = ['HADM_ID', f'lab_{lab_name.lower()}_last']

    features = features.merge(last_values, on='HADM_ID', how='left')

# Count of lab measurements (indicator of monitoring intensity)
lab_intensity = labevents.groupby('HADM_ID').size().reset_index(name='n_lab_measurements')
features = features.merge(lab_intensity, on='HADM_ID', how='left')
features['n_lab_measurements'] = features['n_lab_measurements'].fillna(0)

print(f"✓ Lab features: {len(key_labs) + 1}")


# Cell 10: Feature Set 8 - Utilization History
"""
Prior utilization patterns - strong readmission predictors
"""
print("\n" + "="*60)
print("FEATURE SET 8: UTILIZATION HISTORY")
print("="*60)

# Read all admissions first
all_admissions = pd.read_csv(data_dir / 'ADMISSIONS.csv')

# Uppercase columns
all_admissions.columns = all_admissions.columns.str.upper()
cohort.columns = cohort.columns.str.upper()
features.columns = features.columns.str.upper()

# Ensure datetime
all_admissions['ADMITTIME'] = pd.to_datetime(all_admissions['ADMITTIME'], errors='coerce')
all_admissions['DISCHTIME'] = pd.to_datetime(all_admissions['DISCHTIME'], errors='coerce')

# For each admission in cohort, count prior admissions
def count_prior_admissions(row, window_days):
    """Count admissions in the window_days before this admission"""
    subject_id = row['SUBJECT_ID']
    admit_time = row['ADMITTIME']

    # Get all prior admissions for this patient
    prior = all_admissions[
        (all_admissions['SUBJECT_ID'] == subject_id) &
        (all_admissions['DISCHTIME'] < admit_time) &
        (all_admissions['DISCHTIME'] >= admit_time - pd.Timedelta(days=window_days))
    ]

    return len(prior)

print("Calculating prior admission history...")
features['PRIOR_ADMISSIONS_90D'] = cohort.apply(lambda row: count_prior_admissions(row, 90), axis=1)
features['PRIOR_ADMISSIONS_180D'] = cohort.apply(lambda row: count_prior_admissions(row, 180), axis=1)
features['PRIOR_ADMISSIONS_365D'] = cohort.apply(lambda row: count_prior_admissions(row, 365), axis=1)

# Frequent flyer flag (3+ admissions in 6 months)
features['FREQUENT_FLYER'] = (features['PRIOR_ADMISSIONS_180D'] >= 3).astype(int)

print(f"✓ Utilization history features: 4")
print(f"   Frequent flyers: {features['FREQUENT_FLYER'].sum()} ({features['FREQUENT_FLYER'].mean()*100:.1f}%)")

# Cell 11: Handle Missing Values
"""
Implement missing value strategy
"""
print("\n" + "="*60)
print("HANDLING MISSING VALUES")
print("="*60)

# Check missing values
missing_summary = features.isnull().sum()
missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)

print("Features with missing values:")
print(missing_summary)

# Strategy: Create missingness indicators for labs, then impute with median
lab_cols = [col for col in features.columns if col.startswith('lab_')]

for col in lab_cols:
    # Create missingness indicator
    features[f'{col}_missing'] = features[col].isnull().astype(int)

    # Impute with median (only if there are non-null values)
    if features[col].notna().sum() > 0:
        median_val = features[col].median()
        features[col] = features[col].fillna(median_val)
    else:
        features[col] = features[col].fillna(0)

print(f"\n✓ Created {len(lab_cols)} missingness indicators")
print("✓ Imputed missing lab values with median")

# Drop any remaining text/categorical columns used for intermediate processing
columns_to_drop = ['AGE_GROUP', 'ETHNICITY_CATEGORY', 'LOS_CATEGORY', 'DISCHARGE_SEASON']
features = features.drop([col for col in columns_to_drop if col in features.columns], axis=1, errors='ignore')

# Add verification:
object_cols = features.select_dtypes(include=['object']).columns.tolist()
if object_cols:
    print(f"⚠️ Warning: Object columns still present: {object_cols}")
    cols_to_keep = ['HADM_ID', 'SUBJECT_ID']
    features = features.drop([col for col in object_cols if col not in cols_to_keep], axis=1, errors='ignore')
else:
    print("✓ All categorical columns successfully removed")


# Cell 12: Add Target Variable and Create Final Dataset
"""
Merge with outcomes and create final feature matrix
"""
print("\n" + "="*60)
print("CREATING FINAL FEATURE MATRIX")
print("="*60)

# Add target variable
features['target_readmit_30'] = cohort['TARGET_READMIT_30'].astype(int)
features['is_last_admission'] = cohort['IS_LAST_ADMISSION'].astype(int)

# Add text availability flag
features['has_discharge_note'] = cohort['HAS_DISCHARGE_NOTE'].astype(int)

# Separate feature columns from metadata
metadata_cols = ['HADM_ID', 'SUBJECT_ID', 'target_readmit_30', 'is_last_admission', 'has_discharge_note']
feature_cols = [col for col in features.columns if col not in metadata_cols]

print(f"Total engineered features: {len(feature_cols)}")
print(f"Metadata columns: {len(metadata_cols)}")
print(f"Total columns: {len(features.columns)}")

# Check for any remaining missing values
remaining_missing = features[feature_cols].isnull().sum().sum()
if remaining_missing > 0:
    print(f"\n⚠️  Warning: {remaining_missing} missing values remaining")
    print("Filling with 0...")
    features[feature_cols] = features[feature_cols].fillna(0)
else:
    print("\n✓ No missing values in features")


# Cell 13: Feature Statistics and Importance Preview
"""
Analyze feature distributions
"""
print("\n" + "="*60)
print("FEATURE STATISTICS")
print("="*60)

# Ensure column names are uppercase
features.columns = features.columns.str.upper()

# Numeric feature summary
numeric_features = features[feature_cols].select_dtypes(include=[np.number])
print(f"\nNumeric features: {len(numeric_features.columns)}")
print("\nSample statistics:")
print(numeric_features.describe().T[['mean', 'std', 'min', 'max']].head(10))

# Check for constant features (zero variance)
constant_features = [col for col in numeric_features.columns if features[col].nunique() <= 1]
if constant_features:
    print(f"\n⚠️  Warning: {len(constant_features)} constant features found:")
    print(constant_features[:10])
    print("Consider removing these in modeling")

# Check feature correlations with target (for evaluable admissions only)
if 'IS_LAST_ADMISSION' in features.columns and 'TARGET_READMIT_30' in features.columns:
    evaluable = features[features['IS_LAST_ADMISSION'] == 0].copy()
    numeric_evaluable = evaluable[numeric_features.columns.tolist() + ['TARGET_READMIT_30']].select_dtypes(include=[np.number])
    if len(numeric_evaluable) > 0:
        print(f"\nTop 10 features correlated with readmission:")
        correlations = numeric_evaluable.corr()['TARGET_READMIT_30'].abs().sort_values(ascending=False)[1:11]
        for feat, corr in correlations.items():
            print(f"  {feat:40s}: {corr:.3f}")

# Cell 14: Create Train/Calibration/Test Splits
"""
Split data temporally to prevent leakage
"""
print("\n" + "="*60)
print("CREATING DATA SPLITS")
print("="*60)

# Ensure cohort and features have consistent column names
cohort.columns = cohort.columns.str.upper()
features.columns = features.columns.str.upper()

# Mark last admission per patient
last_admissions = cohort.groupby('SUBJECT_ID')['DISCHTIME'].idxmax()
features['IS_LAST_ADMISSION'] = 0
features.loc[last_admissions, 'IS_LAST_ADMISSION'] = 1

# Temporal split based on discharge time
cohort_with_time = cohort[['HADM_ID', 'DISCHTIME']].copy()
features_with_time = features.merge(cohort_with_time, on='HADM_ID')
features_with_time = features_with_time.sort_values('DISCHTIME')

# Remove last admissions (can't evaluate outcome)
evaluable_data = features_with_time[features_with_time['IS_LAST_ADMISSION'] == 0].copy()

print(f"Total admissions: {len(features)}")
print(f"Evaluable admissions (excluding last per patient): {len(evaluable_data)}")

# Split: 70% train, 10% calibration, 20% test (temporal)
n_total = len(evaluable_data)
n_train = int(n_total * 0.7)
n_calib = int(n_total * 0.1)

train_data = evaluable_data.iloc[:n_train]
calib_data = evaluable_data.iloc[n_train:n_train+n_calib]
test_data = evaluable_data.iloc[n_train+n_calib:]

print(f"\nSplit sizes:")
print(f"  Training:    {len(train_data):4d} ({len(train_data)/n_total*100:.1f}%)")
print(f"  Calibration: {len(calib_data):4d} ({len(calib_data)/n_total*100:.1f}%)")
print(f"  Test:        {len(test_data):4d} ({len(test_data)/n_total*100:.1f}%)")

# Check readmission rates in each split
print(f"\nReadmission rates:")
print(f"  Training:    {train_data['TARGET_READMIT_30'].mean()*100:.2f}%")
print(f"  Calibration: {calib_data['TARGET_READMIT_30'].mean()*100:.2f}%")
print(f"  Test:        {test_data['TARGET_READMIT_30'].mean()*100:.2f}%")

# Cell 15: Save Datasets
"""
Save processed datasets for modeling
"""
print("\n" + "="*60)
print("SAVING PROCESSED DATASETS")
print("="*60)

# Drop DISCHTIME before saving
train_data = train_data.drop('DISCHTIME', axis=1)
calib_data = calib_data.drop('DISCHTIME', axis=1)
test_data = test_data.drop('DISCHTIME', axis=1)

# Save as parquet (efficient storage)
train_data.to_parquet(processed_dir / 'train_features.parquet', index=False)
calib_data.to_parquet(processed_dir / 'calibration_features.parquet', index=False)
test_data.to_parquet(processed_dir / 'test_features.parquet', index=False)

print(f"✓ Saved training set: {train_data.shape}")
print(f"✓ Saved calibration set: {calib_data.shape}")
print(f"✓ Saved test set: {test_data.shape}")

# Also save feature names for reference
feature_info = {
    'n_features': len(feature_cols),
    'feature_names': feature_cols,
    'metadata_cols': metadata_cols,
    'target_variable': 'target_readmit_30'
}

with open(processed_dir / 'feature_info.json', 'w') as f:
    json.dump(feature_info, f, indent=2)

print(f"✓ Saved feature metadata")

# Save feature list as text for easy reference
with open(processed_dir / 'feature_list.txt', 'w') as f:
    f.write("TRANCE Structured Features\n")
    f.write("="*60 + "\n\n")
    for i, feat in enumerate(feature_cols, 1):
        f.write(f"{i:3d}. {feat}\n")

print(f"✓ Saved feature list")


# Cell 16: Visualization
"""
Visualize feature distributions and correlations
"""
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Structured Feature Analysis', fontsize=16, fontweight='bold')

# Ensure column names are uppercase
features.columns = features.columns.str.upper()
evaluable = features[features['IS_LAST_ADMISSION'] == 0].copy()
target_col = 'TARGET_READMIT_30'

# 1. Feature count by category
feature_categories = {
    'Demographics': len([f for f in feature_cols if any(x in f.lower() for x in ['age', 'gender', 'ethnicity', 'insurance', 'marital'])]),
    'Admission': len([f for f in feature_cols if any(x in f.lower() for x in ['los', 'admission', 'discharge', 'season', 'weekend'])]),
    'Clinical': len([f for f in feature_cols if any(x in f.lower() for x in ['dx_', 'charlson', 'n_diagnoses'])]),
    'Utilization': len([f for f in feature_cols if any(x in f.lower() for x in ['prior', 'frequent', 'icu'])]),
    'Labs': len([f for f in feature_cols if 'lab_' in f.lower()]),
    'Medications': len([f for f in feature_cols if 'med_' in f.lower() or 'n_medications' in f.lower()]),
    'Procedures': len([f for f in feature_cols if 'proc' in f.lower() or 'n_procedures' in f.lower()])
}

axes[0, 0].bar(feature_categories.keys(), feature_categories.values(), edgecolor='black', alpha=0.7)
axes[0, 0].set_ylabel('Count')
axes[0, 0].set_title('Features by Category')
axes[0, 0].tick_params(axis='x', rotation=45)

# 2. Top correlated features
if target_col in evaluable.columns:
    numeric_features = evaluable[feature_cols + [target_col]].select_dtypes(include=[np.number])
    top_corr = numeric_features.corr()[target_col].abs().sort_values(ascending=False)[1:11]
    axes[0, 1].barh(range(len(top_corr)), top_corr.values)
    axes[0, 1].set_yticks(range(len(top_corr)))
    axes[0, 1].set_yticklabels([f[:30] for f in top_corr.index], fontsize=8)
    axes[0, 1].set_xlabel('|Correlation|')
    axes[0, 1].set_title('Top 10 Features Correlated with Readmission')
    axes[0, 1].invert_yaxis()
else:
    print(f"⚠️  Column '{target_col}' not found in evaluable data. Skipping correlation plot.")

# 3. Key feature distributions by readmission
key_features_plot = ['CHARLSON_SCORE', 'LOS_DAYS', 'PRIOR_ADMISSIONS_180D', 'N_MEDICATIONS']
for i, feat in enumerate(key_features_plot[:2]):
    if feat in evaluable.columns and target_col in evaluable.columns:
        evaluable.groupby(target_col)[feat].plot.hist(alpha=0.6, ax=axes[1, i], bins=20, legend=True)
        axes[1, i].set_xlabel(feat.replace('_', ' ').title())
        axes[1, i].set_ylabel('Frequency')
        axes[1, i].set_title(f'Distribution of {feat.replace("_", " ").title()}')
        axes[1, i].legend(['No Readmit', 'Readmit'])

plt.tight_layout()
plt.savefig('outputs/figures/feature_engineering_summary.png', dpi=300, bbox_inches='tight')
print("\n✓ Visualization saved")
plt.show()

# Cell 17: Summary
print("\n" + "="*60)
print("FEATURE ENGINEERING COMPLETE!")
print("="*60)

target_col = 'TARGET_READMIT_30'  # use uppercase

print(f"\n✅ Engineered {len(feature_cols)} structured features")
print(f"✅ Created train/calibration/test splits")
if target_col in evaluable.columns:
    print(f"✅ Readmission rate: {evaluable[target_col].mean()*100:.2f}%")
else:
    print(f"⚠️  Column '{target_col}' not found in evaluable data")

print(f"\n📊 Outputs:")
print(f"   - data/processed/train_features.parquet")
print(f"   - data/processed/calibration_features.parquet")
print(f"   - data/processed/test_features.parquet")
print(f"   - data/processed/feature_info.json")
print(f"   - outputs/figures/feature_engineering_summary.png")

print(f"\n📝 Next Steps:")
print("   1. Generate clinical text embeddings (04_generate_embeddings.ipynb)")
print("   2. Train baseline and fused models (05_train_models.ipynb)")
print("   3. Evaluate and calibrate predictions")



# =========================================
# 04_generate_embeddings_optimized.py
# ClinicalT5-large embeddings with GPU
# =========================================

# Cell 1: Setup and GPU Check
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from pathlib import Path
import json
from tqdm.auto import tqdm
import h5py
import warnings
warnings.filterwarnings('ignore')

# GPU setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"🖥️  Using device: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# Cell 2: Load Config & Cohort
config_path = 'configs/config.json'
with open(config_path, 'r') as f:
    config = json.load(f)

cohort_path = 'data/processed/cohort_with_outcomes.parquet'
cohort = pd.read_parquet(cohort_path)
print(f"✓ Loaded cohort: {len(cohort)} admissions")
print(f"   With discharge notes: {cohort['HAS_DISCHARGE_NOTE'].sum()}")

cohort_with_notes = cohort[cohort['HAS_DISCHARGE_NOTE']].copy()
cohort_with_notes = cohort_with_notes[cohort_with_notes['DISCHARGE_TEXT'].notna()].copy()
print(f"✓ Processing {len(cohort_with_notes)} notes")

# Cell 3: Load Clinical Language Model
import os
os.environ["HF_HUB_TIMEOUT"] = "300"

model_name_original = config['model']['embedding_model']
print(f"Original model: {model_name_original}")
print("⚠️  ClinicalT5-large has Flax conversion issues. Using alternative clinical model...")

# Use a clinically-trained model with native PyTorch weights
# Options (in order of preference):
alternatives = [
    "emilyalsentzer/Bio_ClinicalBERT",  # Clinical BERT - 768 dim
    "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract",  # PubMed BERT - 768 dim
    "dmis-lab/biobert-v1.1",  # BioBERT - 768 dim
]

# Try to load an alternative
model_name = alternatives[0]
print(f"Using: {model_name}")

from transformers import AutoModel  # Use AutoModel for BERT-style models

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
model = AutoModel.from_pretrained(model_name)

# Move to device
print(f"Moving model to {device}...")
model = model.to(device)
model.eval()

print(f"✓ Model loaded successfully")
print(f"   Parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"   Device: {next(model.parameters()).device}")
print(f"   Actual embedding dimension: {model.config.hidden_size}")

# Update config to match actual model
config['model']['embedding_dim'] = model.config.hidden_size
print(f"   Updated config embedding_dim to: {config['model']['embedding_dim']}")

# Cell 4: Text Preprocessing
def preprocess_text(text):
    if pd.isna(text): return ""
    text = str(text).lower().strip()
    return text if len(text) >= 5 else ""

cohort_with_notes['PROCESSED_TEXT'] = cohort_with_notes['DISCHARGE_TEXT'].apply(preprocess_text)
cohort_with_notes = cohort_with_notes[cohort_with_notes['PROCESSED_TEXT'] != ""].copy()
print(f"✓ {len(cohort_with_notes)} notes after preprocessing")

# Cell 5: Improved Chunking Function
max_length = min(config['model']['max_text_length'], 512)  # BERT models max out at 512
overlap = config['model']['chunk_overlap']

def chunk_text(text, tokenizer, max_length=512, overlap=128):
    """
    Chunk text into overlapping segments that fit within max_length tokens.
    Ensures no chunk exceeds the model's maximum sequence length.
    """
    # Enforce absolute maximum
    max_length = min(max_length, 512)

    # Tokenize without special tokens first
    tokens = tokenizer.encode(text, add_special_tokens=False, truncation=False)

    # If text fits in one chunk, return it with special tokens
    if len(tokens) <= max_length - 2:  # -2 for [CLS] and [SEP]
        return [tokenizer.encode(
            text,
            max_length=max_length,
            truncation=True,
            add_special_tokens=True
        )]

    chunks = []
    stride = max_length - overlap - 2  # Account for special tokens

    for i in range(0, len(tokens), stride):
        # Extract chunk
        chunk_tokens = tokens[i:i + max_length - 2]

        # Build with special tokens using tokenizer
        chunk_with_special = tokenizer.build_inputs_with_special_tokens(chunk_tokens)

        # Safety check: ensure we don't exceed max_length
        if len(chunk_with_special) > max_length:
            chunk_with_special = chunk_with_special[:max_length]

        chunks.append(chunk_with_special)

        # Break if we've processed all tokens
        if i + max_length - 2 >= len(tokens):
            break

    return chunks

print(f"✓ Chunking function ready (max_length={max_length}, overlap={overlap})")

# Cell 6: Embedding Generation (Updated for BERT-style models)
@torch.no_grad()
def generate_embeddings(texts, tokenizer, model, batch_size=8, max_length=512, overlap=128):
    """Generate embeddings using a BERT-style encoder model"""
    all_embeddings = []

    # Get model's actual max length
    model_max_length = min(max_length, tokenizer.model_max_length, 512)

    for text in tqdm(texts, desc="Generating embeddings"):
        chunks = chunk_text(text, tokenizer, model_max_length, overlap)

        if len(chunks) == 0:
            # Use actual embedding dimension from model
            all_embeddings.append(np.zeros(model.config.hidden_size))
            continue

        chunk_embeddings = []

        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]

            # Ensure no chunk exceeds model_max_length
            batch = [c[:model_max_length] for c in batch]

            # Pad batch
            max_len = max(len(c) for c in batch)
            max_len = min(max_len, model_max_length)  # Safety check

            padded = [c + [tokenizer.pad_token_id]*(max_len - len(c)) for c in batch]
            attention_mask = [[1]*len(c) + [0]*(max_len - len(c)) for c in batch]

            # Move to device
            input_ids = torch.tensor(padded).to(device)
            attn_mask = torch.tensor(attention_mask).to(device)

            # Forward pass - for BERT-style models use the model directly
            outputs = model(input_ids=input_ids, attention_mask=attn_mask)

            # Get embeddings from last hidden state
            emb = outputs.last_hidden_state

            # Mean pooling with attention mask
            mask_expanded = attn_mask.unsqueeze(-1).expand(emb.size()).float()
            chunk_emb = (emb * mask_expanded).sum(1) / mask_expanded.sum(1).clamp(min=1e-9)

            chunk_embeddings.append(chunk_emb.cpu().numpy())

        # Average all chunk embeddings
        final_emb = np.mean(np.vstack(chunk_embeddings), axis=0)
        all_embeddings.append(final_emb)

    return np.array(all_embeddings)

print("✓ Embedding function ready (BERT-style)")

# Cell 7: Generate embeddings with safe parameters
batch_size = 16 if torch.cuda.is_available() else 4
texts = cohort_with_notes['PROCESSED_TEXT'].tolist()

# Use safe max_length
safe_max_length = min(max_length, 512)
print(f"Generating embeddings with max_length={safe_max_length}, overlap={overlap}")

embeddings = generate_embeddings(
    texts,
    tokenizer,
    model,
    batch_size=batch_size,
    max_length=safe_max_length,
    overlap=overlap
)

print(f"✓ Generated embeddings: {embeddings.shape}, {embeddings.nbytes/1e6:.2f} MB")
# Cell 8: Quality Checks
zero_vectors = np.all(embeddings == 0, axis=1).sum()
nan_vectors = np.any(np.isnan(embeddings), axis=1).sum()
inf_vectors = np.any(np.isinf(embeddings), axis=1).sum()
norms = np.linalg.norm(embeddings, axis=1)

print(f"Zero vectors: {zero_vectors}, NaN: {nan_vectors}, Inf: {inf_vectors}")
print(f"Norms - mean: {norms.mean():.4f}, std: {norms.std():.4f}")

# Cell 9: Save embeddings (HDF5 & Parquet)
Path('data/embeddings').mkdir(parents=True, exist_ok=True)
h5_path = 'data/embeddings/discharge_note_embeddings.h5'
with h5py.File(h5_path, 'w') as f:
    f.create_dataset('embeddings', data=embeddings, compression='gzip')
    f.create_dataset('hadm_ids', data=cohort_with_notes['HADM_ID'].values)
    f.attrs['n_admissions'] = len(embeddings)
    f.attrs['embedding_dim'] = embeddings.shape[1]
    f.attrs['model'] = model_name
    f.attrs['max_length'] = max_length
    f.attrs['overlap'] = overlap
print(f"✓ Saved HDF5 embeddings: {h5_path}")

emb_df = pd.DataFrame(embeddings, columns=[f'emb_{i}' for i in range(embeddings.shape[1])])
emb_df['HADM_ID'] = cohort_with_notes['HADM_ID'].values
parquet_path = 'data/embeddings/embeddings.parquet'
emb_df.to_parquet(parquet_path, index=False)
print(f"✓ Saved Parquet embeddings: {parquet_path}")

# Cell 10: Cleanup GPU
if torch.cuda.is_available():
    del model
    torch.cuda.empty_cache()
    print("✓ GPU memory cleared")

print("✅ Embedding generation complete!")



"""
04_create_fused_features.py
Merge structured features with discharge summary embeddings
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json

print("="*70)
print("CREATING FUSED FEATURES (STRUCTURED + EMBEDDINGS)")
print("="*70)

# ============================================================================
# Load Configuration
# ============================================================================
base_path = Path('')
processed_dir = base_path / 'data/processed'

with open(base_path / 'configs/config.json', 'r') as f:
    config = json.load(f)

# ============================================================================
# Load Structured Features
# ============================================================================
print("\n1. Loading structured features...")
train_struct = pd.read_parquet(processed_dir / 'train_features.parquet')
calib_struct = pd.read_parquet(processed_dir / 'calibration_features.parquet')
test_struct = pd.read_parquet(processed_dir / 'test_features.parquet')

print(f"   Train: {train_struct.shape}")
print(f"   Calib: {calib_struct.shape}")
print(f"   Test:  {test_struct.shape}")

# ============================================================================
# Load Embeddings
# ============================================================================
print("\n2. Loading embeddings...")
embeddings_path = base_path / 'data/embeddings/embeddings.parquet'

if not embeddings_path.exists():
    print(f"\n❌ ERROR: Embeddings file not found!")
    print(f"   Expected: {embeddings_path}")
    print(f"\n   Please run: 03_generate_embeddings.py first")
    raise FileNotFoundError(f"Missing embeddings file: {embeddings_path}")

embeddings_df = pd.read_parquet(embeddings_path)
print(f"   Embeddings: {embeddings_df.shape}")
print(f"   Embedding dimensions: {len([c for c in embeddings_df.columns if c.startswith('emb_')])}")

# ============================================================================
# Merge Features
# ============================================================================
print("\n3. Merging structured features with embeddings...")

def merge_with_embeddings(struct_df, embeddings_df, split_name):
    """Merge structured features with embeddings"""

    print(f"\n   Processing {split_name} set...")

    # Merge on HADM_ID
    merged = struct_df.merge(
        embeddings_df,
        on='HADM_ID',
        how='left'
    )

    print(f"      Before merge: {struct_df.shape}")
    print(f"      After merge:  {merged.shape}")

    # Check for missing embeddings
    embedding_cols = [c for c in embeddings_df.columns if c.startswith('emb_')]
    n_missing = merged[embedding_cols[0]].isna().sum()

    if n_missing > 0:
        print(f"      ⚠ Warning: {n_missing} admissions missing embeddings")
        print(f"         Filling with zeros...")
        merged[embedding_cols] = merged[embedding_cols].fillna(0)
    else:
        print(f"      ✓ All admissions have embeddings")

    # Verify merge
    assert len(merged) == len(struct_df), "Row count mismatch after merge!"

    return merged

# Merge all splits
train_fused = merge_with_embeddings(train_struct, embeddings_df, "Train")
calib_fused = merge_with_embeddings(calib_struct, embeddings_df, "Calibration")
test_fused = merge_with_embeddings(test_struct, embeddings_df, "Test")

# ============================================================================
# Verify Fused Features
# ============================================================================
print("\n4. Verifying fused features...")

# Get column counts
embedding_cols = [c for c in train_fused.columns if c.startswith('emb_')]
id_cols = ['HADM_ID', 'SUBJECT_ID']
target_col = 'TARGET_READMIT_30'
structured_cols = [c for c in train_fused.columns
                   if c not in embedding_cols + id_cols]

print(f"\n   Column breakdown:")
print(f"      ID columns:         {len(id_cols)}")
# print(f"      Target column:      1 ({target_col})")
print(f"      Structured features: {len(structured_cols)}")
print(f"      Embedding features:  {len(embedding_cols)}")
print(f"      Total columns:       {len(train_fused.columns)}")

# Check for NaN values
print(f"\n   Checking for missing values...")
for df_name, df in [("Train", train_fused), ("Calib", calib_fused), ("Test", test_fused)]:
    nan_counts = df[structured_cols + embedding_cols].isna().sum().sum()
    if nan_counts > 0:
        print(f"      ⚠ {df_name}: {nan_counts} NaN values found")
    else:
        print(f"      ✓ {df_name}: No missing values")

# Check target distribution
print(f"\n   Target distribution:")
print(f"      Train readmission rate: {train_fused['TARGET_READMIT_30'].mean():.2%}")
print(f"      Calib readmission rate: {calib_fused['TARGET_READMIT_30'].mean():.2%}")
print(f"      Test readmission rate:  {test_fused['TARGET_READMIT_30'].mean():.2%}")

# ============================================================================
# Save Fused Features
# ============================================================================
print("\n5. Saving fused features...")

output_files = {
    'train_fused.parquet': train_fused,
    'calibration_fused.parquet': calib_fused,
    'test_fused.parquet': test_fused
}

for filename, df in output_files.items():
    output_path = processed_dir / filename
    df.to_parquet(output_path, index=False)
    print(f"   ✓ Saved: {filename} ({df.shape})")

# ============================================================================
# Create Feature Metadata
# ============================================================================
print("\n6. Creating feature metadata...")

feature_metadata = {
    'creation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
    'datasets': {
        'train': {'n_samples': len(train_fused), 'n_features': len(train_fused.columns)},
        'calibration': {'n_samples': len(calib_fused), 'n_features': len(calib_fused.columns)},
        'test': {'n_samples': len(test_fused), 'n_features': len(test_fused.columns)}
    },
    'features': {
        'n_structured': len(structured_cols),
        'n_embeddings': len(embedding_cols),
        'n_total': len(structured_cols) + len(embedding_cols),
        'structured_features': structured_cols,
        'embedding_dimensions': len(embedding_cols)
    },
    'target': {
        'column': target_col,
        'train_mean': float(train_fused[target_col].mean()),
        'calib_mean': float(calib_fused[target_col].mean()),
        'test_mean': float(test_fused[target_col].mean())
    }
}

metadata_path = processed_dir / 'fused_features_metadata.json'
with open(metadata_path, 'w') as f:
    json.dump(feature_metadata, f, indent=2)

print(f"   ✓ Metadata saved: {metadata_path.name}")

# ============================================================================
# Summary Statistics
# ============================================================================
print("\n" + "="*70)
print("FUSED FEATURES CREATED SUCCESSFULLY!")
print("="*70)

print(f"\n📊 Summary:")
print(f"   Total features:      {len(structured_cols) + len(embedding_cols)}")
print(f"   - Structured:        {len(structured_cols)}")
print(f"   - Embeddings:        {len(embedding_cols)}")
print(f"\n   Training samples:    {len(train_fused):,}")
print(f"   Calibration samples: {len(calib_fused):,}")
print(f"   Test samples:        {len(test_fused):,}")

print(f"\n💾 Saved files:")
print(f"   • data/processed/train_fused.parquet")
print(f"   • data/processed/calibration_fused.parquet")
print(f"   • data/processed/test_fused.parquet")
print(f"   • data/processed/fused_features_metadata.json")

print(f"\n📝 Next step:")
print(f"   Run: 05_train_models.py")
print("="*70 + "\n")

"""
Pre-flight check and execution script
Checks if all required files exist before training
"""

from pathlib import Path
import pandas as pd

print("="*70)
print("PRE-FLIGHT CHECK")
print("="*70)

base_path = Path('')
processed_dir = base_path / 'data/processed'

# ============================================================================
# Check Required Files
# ============================================================================
required_files = {
    'Structured Features': [
        'train_features.parquet',
        'calibration_features.parquet',
        'test_features.parquet'
    ],
    'Embeddings': [
        'embeddings.parquet'
    ],
    'Fused Features': [
        'train_fused.parquet',
        'calibration_fused.parquet',
        'test_fused.parquet'
    ]
}

print("\nChecking required files...")
missing_files = []
existing_files = []

for category, files in required_files.items():
    print(f"\n{category}:")
    for file in files:
        path = processed_dir / file
        if path.exists():
            try:
                df = pd.read_parquet(path)
                print(f"   ✓ {file} ({df.shape[0]} rows, {df.shape[1]} cols)")
                existing_files.append(file)
            except Exception as e:
                print(f"   ⚠ {file} exists but cannot be read: {e}")
                missing_files.append(file)
        else:
            print(f"   ✗ {file} - NOT FOUND")
            missing_files.append(file)

# ============================================================================
# Dataset Size Check
# ============================================================================
print("\n" + "="*70)
print("DATASET SIZE CHECK")
print("="*70)

try:
    train = pd.read_parquet(processed_dir / 'train_features.parquet')
    n_train = len(train)

    print(f"\nTraining samples: {n_train}")

    if n_train < 50:
        print(f"\n⚠ WARNING: Very small dataset detected!")
        print(f"   - You have only {n_train} training samples")
        print(f"   - This appears to be a test/sample dataset")
        print(f"   - Results may not be meaningful")
        print(f"\n   Recommendations:")
        print(f"      1. Use full MIMIC-III dataset for real experiments")
        print(f"      2. Expect high variance in metrics")
        print(f"      3. Data augmentation will be applied automatically")
    elif n_train < 500:
        print(f"\n⚠ Small dataset: {n_train} samples")
        print(f"   - Data augmentation will be applied")
        print(f"   - Results may have moderate variance")
    else:
        print(f"\n✓ Sufficient training data: {n_train} samples")

except FileNotFoundError:
    print("\n❌ Cannot check dataset size - training features not found")

# ============================================================================
# Action Plan
# ============================================================================
print("\n" + "="*70)
print("ACTION PLAN")
print("="*70)

if missing_files:
    print(f"\n❌ Missing {len(missing_files)} required file(s)")

    # Determine what needs to be run
    if 'discharge_embeddings.parquet' in missing_files:
        print(f"\n📝 Step 1: Generate embeddings")
        print(f"   Run: %run ../scripts/03_generate_embeddings.py")

    if any('fused' in f for f in missing_files):
        print(f"\n📝 Step 2: Create fused features")
        print(f"   Run: %run ../scripts/04_create_fused_features.py")

    print(f"\n📝 Step 3: Train models")
    print(f"   Run: %run ../scripts/05_train_models_fixed.py")

else:
    print(f"\n✅ All required files exist!")
    print(f"\n📝 Ready to train models")
    print(f"   Run: %run ../scripts/05_train_models_fixed.py")

    # Ask if user wants to proceed
    print(f"\n" + "="*70)
    response = input("Proceed with training? (y/n): ").lower().strip()

    if response == 'y':
        print(f"\nStarting training...")
        exec(open('../scripts/05_train_models_fixed.py').read())
    else:
        print(f"\nTraining cancelled.")

print("\n" + "="*70)

# @title
"""
05_train_models.py (Fixed for Colab)
Train and compare baseline (structured-only) vs fused (structured + embeddings) models
"""

# ============================================================================
# Cell 1: Setup and Install Dependencies
# ============================================================================
import sys
import subprocess

print("Checking and installing dependencies...")
required_packages = ['mlflow', 'lightgbm', 'shap', 'imbalanced-learn']
for package in required_packages:
    try:
        __import__(package.replace('-', '_'))
    except ImportError:
        print(f"Installing {package}...")
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import joblib
from datetime import datetime

# ML libraries
import lightgbm as lgb
from sklearn.metrics import (
    roc_auc_score, average_precision_score,
    roc_curve, precision_recall_curve,
    confusion_matrix, classification_report,
    brier_score_loss
)
from sklearn.calibration import calibration_curve
from sklearn.isotonic import IsotonicRegression

# Optional MLflow
try:
    import mlflow
    import mlflow.lightgbm
    USE_MLFLOW = True
    print("✓ MLflow enabled")
except ImportError:
    USE_MLFLOW = False
    print("⚠ MLflow not available - skipping experiment tracking")

# SHAP (will try to import later)
SHAP_AVAILABLE = False

import warnings
warnings.filterwarnings('ignore')

print("✓ Setup complete")

# ============================================================================
# Cell 2: Load Configuration and Data
# ============================================================================
base_path = Path('')
processed_dir = base_path / 'data/processed'

# Load config
with open(base_path / 'configs/config.json', 'r') as f:
    config = json.load(f)

# Load structured-only features
print("\nLoading structured-only datasets...")
train_struct = pd.read_parquet(processed_dir / 'train_features.parquet')
calib_struct = pd.read_parquet(processed_dir / 'calibration_features.parquet')
test_struct = pd.read_parquet(processed_dir / 'test_features.parquet')

print(f"✓ Structured features:")
print(f"   Train: {train_struct.shape}")
print(f"   Calib: {calib_struct.shape}")
print(f"   Test:  {test_struct.shape}")

# Load fused features (structured + embeddings)
print("\nLoading fused datasets...")
train_fused = pd.read_parquet(processed_dir / 'train_fused.parquet')
calib_fused = pd.read_parquet(processed_dir / 'calibration_fused.parquet')
test_fused = pd.read_parquet(processed_dir / 'test_fused.parquet')

print(f"✓ Fused features:")
print(f"   Train: {train_fused.shape}")
print(f"   Calib: {calib_fused.shape}")
print(f"   Test:  {test_fused.shape}")

# ============================================================================
# Cell 3: Prepare Features and Labels
# ============================================================================
# Remove categorical columns if they still exist
categorical_cols = ['AGE_GROUP', 'ETHNICITY_CATEGORY', 'LOS_CATEGORY', 'DISCHARGE_SEASON']
for df in [train_struct, calib_struct, test_struct]:
    for col in categorical_cols:
        if col in df.columns:
            df.drop(col, axis=1, inplace=True)

# Extract feature names
struct_features = [col for col in train_struct.columns
                   if col not in ['HADM_ID', 'SUBJECT_ID', 'TARGET_READMIT_30']]
fused_features = [col for col in train_fused.columns
                  if col not in ['HADM_ID', 'SUBJECT_ID', 'TARGET_READMIT_30']]
embedding_features = [col for col in fused_features if col.startswith('emb_')]

print(f"\n✓ Feature counts:")
print(f"   Structured features: {len(struct_features)}")
print(f"   Embedding features: {len(embedding_features)}")
print(f"   Total fused features: {len(fused_features)}")

# Prepare structured-only datasets
X_train_struct = train_struct[struct_features].values
y_train = train_struct['TARGET_READMIT_30'].values
X_calib_struct = calib_struct[struct_features].values
y_calib = calib_struct['TARGET_READMIT_30'].values
X_test_struct = test_struct[struct_features].values
y_test = test_struct['TARGET_READMIT_30'].values

# Prepare fused datasets
X_train_fused = train_fused[fused_features].values
X_calib_fused = calib_fused[fused_features].values
X_test_fused = test_fused[fused_features].values

print(f"\n✓ Structured datasets:")
print(f"   Train: {X_train_struct.shape}, Positive: {y_train.mean():.2%}")
print(f"   Calib: {X_calib_struct.shape}, Positive: {y_calib.mean():.2%}")
print(f"   Test:  {X_test_struct.shape}, Positive: {y_test.mean():.2%}")

print(f"\n✓ Fused datasets:")
print(f"   Train: {X_train_fused.shape}")
print(f"   Calib: {X_calib_fused.shape}")
print(f"   Test:  {X_test_fused.shape}")

# ============================================================================
# Cell 4: Data Augmentation (Optional - for small datasets)
# ============================================================================
USE_AUGMENTATION = len(y_train) < 200  # Only if very small dataset

if USE_AUGMENTATION:
    from imblearn.over_sampling import SMOTE
    from collections import Counter

    print("\n" + "="*70)
    print("DATA AUGMENTATION (Small Dataset)")
    print("="*70)

    def apply_combined_augmentation(X, y, target_multiplier=4):
        """SMOTE + Gaussian noise augmentation"""
        print(f"\nOriginal: {X.shape}, {Counter(y)}")

        # Step 1: SMOTE for class balance
        n_minority = min(Counter(y).values())
        k_neighbors = min(5, n_minority - 1) if n_minority > 1 else 1
        smote = SMOTE(sampling_strategy='auto', k_neighbors=k_neighbors, random_state=42)
        X_balanced, y_balanced = smote.fit_resample(X, y)
        print(f"After SMOTE: {X_balanced.shape}, {Counter(y_balanced)}")

        # Step 2: Add noise copies
        X_augmented = [X_balanced]
        y_augmented = [y_balanced]

        for i in range(target_multiplier - 1):
            noise = np.random.normal(0, 0.03, X_balanced.shape) * X_balanced.std(axis=0)
            X_augmented.append(X_balanced + noise)
            y_augmented.append(y_balanced)

        X_final = np.vstack(X_augmented)
        y_final = np.hstack(y_augmented)
        print(f"Final: {X_final.shape}, {Counter(y_final)}")

        return X_final, y_final

    # Apply to both structured and fused
    X_train_struct_aug, y_train_aug = apply_combined_augmentation(
        X_train_struct, y_train, target_multiplier=4
    )
    X_train_fused_aug, _ = apply_combined_augmentation(
        X_train_fused, y_train, target_multiplier=4
    )

    # Use augmented data
    X_train_struct = X_train_struct_aug
    X_train_fused = X_train_fused_aug
    y_train = y_train_aug

    print(f"\n✓ Augmentation complete")
else:
    print(f"\n✓ Skipping augmentation (sufficient training data: {len(y_train)} samples)")

# ============================================================================
# Cell 5: Setup MLflow Tracking (if available)
# ============================================================================
if USE_MLFLOW:
    mlflow.set_tracking_uri(str(base_path / 'mlruns'))
    mlflow.set_experiment('TRANCE_Readmission_Prediction')
    print("\n✓ MLflow tracking initialized")
    print(f"   Tracking URI: {mlflow.get_tracking_uri()}")

# ============================================================================
# Cell 6: Train Baseline Model (Structured Only)
# ============================================================================
print("\n" + "="*70)
print("TRAINING BASELINE MODEL (STRUCTURED ONLY)")
print("="*70)

# Start MLflow run if available
if USE_MLFLOW:
    mlflow.start_run(run_name="Baseline_Structured_Only")

# Model parameters
params_baseline = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 7,
    'max_depth': 3,
    'learning_rate': 0.01,
    'n_estimators': 200,
    'min_child_samples': 10,
    'reg_alpha': 0.5,
    'reg_lambda': 0.5,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': -1,
    'random_state': 42
}

# Log params
if USE_MLFLOW:
    mlflow.log_params(params_baseline)
    mlflow.log_param("n_features", X_train_struct.shape[1])
    mlflow.log_param("n_train", len(X_train_struct))

# Create LightGBM datasets
train_data = lgb.Dataset(X_train_struct, label=y_train)
valid_data = lgb.Dataset(X_calib_struct, label=y_calib, reference=train_data)

# Train model
print("Training baseline model...")
model_baseline = lgb.train(
    params_baseline,
    train_data,
    num_boost_round=1000,
    valid_sets=[train_data, valid_data],
    valid_names=['train', 'calib'],
    callbacks=[
        lgb.early_stopping(stopping_rounds=50),
        lgb.log_evaluation(period=100)
    ]
)

print(f"✓ Training complete")
print(f"   Best iteration: {model_baseline.best_iteration}")
print(f"   Best score: {model_baseline.best_score['calib']['auc']:.4f}")

# Predictions
y_pred_baseline_train = model_baseline.predict(X_train_struct)
y_pred_baseline_calib = model_baseline.predict(X_calib_struct)
y_pred_baseline_test = model_baseline.predict(X_test_struct)

# Metrics
train_auc = roc_auc_score(y_train, y_pred_baseline_train)
calib_auc = roc_auc_score(y_calib, y_pred_baseline_calib)
test_auc = roc_auc_score(y_test, y_pred_baseline_test)

train_auprc = average_precision_score(y_train, y_pred_baseline_train)
calib_auprc = average_precision_score(y_calib, y_pred_baseline_calib)
test_auprc = average_precision_score(y_test, y_pred_baseline_test)

if USE_MLFLOW:
    mlflow.log_metric("train_auc", train_auc)
    mlflow.log_metric("calib_auc", calib_auc)
    mlflow.log_metric("test_auc", test_auc)
    mlflow.log_metric("train_auprc", train_auprc)
    mlflow.log_metric("calib_auprc", calib_auprc)
    mlflow.log_metric("test_auprc", test_auprc)

print(f"\nBaseline Model Performance:")
print(f"  Train AUROC: {train_auc:.4f} | AUPRC: {train_auprc:.4f}")
print(f"  Calib AUROC: {calib_auc:.4f} | AUPRC: {calib_auprc:.4f}")
print(f"  Test  AUROC: {test_auc:.4f} | AUPRC: {test_auprc:.4f}")

# Save model
model_path = base_path / 'outputs/models/baseline_model.txt'
model_path.parent.mkdir(parents=True, exist_ok=True)
model_baseline.save_model(str(model_path))
if USE_MLFLOW:
    mlflow.log_artifact(str(model_path))
print(f"\n✓ Model saved to: {model_path}")

if USE_MLFLOW:
    mlflow.end_run()

# ============================================================================
# Cell 7: Train Fused Model (Structured + Embeddings)
# ============================================================================
print("\n" + "="*70)
print("TRAINING FUSED MODEL (STRUCTURED + EMBEDDINGS)")
print("="*70)

if USE_MLFLOW:
    mlflow.start_run(run_name="Fused_Structured_Embeddings")
    mlflow.log_param("model_type", "LightGBM")
    mlflow.log_param("features", "structured_and_embeddings")
    mlflow.log_param("n_features", len(fused_features))
    mlflow.log_param("n_embedding_dims", len(embedding_features))
    mlflow.log_param("n_train", len(X_train_fused))
    for key, value in params_baseline.items():
        mlflow.log_param(f"lgbm_{key}", value)

# Model parameters (same as baseline for fair comparison)
params_fused = params_baseline.copy()

# Create datasets
train_data = lgb.Dataset(X_train_fused, label=y_train)
valid_data = lgb.Dataset(X_calib_fused, label=y_calib, reference=train_data)

# Train model
print("Training fused model...")
model_fused = lgb.train(
    params_fused,
    train_data,
    num_boost_round=1000,
    valid_sets=[train_data, valid_data],
    valid_names=['train', 'valid'],
    callbacks=[
        lgb.early_stopping(stopping_rounds=50),
        lgb.log_evaluation(period=100)
    ]
)

print(f"✓ Training complete")
print(f"   Best iteration: {model_fused.best_iteration}")
print(f"   Best score: {model_fused.best_score['valid']['auc']:.4f}")

# Make predictions
y_pred_fused_train = model_fused.predict(X_train_fused)
y_pred_fused_calib = model_fused.predict(X_calib_fused)
y_pred_fused_test = model_fused.predict(X_test_fused)

# Evaluate
train_auc_f = roc_auc_score(y_train, y_pred_fused_train)
calib_auc_f = roc_auc_score(y_calib, y_pred_fused_calib)
test_auc_f = roc_auc_score(y_test, y_pred_fused_test)

train_auprc_f = average_precision_score(y_train, y_pred_fused_train)
calib_auprc_f = average_precision_score(y_calib, y_pred_fused_calib)
test_auprc_f = average_precision_score(y_test, y_pred_fused_test)

# Log metrics
if USE_MLFLOW:
    mlflow.log_metric("train_auc", train_auc_f)
    mlflow.log_metric("calib_auc", calib_auc_f)
    mlflow.log_metric("test_auc", test_auc_f)
    mlflow.log_metric("train_auprc", train_auprc_f)
    mlflow.log_metric("calib_auprc", calib_auprc_f)
    mlflow.log_metric("test_auprc", test_auprc_f)

print(f"\nFused Model Performance:")
print(f"  Train AUROC: {train_auc_f:.4f} | AUPRC: {train_auprc_f:.4f}")
print(f"  Calib AUROC: {calib_auc_f:.4f} | AUPRC: {calib_auprc_f:.4f}")
print(f"  Test  AUROC: {test_auc_f:.4f} | AUPRC: {test_auprc_f:.4f}")

# Save model
model_path = base_path / 'outputs/models/fused_model.txt'
model_fused.save_model(str(model_path))
if USE_MLFLOW:
    mlflow.log_artifact(str(model_path))
    mlflow.end_run()

print(f"\n✓ Model saved to: {model_path}")

# ============================================================================
# Cell 8: Model Comparison
# ============================================================================
print("\n" + "="*70)
print("MODEL COMPARISON")
print("="*70)

# Calculate improvements
auc_improvement = test_auc_f - test_auc
auprc_improvement = test_auprc_f - test_auprc

print("\nTest Set Performance Comparison:")
print("-" * 70)
print(f"{'Metric':<20} {'Baseline':<12} {'Fused':<12} {'Improvement':<12}")
print("-" * 70)
print(f"{'AUROC':<20} {test_auc:<12.4f} {test_auc_f:<12.4f} {auc_improvement:+.4f} ({auc_improvement/test_auc*100:+.2f}%)")
print(f"{'AUPRC':<20} {test_auprc:<12.4f} {test_auprc_f:<12.4f} {auprc_improvement:+.4f} ({auprc_improvement/test_auprc*100:+.2f}%)")
print("-" * 70)

if auc_improvement > 0:
    print(f"\n✅ Embeddings provide {auc_improvement:.4f} AUROC improvement!")
else:
    print(f"\n⚠️  Embeddings don't improve performance significantly")

# ============================================================================
# Cell 9: ROC and PR Curves
# ============================================================================
print("\n" + "="*70)
print("GENERATING PERFORMANCE CURVES")
print("="*70)

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ROC Curve
fpr_baseline, tpr_baseline, _ = roc_curve(y_test, y_pred_baseline_test)
fpr_fused, tpr_fused, _ = roc_curve(y_test, y_pred_fused_test)

axes[0].plot(fpr_baseline, tpr_baseline, label=f'Baseline (AUROC={test_auc:.3f})', linewidth=2)
axes[0].plot(fpr_fused, tpr_fused, label=f'Fused (AUROC={test_auc_f:.3f})', linewidth=2)
axes[0].plot([0, 1], [0, 1], 'k--', label='Random', alpha=0.3)
axes[0].set_xlabel('False Positive Rate')
axes[0].set_ylabel('True Positive Rate')
axes[0].set_title('ROC Curve - Test Set')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Precision-Recall Curve
precision_baseline, recall_baseline, _ = precision_recall_curve(y_test, y_pred_baseline_test)
precision_fused, recall_fused, _ = precision_recall_curve(y_test, y_pred_fused_test)

axes[1].plot(recall_baseline, precision_baseline, label=f'Baseline (AUPRC={test_auprc:.3f})', linewidth=2)
axes[1].plot(recall_fused, precision_fused, label=f'Fused (AUPRC={test_auprc_f:.3f})', linewidth=2)
axes[1].axhline(y=y_test.mean(), color='k', linestyle='--', label='Baseline Rate', alpha=0.3)
axes[1].set_xlabel('Recall')
axes[1].set_ylabel('Precision')
axes[1].set_title('Precision-Recall Curve - Test Set')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
fig_path = base_path / 'outputs/figures/model_performance_curves.png'
fig_path.parent.mkdir(parents=True, exist_ok=True)
plt.savefig(fig_path, dpi=300, bbox_inches='tight')
print(f"✓ Performance curves saved to: {fig_path}")
plt.show()

# ============================================================================
# Cell 10: Feature Importance Analysis
# ============================================================================
print("\n" + "="*70)
print("FEATURE IMPORTANCE ANALYSIS")
print("="*70)

# Get feature importance
importance_baseline = model_baseline.feature_importance(importance_type='gain')
importance_fused = model_fused.feature_importance(importance_type='gain')

# Create dataframes
feat_imp_baseline = pd.DataFrame({
    'feature': struct_features,
    'importance': importance_baseline
}).sort_values('importance', ascending=False)

feat_imp_fused = pd.DataFrame({
    'feature': fused_features,
    'importance': importance_fused
}).sort_values('importance', ascending=False)

# Separate structured vs embedding importance in fused model
feat_imp_fused['type'] = feat_imp_fused['feature'].apply(
    lambda x: 'Embedding' if x.startswith('emb_') else 'Structured'
)

# Calculate total importance by type
total_importance = feat_imp_fused.groupby('type')['importance'].sum()
print("\nImportance Distribution in Fused Model:")
print(f"  Structured features: {total_importance.get('Structured', 0):.0f} ({total_importance.get('Structured', 0)/feat_imp_fused['importance'].sum()*100:.1f}%)")
print(f"  Embedding features:  {total_importance.get('Embedding', 0):.0f} ({total_importance.get('Embedding', 0)/feat_imp_fused['importance'].sum()*100:.1f}%)")

# Top features
print("\nTop 15 Features (Baseline Model):")
print(feat_imp_baseline.head(15).to_string(index=False))

print("\nTop 15 Features (Fused Model):")
print(feat_imp_fused.head(15).to_string(index=False))

# Visualize
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Baseline top features
top_n = 20
feat_imp_baseline.head(top_n).plot.barh(x='feature', y='importance', ax=axes[0], legend=False)
axes[0].set_title('Top 20 Features - Baseline Model')
axes[0].set_xlabel('Importance (Gain)')
axes[0].invert_yaxis()

# Fused top features with color coding
top_fused = feat_imp_fused.head(top_n)
colors = ['steelblue' if t == 'Structured' else 'orange' for t in top_fused['type']]
axes[1].barh(range(len(top_fused)), top_fused['importance'], color=colors)
axes[1].set_yticks(range(len(top_fused)))
axes[1].set_yticklabels(top_fused['feature'])
axes[1].set_title('Top 20 Features - Fused Model')
axes[1].set_xlabel('Importance (Gain)')
axes[1].invert_yaxis()
axes[1].legend(['Structured', 'Embedding'], loc='lower right')

plt.tight_layout()
plt.savefig(base_path / 'outputs/figures/feature_importance.png', dpi=300, bbox_inches='tight')
print("\n✓ Feature importance plots saved")
plt.show()

# ============================================================================
# Cell 11: SHAP Analysis (Fused Model) - Optional
# ============================================================================
print("\n" + "="*70)
print("GENERATING SHAP VALUES")
print("="*70)

try:
    import shap
    SHAP_AVAILABLE = True

    # Sample for SHAP (use subset for speed)
    n_shap_samples = min(500, len(X_test_fused))

    # Create DataFrame for sampling
    test_fused_df = pd.DataFrame(X_test_fused, columns=fused_features)
    X_shap = test_fused_df.sample(n=n_shap_samples, random_state=42)

    print(f"Computing SHAP values for {n_shap_samples} samples...")

    # Create SHAP explainer
    explainer = shap.TreeExplainer(model_fused)
    shap_values = explainer.shap_values(X_shap)

    # If binary classification, shap_values might be a list
    if isinstance(shap_values, list):
        shap_values = shap_values[1]  # Positive class

    print("✓ SHAP values computed")

    # Save SHAP values
    shap_data = {
        'shap_values': shap_values,
        'data': X_shap.values,
        'feature_names': X_shap.columns.tolist(),
        'expected_value': explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value
    }

    joblib.dump(shap_data, base_path / 'outputs/models/shap_values.pkl')
    print("✓ SHAP values saved")

    # SHAP summary plot
    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X_shap, max_display=20, show=False)
    plt.tight_layout()
    plt.savefig(base_path / 'outputs/figures/shap_summary.png', dpi=300, bbox_inches='tight')
    print("✓ SHAP summary plot saved")
    plt.show()

except ImportError:
    print("⚠ SHAP not available - skipping interpretability analysis")
    print("  Install with: pip install shap")
except Exception as e:
    print(f"⚠ SHAP analysis failed: {e}")
    print("  Continuing without SHAP...")

# ============================================================================
# Cell 12: Probability Calibration
# ============================================================================
print("\n" + "="*70)
print("PROBABILITY CALIBRATION")
print("="*70)

# Fit calibrator on calibration set
print("Fitting isotonic regression calibrator...")
calibrator = IsotonicRegression(out_of_bounds='clip')
calibrator.fit(y_pred_fused_calib, y_calib)

# Calibrate test predictions
y_pred_calibrated = calibrator.predict(y_pred_fused_test)

# Evaluate calibration
brier_uncalibrated = brier_score_loss(y_test, y_pred_fused_test)
brier_calibrated = brier_score_loss(y_test, y_pred_calibrated)

print(f"\nBrier Score:")
print(f"  Uncalibrated: {brier_uncalibrated:.4f}")
print(f"  Calibrated:   {brier_calibrated:.4f}")
print(f"  Improvement:  {brier_uncalibrated - brier_calibrated:.4f}")

# Calibration curve
fraction_of_positives_uncal, mean_predicted_value_uncal = calibration_curve(
    y_test, y_pred_fused_test, n_bins=10, strategy='uniform'
)
fraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(
    y_test, y_pred_calibrated, n_bins=10, strategy='uniform'
)

# Plot
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Reliability diagram
axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect calibration')
axes[0].plot(mean_predicted_value_uncal, fraction_of_positives_uncal,
             'o-', label=f'Uncalibrated (Brier={brier_uncalibrated:.3f})')
axes[0].plot(mean_predicted_value_cal, fraction_of_positives_cal,
             's-', label=f'Calibrated (Brier={brier_calibrated:.3f})')
axes[0].set_xlabel('Mean Predicted Probability')
axes[0].set_ylabel('Observed Frequency')
axes[0].set_title('Reliability Diagram')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Distribution of predictions
axes[1].hist(y_pred_fused_test, bins=30, alpha=0.5, label='Uncalibrated', edgecolor='black')
axes[1].hist(y_pred_calibrated, bins=30, alpha=0.5, label='Calibrated', edgecolor='black')
axes[1].set_xlabel('Predicted Probability')
axes[1].set_ylabel('Count')
axes[1].set_title('Distribution of Predictions')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig(base_path / 'outputs/figures/calibration_analysis.png', dpi=300, bbox_inches='tight')
print("\n✓ Calibration plots saved")
plt.show()

# Save calibrator
joblib.dump(calibrator, base_path / 'outputs/models/calibrator.pkl')
print("✓ Calibrator saved")

# ============================================================================
# Cell 13: Operating Point Analysis
# ============================================================================
print("\n" + "="*70)
print("OPERATING POINT ANALYSIS")
print("="*70)

# Calculate precision, recall, F1 at different thresholds
thresholds = np.linspace(0, 1, 100)
metrics_at_threshold = []

for thresh in thresholds:
    y_pred_binary = (y_pred_calibrated >= thresh).astype(int)

    if y_pred_binary.sum() == 0:
        continue

    tp = ((y_pred_binary == 1) & (y_test == 1)).sum()
    fp = ((y_pred_binary == 1) & (y_test == 0)).sum()
    fn = ((y_pred_binary == 0) & (y_test == 1)).sum()

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    metrics_at_threshold.append({
        'threshold': thresh,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'n_predicted_positive': y_pred_binary.sum()
    })

metrics_df = pd.DataFrame(metrics_at_threshold)

# Find optimal thresholds
optimal_f1_idx = metrics_df['f1'].idxmax()
optimal_f1_threshold = metrics_df.loc[optimal_f1_idx, 'threshold']

print(f"Optimal F1 Threshold: {optimal_f1_threshold:.3f}")
print(f"  Precision: {metrics_df.loc[optimal_f1_idx, 'precision']:.3f}")
print(f"  Recall:    {metrics_df.loc[optimal_f1_idx, 'recall']:.3f}")
print(f"  F1 Score:  {metrics_df.loc[optimal_f1_idx, 'f1']:.3f}")

# Capacity-based threshold (e.g., top 20% patients)
capacity_percentile = 80  # Top 20%
capacity_threshold = np.percentile(y_pred_calibrated, capacity_percentile)

capacity_mask = y_pred_calibrated >= capacity_threshold
capacity_precision = y_test[capacity_mask].mean()
capacity_recall = y_test[capacity_mask].sum() / y_test.sum()

print(f"\nCapacity-Based Threshold (top {100-capacity_percentile}%): {capacity_threshold:.3f}")
print(f"  Precision: {capacity_precision:.3f}")
print(f"  Recall:    {capacity_recall:.3f}")
print(f"  N flagged: {capacity_mask.sum()} ({capacity_mask.mean()*100:.1f}%)")

# Plot
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Precision-Recall tradeoff
axes[0].plot(metrics_df['threshold'], metrics_df['precision'], label='Precision')
axes[0].plot(metrics_df['threshold'], metrics_df['recall'], label='Recall')
axes[0].plot(metrics_df['threshold'], metrics_df['f1'], label='F1 Score', linewidth=2)
axes[0].axvline(optimal_f1_threshold, color='red', linestyle='--',
                label=f'Optimal F1 ({optimal_f1_threshold:.3f})')
axes[0].set_xlabel('Threshold')
axes[0].set_ylabel('Score')
axes[0].set_title('Precision-Recall Tradeoff')
axes[0].legend()
axes[0].grid(alpha=0.3)

# Number flagged vs threshold
axes[1].plot(metrics_df['threshold'], metrics_df['n_predicted_positive'])
axes[1].axhline(len(y_test) * 0.2, color='red', linestyle='--', label='20% capacity')
axes[1].set_xlabel('Threshold')
axes[1].set_ylabel('Number Flagged as High Risk')
axes[1].set_title('Workload vs Threshold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig(base_path / 'outputs/figures/operating_point_analysis.png', dpi=300, bbox_inches='tight')
print("\n✓ Operating point plots saved")
plt.show()

# ============================================================================
# Cell 14: Save All Predictions
# ============================================================================
print("\n" + "="*70)
print("SAVING PREDICTIONS")
print("="*70)

# Create predictions dataframe
predictions_df = pd.DataFrame({
    'HADM_ID': test_fused['HADM_ID'].values,
    'SUBJECT_ID': test_fused['SUBJECT_ID'].values,
    'true_label': y_test,
    'pred_prob_baseline': y_pred_baseline_test,
    'pred_prob_fused_raw': y_pred_fused_test,
    'pred_prob_fused_calibrated': y_pred_calibrated,
    'pred_binary_f1_optimal': (y_pred_calibrated >= optimal_f1_threshold).astype(int),
    'pred_binary_capacity': (y_pred_calibrated >= capacity_threshold).astype(int),
    'risk_score': (y_pred_calibrated * 100).round(1)  # Convert to 0-100 scale
})

# Save
predictions_path = base_path / 'outputs/results/test_predictions.parquet'
predictions_path.parent.mkdir(parents=True, exist_ok=True)
predictions_df.to_parquet(predictions_path, index=False)

print(f"✓ Predictions saved to: {predictions_path}")
print(f"   Shape: {predictions_df.shape}")

# Also save threshold configuration
threshold_config = {
    'optimal_f1_threshold': float(optimal_f1_threshold),
    'capacity_threshold': float(capacity_threshold),
    'capacity_percentile': capacity_percentile
}
with open(base_path / 'outputs/results/thresholds.json', 'w') as f:
    json.dump(threshold_config, f, indent=2)

# ============================================================================
# Cell 15: Create Results Summary
# ============================================================================
print("\n" + "="*70)
print("CREATING RESULTS SUMMARY")
print("="*70)

results_summary = {
    'experiment_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'data': {
        'n_train': int(len(y_train)),
        'n_calib': int(len(y_calib)),
        'n_test': int(len(y_test)),
        'readmission_rate': float(y_test.mean())
    },
    'features': {
        'n_structured': len(struct_features),
        'n_embedding': len(embedding_features),
        'n_total_fused': len(fused_features)
    },
    'baseline_model': {
        'features': 'structured_only',
        'test_auroc': float(test_auc),
        'test_auprc': float(test_auprc),
        'brier_score': float(brier_score_loss(y_test, y_pred_baseline_test))
    },
    'fused_model': {
        'features': 'structured_and_embeddings',
        'test_auroc': float(test_auc_f),
        'test_auprc': float(test_auprc_f),
        'brier_score_uncalibrated': float(brier_uncalibrated),
        'brier_score_calibrated': float(brier_calibrated)
    },
    'improvements': {
        'auroc_gain': float(auc_improvement),
        'auroc_gain_pct': float(auc_improvement / test_auc * 100),
        'auprc_gain': float(auprc_improvement),
        'auprc_gain_pct': float(auprc_improvement / test_auprc * 100)
    },
    'embedding_contribution': {
        'importance_pct': float(total_importance.get('Embedding', 0) / feat_imp_fused['importance'].sum() * 100)
    },
    'operating_points': {
        'optimal_f1_threshold': float(optimal_f1_threshold),
        'optimal_f1_precision': float(metrics_df.loc[optimal_f1_idx, 'precision']),
        'optimal_f1_recall': float(metrics_df.loc[optimal_f1_idx, 'recall']),
        'capacity_threshold': float(capacity_threshold),
        'capacity_precision': float(capacity_precision),
        'capacity_recall': float(capacity_recall)
    }
}

# Save summary
summary_path = base_path / 'outputs/results/model_results_summary.json'
with open(summary_path, 'w') as f:
    json.dump(results_summary, f, indent=2)

print(f"✓ Results summary saved to: {summary_path}")

# ============================================================================
# Cell 16: Final Summary Report
# ============================================================================
print("\n" + "="*70)
print("MODEL TRAINING COMPLETE!")
print("="*70)

print(f"\n{'='*70}")
print(f"{'FINAL RESULTS SUMMARY':^70}")
print(f"{'='*70}")

print(f"\n📊 Dataset Statistics:")
print(f"   Training samples:   {len(y_train):,}")
print(f"   Calibration samples: {len(y_calib):,}")
print(f"   Test samples:       {len(y_test):,}")
print(f"   Readmission rate:   {y_test.mean():.2%}")

print(f"\n🔧 Features:")
print(f"   Structured features: {len(struct_features)}")
print(f"   Embedding features:  {len(embedding_features)}")
print(f"   Total fused:         {len(fused_features)}")

print(f"\n📈 Model Performance (Test Set):")
print(f"\n   Baseline (Structured Only):")
print(f"      AUROC: {test_auc:.4f}")
print(f"      AUPRC: {test_auprc:.4f}")
print(f"      Brier: {brier_score_loss(y_test, y_pred_baseline_test):.4f}")

print(f"\n   Fused (Structured + Embeddings):")
print(f"      AUROC: {test_auc_f:.4f} ({auc_improvement:+.4f}, {auc_improvement/test_auc*100:+.1f}%)")
print(f"      AUPRC: {test_auprc_f:.4f} ({auprc_improvement:+.4f}, {auprc_improvement/test_auprc*100:+.1f}%)")
print(f"      Brier: {brier_calibrated:.4f} (calibrated)")

if auc_improvement > 0:
    print(f"\n   ✅ Embeddings improve AUROC by {auc_improvement:.4f} ({auc_improvement/test_auc*100:.1f}%)")
else:
    print(f"\n   ⚠️  Embeddings show minimal improvement")

print(f"\n🎯 Operating Points:")
print(f"   Optimal F1 Threshold: {optimal_f1_threshold:.3f}")
print(f"      Precision: {metrics_df.loc[optimal_f1_idx, 'precision']:.3f}")
print(f"      Recall:    {metrics_df.loc[optimal_f1_idx, 'recall']:.3f}")
print(f"      F1:        {metrics_df.loc[optimal_f1_idx, 'f1']:.3f}")

print(f"\n   Capacity Threshold (Top 20%): {capacity_threshold:.3f}")
print(f"      Precision: {capacity_precision:.3f}")
print(f"      Recall:    {capacity_recall:.3f}")
print(f"      N flagged: {capacity_mask.sum()}")

print(f"\n💾 Saved Outputs:")
print(f"   Models:")
print(f"      • outputs/models/baseline_model.txt")
print(f"      • outputs/models/fused_model.txt")
print(f"      • outputs/models/calibrator.pkl")
if SHAP_AVAILABLE:
    print(f"      • outputs/models/shap_values.pkl")

print(f"\n   Results:")
print(f"      • outputs/results/test_predictions.parquet")
print(f"      • outputs/results/model_results_summary.json")
print(f"      • outputs/results/thresholds.json")

print(f"\n   Figures:")
print(f"      • outputs/figures/model_performance_curves.png")
print(f"      • outputs/figures/feature_importance.png")
print(f"      • outputs/figures/calibration_analysis.png")
print(f"      • outputs/figures/operating_point_analysis.png")
if SHAP_AVAILABLE:
    print(f"      • outputs/figures/shap_summary.png")

print(f"\n📝 Next Steps:")
print(f"   1. Review model performance and feature importance")
print(f"   2. Build Streamlit dashboard (06_build_dashboard.py)")
print(f"   3. Implement volume forecasting")
print(f"   4. Create patient risk interface")
print(f"   5. Set up model monitoring and retraining pipeline")

print(f"\n{'='*70}\n")

"""
05_train_models.py (Fixed for SHAP & Colab)
Train and compare baseline vs fused models with proper SHAP support
"""

import sys
import subprocess

print("Checking and installing dependencies...")
required_packages = ['mlflow', 'lightgbm', 'shap', 'imbalanced-learn']
for package in required_packages:
    try:
        __import__(package.replace('-', '_'))
    except ImportError:
        print(f"Installing {package}...")
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import joblib
from datetime import datetime

# ML libraries
import lightgbm as lgb
from sklearn.metrics import (
    roc_auc_score, average_precision_score,
    roc_curve, precision_recall_curve,
    brier_score_loss
)
from sklearn.calibration import calibration_curve
from sklearn.isotonic import IsotonicRegression

# Optional MLflow
try:
    import mlflow
    import mlflow.lightgbm
    USE_MLFLOW = True
    print("✓ MLflow enabled")
except ImportError:
    USE_MLFLOW = False
    print("⚠ MLflow not available - skipping experiment tracking")

import warnings
warnings.filterwarnings('ignore')

print("✓ Setup complete")

# ============================================================================
# Load Configuration and Data
# ============================================================================
base_path = Path('')
processed_dir = base_path / 'data/processed'

# Load config
with open(base_path / 'configs/config.json', 'r') as f:
    config = json.load(f)

# Load structured-only features
print("\nLoading structured-only datasets...")
train_struct = pd.read_parquet(processed_dir / 'train_features.parquet')
calib_struct = pd.read_parquet(processed_dir / 'calibration_features.parquet')
test_struct = pd.read_parquet(processed_dir / 'test_features.parquet')

print(f"✓ Structured features:")
print(f"   Train: {train_struct.shape}")
print(f"   Calib: {calib_struct.shape}")
print(f"   Test:  {test_struct.shape}")

# Load fused features
print("\nLoading fused datasets...")
train_fused = pd.read_parquet(processed_dir / 'train_fused.parquet')
calib_fused = pd.read_parquet(processed_dir / 'calibration_fused.parquet')
test_fused = pd.read_parquet(processed_dir / 'test_fused.parquet')

print(f"✓ Fused features:")
print(f"   Train: {train_fused.shape}")
print(f"   Calib: {calib_fused.shape}")
print(f"   Test:  {test_fused.shape}")

# ============================================================================
# Prepare Features and Labels (FIX DTYPES!)
# ============================================================================
print("\n" + "="*70)
print("PREPARING DATA AND FIXING DTYPES")
print("="*70)

# Remove categorical columns if they exist
categorical_cols = ['AGE_GROUP', 'ETHNICITY_CATEGORY', 'LOS_CATEGORY', 'DISCHARGE_SEASON']
for df in [train_struct, calib_struct, test_struct, train_fused, calib_fused, test_fused]:
    for col in categorical_cols:
        if col in df.columns:
            df.drop(col, axis=1, inplace=True)

# Extract feature names
struct_features = [col for col in train_struct.columns
                   if col not in ['HADM_ID', 'SUBJECT_ID', 'TARGET_READMIT_30']]
fused_features = [col for col in train_fused.columns
                  if col not in ['HADM_ID', 'SUBJECT_ID', 'TARGET_READMIT_30']]
embedding_features = [col for col in fused_features if col.startswith('emb_')]

print(f"\n✓ Feature counts:")
print(f"   Structured features: {len(struct_features)}")
print(f"   Embedding features: {len(embedding_features)}")
print(f"   Total fused features: {len(fused_features)}")

# **CRITICAL FIX**: Convert all features to numeric dtype
print("\n🔧 Converting all features to numeric dtypes...")

def ensure_numeric(df, feature_list):
    """Ensure all features are numeric (float64)"""
    for col in feature_list:
        if col in df.columns:
            # Convert to numeric, coercing errors to NaN
            df[col] = pd.to_numeric(df[col], errors='coerce')
            # Fill NaN with 0
            df[col] = df[col].fillna(0)
            # Convert to float64
            df[col] = df[col].astype(np.float64)
    return df

# Apply to all datasets
train_struct = ensure_numeric(train_struct, struct_features)
calib_struct = ensure_numeric(calib_struct, struct_features)
test_struct = ensure_numeric(test_struct, struct_features)

train_fused = ensure_numeric(train_fused, fused_features)
calib_fused = ensure_numeric(calib_fused, fused_features)
test_fused = ensure_numeric(test_fused, fused_features)

print("✓ All features converted to float64")

# Verify dtypes
print("\n📊 Dtype verification:")
print(f"   Struct features: {train_struct[struct_features].dtypes.unique()}")
print(f"   Fused features: {train_fused[fused_features].dtypes.unique()}")

# Prepare datasets
X_train_struct = train_struct[struct_features].values
y_train = train_struct['TARGET_READMIT_30'].values
X_calib_struct = calib_struct[struct_features].values
y_calib = calib_struct['TARGET_READMIT_30'].values
X_test_struct = test_struct[struct_features].values
y_test = test_struct['TARGET_READMIT_30'].values

X_train_fused = train_fused[fused_features].values
X_calib_fused = calib_fused[fused_features].values
X_test_fused = test_fused[fused_features].values

print(f"\n✓ Structured datasets:")
print(f"   Train: {X_train_struct.shape}, Positive: {y_train.mean():.2%}")
print(f"   Calib: {X_calib_struct.shape}, Positive: {y_calib.mean():.2%}")
print(f"   Test:  {X_test_struct.shape}, Positive: {y_test.mean():.2%}")

print(f"\n✓ Fused datasets:")
print(f"   Train: {X_train_fused.shape}")
print(f"   Calib: {X_calib_fused.shape}")
print(f"   Test:  {X_test_fused.shape}")

# ============================================================================
# Data Augmentation (for small datasets)
# ============================================================================
USE_AUGMENTATION = len(y_train) < 200

if USE_AUGMENTATION:
    from imblearn.over_sampling import SMOTE
    from collections import Counter

    print("\n" + "="*70)
    print("DATA AUGMENTATION (Small Dataset)")
    print("="*70)

    def apply_combined_augmentation(X, y, target_multiplier=4):
        """SMOTE + Gaussian noise augmentation"""
        print(f"\nOriginal: {X.shape}, {Counter(y)}")

        # Step 1: SMOTE
        n_minority = min(Counter(y).values())
        k_neighbors = min(5, n_minority - 1) if n_minority > 1 else 1
        smote = SMOTE(sampling_strategy='auto', k_neighbors=k_neighbors, random_state=42)
        X_balanced, y_balanced = smote.fit_resample(X, y)
        print(f"After SMOTE: {X_balanced.shape}, {Counter(y_balanced)}")

        # Step 2: Add noise copies
        X_augmented = [X_balanced]
        y_augmented = [y_balanced]

        for i in range(target_multiplier - 1):
            noise = np.random.normal(0, 0.03, X_balanced.shape) * X_balanced.std(axis=0)
            X_augmented.append(X_balanced + noise)
            y_augmented.append(y_balanced)

        X_final = np.vstack(X_augmented)
        y_final = np.hstack(y_augmented)
        print(f"Final: {X_final.shape}, {Counter(y_final)}")

        return X_final, y_final

    # **FIX**: Store original y_train before augmentation
    y_train_original = y_train.copy()

    # Augment structured features
    print("\n🔧 Augmenting structured features...")
    X_train_struct, y_train_augmented = apply_combined_augmentation(
        X_train_struct, y_train_original, target_multiplier=4
    )

    # Augment fused features with SAME y_train_original
    print("\n🔧 Augmenting fused features...")
    X_train_fused, _ = apply_combined_augmentation(
        X_train_fused, y_train_original, target_multiplier=4
    )

    # Update y_train to augmented version
    y_train = y_train_augmented

    print(f"\n✓ Augmentation complete")
    print(f"   Final train size: {len(y_train)} samples")
else:
    print(f"\n✓ Skipping augmentation (sufficient data: {len(y_train)} samples)")

# ============================================================================
# Setup MLflow
# ============================================================================
if USE_MLFLOW:
    mlflow.set_tracking_uri(str(base_path / 'mlruns'))
    mlflow.set_experiment('TRANCE_Readmission_Prediction')
    print("\n✓ MLflow tracking initialized")

# ============================================================================
# Train Baseline Model
# ============================================================================
print("\n" + "="*70)
print("TRAINING BASELINE MODEL (STRUCTURED ONLY)")
print("="*70)

if USE_MLFLOW:
    mlflow.start_run(run_name="Baseline_Structured_Only")

params_baseline = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 7,
    'max_depth': 3,
    'learning_rate': 0.01,
    'n_estimators': 200,
    'min_child_samples': 10,
    'reg_alpha': 0.5,
    'reg_lambda': 0.5,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': -1,
    'random_state': 42
}

if USE_MLFLOW:
    mlflow.log_params(params_baseline)

train_data = lgb.Dataset(X_train_struct, label=y_train)
valid_data = lgb.Dataset(X_calib_struct, label=y_calib, reference=train_data)

print("Training baseline model...")
model_baseline = lgb.train(
    params_baseline,
    train_data,
    num_boost_round=1000,
    valid_sets=[train_data, valid_data],
    valid_names=['train', 'calib'],
    callbacks=[
        lgb.early_stopping(stopping_rounds=50),
        lgb.log_evaluation(period=100)
    ]
)

print(f"✓ Training complete")

# Predictions
y_pred_baseline_train = model_baseline.predict(X_train_struct)
y_pred_baseline_calib = model_baseline.predict(X_calib_struct)
y_pred_baseline_test = model_baseline.predict(X_test_struct)

# Metrics
train_auc = roc_auc_score(y_train, y_pred_baseline_train)
calib_auc = roc_auc_score(y_calib, y_pred_baseline_calib)
test_auc = roc_auc_score(y_test, y_pred_baseline_test)

train_auprc = average_precision_score(y_train, y_pred_baseline_train)
calib_auprc = average_precision_score(y_calib, y_pred_baseline_calib)
test_auprc = average_precision_score(y_test, y_pred_baseline_test)

if USE_MLFLOW:
    mlflow.log_metric("train_auc", train_auc)
    mlflow.log_metric("test_auc", test_auc)
    mlflow.log_metric("test_auprc", test_auprc)

print(f"\nBaseline Model Performance:")
print(f"  Train AUROC: {train_auc:.4f} | AUPRC: {train_auprc:.4f}")
print(f"  Calib AUROC: {calib_auc:.4f} | AUPRC: {calib_auprc:.4f}")
print(f"  Test  AUROC: {test_auc:.4f} | AUPRC: {test_auprc:.4f}")

# Save model
model_path = base_path / 'outputs/models/baseline_model.txt'
model_path.parent.mkdir(parents=True, exist_ok=True)
model_baseline.save_model(str(model_path))
print(f"\n✓ Model saved to: {model_path}")

if USE_MLFLOW:
    mlflow.end_run()

# ============================================================================
# Train Fused Model
# ============================================================================
print("\n" + "="*70)
print("TRAINING FUSED MODEL (STRUCTURED + EMBEDDINGS)")
print("="*70)

if USE_MLFLOW:
    mlflow.start_run(run_name="Fused_Structured_Embeddings")
    mlflow.log_params(params_baseline)

params_fused = params_baseline.copy()

train_data = lgb.Dataset(X_train_fused, label=y_train)
valid_data = lgb.Dataset(X_calib_fused, label=y_calib, reference=train_data)

print("Training fused model...")
model_fused = lgb.train(
    params_fused,
    train_data,
    num_boost_round=1000,
    valid_sets=[train_data, valid_data],
    valid_names=['train', 'valid'],
    callbacks=[
        lgb.early_stopping(stopping_rounds=50),
        lgb.log_evaluation(period=100)
    ]
)

print(f"✓ Training complete")

# Predictions
y_pred_fused_train = model_fused.predict(X_train_fused)
y_pred_fused_calib = model_fused.predict(X_calib_fused)
y_pred_fused_test = model_fused.predict(X_test_fused)

# Metrics
train_auc_f = roc_auc_score(y_train, y_pred_fused_train)
calib_auc_f = roc_auc_score(y_calib, y_pred_fused_calib)
test_auc_f = roc_auc_score(y_test, y_pred_fused_test)

train_auprc_f = average_precision_score(y_train, y_pred_fused_train)
calib_auprc_f = average_precision_score(y_calib, y_pred_fused_calib)
test_auprc_f = average_precision_score(y_test, y_pred_fused_test)

if USE_MLFLOW:
    mlflow.log_metric("train_auc", train_auc_f)
    mlflow.log_metric("test_auc", test_auc_f)
    mlflow.log_metric("test_auprc", test_auprc_f)

print(f"\nFused Model Performance:")
print(f"  Train AUROC: {train_auc_f:.4f} | AUPRC: {train_auprc_f:.4f}")
print(f"  Calib AUROC: {calib_auc_f:.4f} | AUPRC: {calib_auprc_f:.4f}")
print(f"  Test  AUROC: {test_auc_f:.4f} | AUPRC: {test_auprc_f:.4f}")

# Save model
model_path = base_path / 'outputs/models/fused_model.txt'
model_fused.save_model(str(model_path))
print(f"\n✓ Model saved to: {model_path}")

if USE_MLFLOW:
    mlflow.end_run()

# ============================================================================
# Model Comparison
# ============================================================================
print("\n" + "="*70)
print("MODEL COMPARISON")
print("="*70)

auc_improvement = test_auc_f - test_auc
auprc_improvement = test_auprc_f - test_auprc

print("\nTest Set Performance Comparison:")
print("-" * 70)
print(f"{'Metric':<20} {'Baseline':<12} {'Fused':<12} {'Improvement':<12}")
print("-" * 70)
print(f"{'AUROC':<20} {test_auc:<12.4f} {test_auc_f:<12.4f} {auc_improvement:+.4f} ({auc_improvement/test_auc*100:+.2f}%)")
print(f"{'AUPRC':<20} {test_auprc:<12.4f} {test_auprc_f:<12.4f} {auprc_improvement:+.4f} ({auprc_improvement/test_auprc*100:+.2f}%)")
print("-" * 70)

# ============================================================================
# Performance Curves
# ============================================================================
print("\n" + "="*70)
print("GENERATING PERFORMANCE CURVES")
print("="*70)

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# ROC Curve
fpr_baseline, tpr_baseline, _ = roc_curve(y_test, y_pred_baseline_test)
fpr_fused, tpr_fused, _ = roc_curve(y_test, y_pred_fused_test)

axes[0].plot(fpr_baseline, tpr_baseline, label=f'Baseline (AUROC={test_auc:.3f})', linewidth=2)
axes[0].plot(fpr_fused, tpr_fused, label=f'Fused (AUROC={test_auc_f:.3f})', linewidth=2)
axes[0].plot([0, 1], [0, 1], 'k--', label='Random', alpha=0.3)
axes[0].set_xlabel('False Positive Rate')
axes[0].set_ylabel('True Positive Rate')
axes[0].set_title('ROC Curve - Test Set')
axes[0].legend()
axes[0].grid(alpha=0.3)

# PR Curve
precision_baseline, recall_baseline, _ = precision_recall_curve(y_test, y_pred_baseline_test)
precision_fused, recall_fused, _ = precision_recall_curve(y_test, y_pred_fused_test)

axes[1].plot(recall_baseline, precision_baseline, label=f'Baseline (AUPRC={test_auprc:.3f})', linewidth=2)
axes[1].plot(recall_fused, precision_fused, label=f'Fused (AUPRC={test_auprc_f:.3f})', linewidth=2)
axes[1].axhline(y=y_test.mean(), color='k', linestyle='--', label='Baseline Rate', alpha=0.3)
axes[1].set_xlabel('Recall')
axes[1].set_ylabel('Precision')
axes[1].set_title('Precision-Recall Curve')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
fig_path = base_path / 'outputs/figures/model_performance_curves.png'
fig_path.parent.mkdir(parents=True, exist_ok=True)
plt.savefig(fig_path, dpi=300, bbox_inches='tight')
print(f"✓ Performance curves saved to: {fig_path}")
plt.close()

# ============================================================================
# Feature Importance
# ============================================================================
print("\n" + "="*70)
print("FEATURE IMPORTANCE ANALYSIS")
print("="*70)

importance_baseline = model_baseline.feature_importance(importance_type='gain')
importance_fused = model_fused.feature_importance(importance_type='gain')

feat_imp_baseline = pd.DataFrame({
    'feature': struct_features,
    'importance': importance_baseline
}).sort_values('importance', ascending=False)

feat_imp_fused = pd.DataFrame({
    'feature': fused_features,
    'importance': importance_fused
}).sort_values('importance', ascending=False)

feat_imp_fused['type'] = feat_imp_fused['feature'].apply(
    lambda x: 'Embedding' if x.startswith('emb_') else 'Structured'
)

total_importance = feat_imp_fused.groupby('type')['importance'].sum()
print("\nImportance Distribution in Fused Model:")
print(f"  Structured: {total_importance.get('Structured', 0):.0f} ({total_importance.get('Structured', 0)/feat_imp_fused['importance'].sum()*100:.1f}%)")
print(f"  Embedding:  {total_importance.get('Embedding', 0):.0f} ({total_importance.get('Embedding', 0)/feat_imp_fused['importance'].sum()*100:.1f}%)")

print("\nTop 15 Features (Fused Model):")
print(feat_imp_fused.head(15).to_string(index=False))

# Visualize
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

top_n = 20
feat_imp_baseline.head(top_n).plot.barh(x='feature', y='importance', ax=axes[0], legend=False)
axes[0].set_title('Top 20 Features - Baseline Model')
axes[0].set_xlabel('Importance (Gain)')
axes[0].invert_yaxis()

top_fused = feat_imp_fused.head(top_n)
colors = ['steelblue' if t == 'Structured' else 'orange' for t in top_fused['type']]
axes[1].barh(range(len(top_fused)), top_fused['importance'], color=colors)
axes[1].set_yticks(range(len(top_fused)))
axes[1].set_yticklabels(top_fused['feature'])
axes[1].set_title('Top 20 Features - Fused Model')
axes[1].set_xlabel('Importance (Gain)')
axes[1].invert_yaxis()

plt.tight_layout()
plt.savefig(base_path / 'outputs/figures/feature_importance.png', dpi=300, bbox_inches='tight')
print("\n✓ Feature importance plots saved")
plt.close()

# ============================================================================
# SHAP Analysis (FIXED!)
# ============================================================================
print("\n" + "="*70)
print("GENERATING SHAP VALUES (FIXED VERSION)")
print("="*70)

try:
    import shap

    # Use subset for speed
    n_shap_samples = min(100, len(X_test_fused))

    # **CRITICAL FIX**: Create DataFrame with proper numeric dtypes
    print(f"Computing SHAP values for {n_shap_samples} samples...")

    # Sample indices
    sample_indices = np.random.choice(len(X_test_fused), size=n_shap_samples, replace=False)
    X_shap = X_test_fused[sample_indices]

    # Create DataFrame with EXPLICIT float64 dtype
    X_shap_df = pd.DataFrame(
        X_shap,
        columns=fused_features
    ).astype(np.float64)  # Ensure all columns are float64

    # Verify dtypes
    print(f"✓ SHAP data dtypes: {X_shap_df.dtypes.unique()}")

    # Create explainer
    explainer = shap.TreeExplainer(model_fused)
    shap_values = explainer.shap_values(X_shap_df)

    # Handle list output
    if isinstance(shap_values, list):
        shap_values = shap_values[1]

    print("✓ SHAP values computed successfully!")

    # Save SHAP data
    shap_data = {
        'shap_values': shap_values,
        'data': X_shap_df.values,
        'feature_names': fused_features,
        'expected_value': explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value
    }

    shap_path = base_path / 'outputs/models/shap_values.pkl'
    joblib.dump(shap_data, shap_path)
    print(f"✓ SHAP values saved to: {shap_path}")

    # SHAP summary plot
    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X_shap_df, max_display=20, show=False)
    plt.tight_layout()
    plt.savefig(base_path / 'outputs/figures/shap_summary.png', dpi=300, bbox_inches='tight')
    print("✓ SHAP summary plot saved")
    plt.close()

    SHAP_SUCCESS = True

except ImportError:
    print("⚠ SHAP not available - install with: pip install shap")
    SHAP_SUCCESS = False
except Exception as e:
    print(f"⚠ SHAP analysis failed: {e}")
    print("  Continuing without SHAP...")
    SHAP_SUCCESS = False

# ============================================================================
# Probability Calibration
# ============================================================================
print("\n" + "="*70)
print("PROBABILITY CALIBRATION")
print("="*70)

print("Fitting isotonic regression calibrator...")
calibrator = IsotonicRegression(out_of_bounds='clip')
calibrator.fit(y_pred_fused_calib, y_calib)

y_pred_calibrated = calibrator.predict(y_pred_fused_test)

brier_uncalibrated = brier_score_loss(y_test, y_pred_fused_test)
brier_calibrated = brier_score_loss(y_test, y_pred_calibrated)

print(f"\nBrier Score:")
print(f"  Uncalibrated: {brier_uncalibrated:.4f}")
print(f"  Calibrated:   {brier_calibrated:.4f}")
print(f"  Improvement:  {brier_uncalibrated - brier_calibrated:.4f}")

# Calibration curve
fraction_of_positives_uncal, mean_predicted_value_uncal = calibration_curve(
    y_test, y_pred_fused_test, n_bins=10, strategy='uniform'
)
fraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(
    y_test, y_pred_calibrated, n_bins=10, strategy='uniform'
)

# Plot
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect calibration')
axes[0].plot(mean_predicted_value_uncal, fraction_of_positives_uncal,
             'o-', label=f'Uncalibrated (Brier={brier_uncalibrated:.3f})')
axes[0].plot(mean_predicted_value_cal, fraction_of_positives_cal,
             's-', label=f'Calibrated (Brier={brier_calibrated:.3f})')
axes[0].set_xlabel('Mean Predicted Probability')
axes[0].set_ylabel('Observed Frequency')
axes[0].set_title('Reliability Diagram')
axes[0].legend()
axes[0].grid(alpha=0.3)

axes[1].hist(y_pred_fused_test, bins=30, alpha=0.5, label='Uncalibrated', edgecolor='black')
axes[1].hist(y_pred_calibrated, bins=30, alpha=0.5, label='Calibrated', edgecolor='black')
axes[1].set_xlabel('Predicted Probability')
axes[1].set_ylabel('Count')
axes[1].set_title('Distribution of Predictions')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig(base_path / 'outputs/figures/calibration_analysis.png', dpi=300, bbox_inches='tight')
print("\n✓ Calibration plots saved")
plt.close()

joblib.dump(calibrator, base_path / 'outputs/models/calibrator.pkl')
print("✓ Calibrator saved")

# ============================================================================
# Operating Point Analysis
# ============================================================================
print("\n" + "="*70)
print("OPERATING POINT ANALYSIS")
print("="*70)

thresholds = np.linspace(0, 1, 100)
metrics_at_threshold = []

for thresh in thresholds:
    y_pred_binary = (y_pred_calibrated >= thresh).astype(int)

    if y_pred_binary.sum() == 0:
        continue

    tp = ((y_pred_binary == 1) & (y_test == 1)).sum()
    fp = ((y_pred_binary == 1) & (y_test == 0)).sum()
    fn = ((y_pred_binary == 0) & (y_test == 1)).sum()

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    metrics_at_threshold.append({
        'threshold': thresh,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'n_predicted_positive': y_pred_binary.sum()
    })

metrics_df = pd.DataFrame(metrics_at_threshold)

optimal_f1_idx = metrics_df['f1'].idxmax()
optimal_f1_threshold = metrics_df.loc[optimal_f1_idx, 'threshold']

print(f"Optimal F1 Threshold: {optimal_f1_threshold:.3f}")
print(f"  Precision: {metrics_df.loc[optimal_f1_idx, 'precision']:.3f}")
print(f"  Recall:    {metrics_df.loc[optimal_f1_idx, 'recall']:.3f}")
print(f"  F1 Score:  {metrics_df.loc[optimal_f1_idx, 'f1']:.3f}")

# Capacity-based threshold
capacity_percentile = 80
capacity_threshold = np.percentile(y_pred_calibrated, capacity_percentile)

capacity_mask = y_pred_calibrated >= capacity_threshold
capacity_precision = y_test[capacity_mask].mean() if capacity_mask.sum() > 0 else 0
capacity_recall = y_test[capacity_mask].sum() / y_test.sum() if y_test.sum() > 0 else 0

print(f"\nCapacity-Based Threshold (top {100-capacity_percentile}%): {capacity_threshold:.3f}")
print(f"  Precision: {capacity_precision:.3f}")
print(f"  Recall:    {capacity_recall:.3f}")
print(f"  N flagged: {capacity_mask.sum()} ({capacity_mask.mean()*100:.1f}%)")

# Plot
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

axes[0].plot(metrics_df['threshold'], metrics_df['precision'], label='Precision')
axes[0].plot(metrics_df['threshold'], metrics_df['recall'], label='Recall')
axes[0].plot(metrics_df['threshold'], metrics_df['f1'], label='F1 Score', linewidth=2)
axes[0].axvline(optimal_f1_threshold, color='red', linestyle='--',
                label=f'Optimal F1 ({optimal_f1_threshold:.3f})')
axes[0].set_xlabel('Threshold')
axes[0].set_ylabel('Score')
axes[0].set_title('Precision-Recall Tradeoff')
axes[0].legend()
axes[0].grid(alpha=0.3)

axes[1].plot(metrics_df['threshold'], metrics_df['n_predicted_positive'])
axes[1].axhline(len(y_test) * 0.2, color='red', linestyle='--', label='20% capacity')
axes[1].set_xlabel('Threshold')
axes[1].set_ylabel('Number Flagged')
axes[1].set_title('Workload vs Threshold')
axes[1].legend()
axes[1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig(base_path / 'outputs/figures/operating_point_analysis.png', dpi=300, bbox_inches='tight')
print("\n✓ Operating point plots saved")
plt.close()

# ============================================================================
# Save Predictions
# ============================================================================
print("\n" + "="*70)
print("SAVING PREDICTIONS")
print("="*70)

predictions_df = pd.DataFrame({
    'HADM_ID': test_fused['HADM_ID'].values,
    'SUBJECT_ID': test_fused['SUBJECT_ID'].values,
    'true_label': y_test,
    'pred_prob_baseline': y_pred_baseline_test,
    'pred_prob_fused_raw': y_pred_fused_test,
    'pred_prob_fused_calibrated': y_pred_calibrated,
    'pred_binary_f1_optimal': (y_pred_calibrated >= optimal_f1_threshold).astype(int),
    'pred_binary_capacity': (y_pred_calibrated >= capacity_threshold).astype(int),
    'risk_score': (y_pred_calibrated * 100).round(1)
})

predictions_path = base_path / 'outputs/results/test_predictions.parquet'
predictions_path.parent.mkdir(parents=True, exist_ok=True)
predictions_df.to_parquet(predictions_path, index=False)

print(f"✓ Predictions saved to: {predictions_path}")
print(f"   Shape: {predictions_df.shape}")

# Save thresholds
threshold_config = {
    'optimal_f1_threshold': float(optimal_f1_threshold),
    'capacity_threshold': float(capacity_threshold),
    'capacity_percentile': capacity_percentile
}
with open(base_path / 'outputs/results/thresholds.json', 'w') as f:
    json.dump(threshold_config, f, indent=2)

# ============================================================================
# Save Feature Info for Dashboard
# ============================================================================
print("\n" + "="*70)
print("SAVING FEATURE INFO FOR DASHBOARD")
print("="*70)

feature_info = {
    'feature_names': fused_features,
    'struct_features': struct_features,
    'embedding_features': embedding_features,
    'n_features': len(fused_features),
    'n_struct': len(struct_features),
    'n_embeddings': len(embedding_features)
}

feature_info_path = base_path / 'data/processed/feature_info.json'
with open(feature_info_path, 'w') as f:
    json.dump(feature_info, f, indent=2)

print(f"✓ Feature info saved to: {feature_info_path}")

# ============================================================================
# Create Results Summary
# ============================================================================
print("\n" + "="*70)
print("CREATING RESULTS SUMMARY")
print("="*70)

results_summary = {
    'experiment_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'data': {
        'n_train': int(len(y_train)),
        'n_calib': int(len(y_calib)),
        'n_test': int(len(y_test)),
        'readmission_rate': float(y_test.mean())
    },
    'features': {
        'n_structured': len(struct_features),
        'n_embedding': len(embedding_features),
        'n_total_fused': len(fused_features)
    },
    'baseline_model': {
        'features': 'structured_only',
        'test_auroc': float(test_auc),
        'test_auprc': float(test_auprc),
        'brier_score': float(brier_score_loss(y_test, y_pred_baseline_test))
    },
    'fused_model': {
        'features': 'structured_and_embeddings',
        'test_auroc': float(test_auc_f),
        'test_auprc': float(test_auprc_f),
        'brier_score_uncalibrated': float(brier_uncalibrated),
        'brier_score_calibrated': float(brier_calibrated)
    },
    'improvements': {
        'auroc_gain': float(auc_improvement),
        'auroc_gain_pct': float(auc_improvement / test_auc * 100) if test_auc > 0 else 0.0,
        'auprc_gain': float(auprc_improvement),
        'auprc_gain_pct': float(auprc_improvement / test_auprc * 100) if test_auprc > 0 else 0.0
    },
    'embedding_contribution': {
        'importance_pct': float(total_importance.get('Embedding', 0) / feat_imp_fused['importance'].sum() * 100) if feat_imp_fused['importance'].sum() > 0 else 0.0
    },
    'operating_points': {
        'optimal_f1_threshold': float(optimal_f1_threshold),
        'optimal_f1_precision': float(metrics_df.loc[optimal_f1_idx, 'precision']),
        'optimal_f1_recall': float(metrics_df.loc[optimal_f1_idx, 'recall']),
        'capacity_threshold': float(capacity_threshold),
        'capacity_precision': float(capacity_precision),
        'capacity_recall': float(capacity_recall)
    },
    'shap_available': SHAP_SUCCESS
}

summary_path = base_path / 'outputs/results/model_results_summary.json'
with open(summary_path, 'w') as f:
    json.dump(results_summary, f, indent=2)

print(f"✓ Results summary saved to: {summary_path}")

# ============================================================================
# Final Summary Report
# ============================================================================
print("\n" + "="*70)
print("MODEL TRAINING COMPLETE!")
print("="*70)

print(f"\n{'='*70}")
print(f"{'FINAL RESULTS SUMMARY':^70}")
print(f"{'='*70}")

print(f"\n📊 Dataset Statistics:")
print(f"   Training samples:   {len(y_train):,}")
print(f"   Calibration samples: {len(y_calib):,}")
print(f"   Test samples:       {len(y_test):,}")
print(f"   Readmission rate:   {y_test.mean():.2%}")

print(f"\n🔧 Features:")
print(f"   Structured features: {len(struct_features)}")
print(f"   Embedding features:  {len(embedding_features)}")
print(f"   Total fused:         {len(fused_features)}")

print(f"\n📈 Model Performance (Test Set):")
print(f"\n   Baseline (Structured Only):")
print(f"      AUROC: {test_auc:.4f}")
print(f"      AUPRC: {test_auprc:.4f}")
print(f"      Brier: {brier_score_loss(y_test, y_pred_baseline_test):.4f}")

print(f"\n   Fused (Structured + Embeddings):")
print(f"      AUROC: {test_auc_f:.4f} ({auc_improvement:+.4f}, {auc_improvement/test_auc*100:+.1f}%)")
print(f"      AUPRC: {test_auprc_f:.4f} ({auprc_improvement:+.4f}, {auprc_improvement/test_auprc*100:+.1f}%)")
print(f"      Brier: {brier_calibrated:.4f} (calibrated)")

if auc_improvement > 0:
    print(f"\n   ✅ Embeddings improve AUROC by {auc_improvement:.4f} ({auc_improvement/test_auc*100:.1f}%)")
else:
    print(f"\n   ⚠️  Embeddings show minimal improvement")

print(f"\n🎯 Operating Points:")
print(f"   Optimal F1 Threshold: {optimal_f1_threshold:.3f}")
print(f"      Precision: {metrics_df.loc[optimal_f1_idx, 'precision']:.3f}")
print(f"      Recall:    {metrics_df.loc[optimal_f1_idx, 'recall']:.3f}")
print(f"      F1:        {metrics_df.loc[optimal_f1_idx, 'f1']:.3f}")

print(f"\n   Capacity Threshold (Top 20%): {capacity_threshold:.3f}")
print(f"      Precision: {capacity_precision:.3f}")
print(f"      Recall:    {capacity_recall:.3f}")
print(f"      N flagged: {capacity_mask.sum()}")

print(f"\n🔍 SHAP Analysis:")
if SHAP_SUCCESS:
    print(f"   ✅ SHAP values computed successfully")
    print(f"   ✅ Feature interpretability available in dashboard")
else:
    print(f"   ⚠️  SHAP analysis failed - limited interpretability")

print(f"\n💾 Saved Outputs:")
print(f"   Models:")
print(f"      • outputs/models/baseline_model.txt")
print(f"      • outputs/models/fused_model.txt")
print(f"      • outputs/models/calibrator.pkl")
if SHAP_SUCCESS:
    print(f"      • outputs/models/shap_values.pkl ✅")

print(f"\n   Results:")
print(f"      • outputs/results/test_predictions.parquet")
print(f"      • outputs/results/model_results_summary.json")
print(f"      • outputs/results/thresholds.json")

print(f"\n   Figures:")
print(f"      • outputs/figures/model_performance_curves.png")
print(f"      • outputs/figures/feature_importance.png")
print(f"      • outputs/figures/calibration_analysis.png")
print(f"      • outputs/figures/operating_point_analysis.png")
if SHAP_SUCCESS:
    print(f"      • outputs/figures/shap_summary.png ✅")

print(f"\n   Data:")
print(f"      • data/processed/feature_info.json")

print(f"\n📝 Next Steps:")
print(f"   1. ✅ Models trained and saved")
print(f"   2. ✅ SHAP values generated" if SHAP_SUCCESS else "   2. ⚠️  SHAP values missing (optional)")
print(f"   3. 🚀 Run dashboard: streamlit run app.py")
print(f"   4. 📊 Review model performance and insights")
print(f"   5. 🎯 Use patient risk interface for predictions")

print(f"\n{'='*70}")
print(f"\n✅ All done! Your dashboard is ready to run.")
print(f"   Execute: streamlit run app.py")
print(f"\n{'='*70}\n")

!pip install streamlit pyngrok -q

!ngrok config add-authtoken "34M5P1Ls7LeFnvLAWZADjIa6PnP_5WLXQzJr3knrH2qTWvvqe"

from pyngrok import ngrok

# Kill existing tunnels if any
ngrok.kill()

# Create a tunnel to port 8501 (Streamlit default)
public_url = ngrok.connect(8501).public_url
print(f"🌍 Public URL: {public_url}")

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%writefile app.py
# """
# TRANCE Dashboard - Main Application
# Save as: src/dashboard/app.py
# Run with: streamlit run src/dashboard/app.py
# """
# 
# import streamlit as st
# import pandas as pd
# import numpy as np
# import plotly.graph_objects as go
# import plotly.express as px
# from pathlib import Path
# import json
# import joblib
# from datetime import datetime, timedelta
# 
# # Page configuration
# st.set_page_config(
#     page_title="TRANCE - Readmission Prediction System",
#     page_icon="🏥",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )
# 
# # Custom CSS
# st.markdown("""
# <style>
#     .main-header {
#         font-size: 2.5rem;
#         font-weight: bold;
#         color: #1f77b4;
#         text-align: center;
#         padding: 1rem 0;
#     }
#     .metric-card {
#         background-color: #f0f2f6;
#         padding: 1rem;
#         border-radius: 0.5rem;
#         border-left: 4px solid #1f77b4;
#     }
#     .risk-high {
#         background-color: #ffebee;
#         border-left-color: #d32f2f;
#     }
#     .risk-medium {
#         background-color: #fff3e0;
#         border-left-color: #f57c00;
#     }
#     .risk-low {
#         background-color: #e8f5e9;
#         border-left-color: #388e3c;
#     }
# </style>
# """, unsafe_allow_html=True)
# 
# # Initialize paths
# @st.cache_resource
# def get_paths():
#     base_path = Path('')
#     return {
#         'results': base_path / 'outputs/results',
#         'models': base_path / 'outputs/models',
#         'processed': base_path / 'data/processed'
#     }
# 
# paths = get_paths()
# 
# # Load data
# @st.cache_data
# def load_data():
#     """Load all necessary data"""
#     try:
#         predictions = pd.read_parquet(paths['results'] / 'test_predictions.parquet')
# 
#         with open(paths['results'] / 'model_results_summary.json', 'r') as f:
#             results_summary = json.load(f)
# 
#         # Load test features for patient details
#         test_features = pd.read_parquet(paths['processed'] / 'test_fused.parquet')
# 
#         # Merge predictions with features
#         full_data = predictions.merge(test_features, on=['HADM_ID', 'SUBJECT_ID'])
# 
#         return predictions, results_summary, full_data
#     except Exception as e:
#         st.error(f"Error loading data: {e}")
#         return None, None, None
# 
# predictions, results_summary, full_data = load_data()
# 
# # Sidebar navigation
# st.sidebar.markdown("# 🏥 TRANCE")
# st.sidebar.markdown("### Readmission Prediction System")
# st.sidebar.markdown("---")
# 
# page = st.sidebar.radio(
#     "Navigation",
#     ["📊 Executive Overview", "📈 Volume Forecasting", "🎯 Patient Risk Dashboard", "🔍 Model Monitoring"]
# )
# 
# st.sidebar.markdown("---")
# st.sidebar.markdown("### About")
# st.sidebar.info(
#     "TRANCE uses machine learning with clinical text embeddings "
#     "to predict 30-day hospital readmissions."
# )
# 
# # ==================== PAGE 1: EXECUTIVE OVERVIEW ====================
# if page == "📊 Executive Overview":
#     st.markdown("<h1 class='main-header'>📊 Executive Overview</h1>", unsafe_allow_html=True)
# 
#     if results_summary is None:
#         st.error("Unable to load results. Please ensure models have been trained.")
#         st.stop()
# 
#     # KPI Cards
#     col1, col2, col3, col4 = st.columns(4)
# 
#     with col1:
#         auroc = results_summary['fused_model']['test_auroc']
#         color = "🟢" if auroc >= 0.75 else "🟡" if auroc >= 0.70 else "🔴"
#         st.metric("Model AUROC", f"{auroc:.3f}", help="Area Under ROC Curve")
#         st.caption(f"{color} {'Excellent' if auroc >= 0.75 else 'Good' if auroc >= 0.70 else 'Fair'} Performance")
# 
#     with col2:
#         auprc = results_summary['fused_model']['test_auprc']
#         st.metric("Model AUPRC", f"{auprc:.3f}", help="Area Under Precision-Recall Curve")
# 
#     with col3:
#         brier = results_summary['fused_model']['brier_score_calibrated']
#         color = "🟢" if brier <= 0.15 else "🟡" if brier <= 0.20 else "🔴"
#         st.metric("Brier Score", f"{brier:.3f}", help="Lower is better")
#         st.caption(f"{color} {'Well' if brier <= 0.15 else 'Moderately' if brier <= 0.20 else 'Poorly'} Calibrated")
# 
#     with col4:
#         improvement = results_summary['improvements']['auroc_gain_pct']
#         st.metric("Embedding Boost", f"+{improvement:.1f}%", help="AUROC improvement from text embeddings")
# 
#     st.markdown("---")
# 
#     # Performance visualizations
#     col1, col2 = st.columns(2)
# 
#     with col1:
#         st.subheader("ROC Curve")
# 
#         # Create synthetic ROC data (in practice, load actual curve data)
#         fpr = np.linspace(0, 1, 100)
#         tpr = np.power(fpr, 0.6)  # Synthetic curve for demo
# 
#         fig = go.Figure()
#         fig.add_trace(go.Scatter(
#             x=fpr, y=tpr,
#             name=f"TRANCE Model (AUROC={auroc:.3f})",
#             line=dict(color='steelblue', width=3)
#         ))
#         fig.add_trace(go.Scatter(
#             x=[0, 1], y=[0, 1],
#             name="Random Classifier",
#             line=dict(color='gray', width=2, dash='dash')
#         ))
#         fig.update_layout(
#             xaxis_title="False Positive Rate",
#             yaxis_title="True Positive Rate",
#             height=400,
#             hovermode='x unified'
#         )
#         st.plotly_chart(fig, use_container_width=True)
# 
#     with col2:
#         st.subheader("Model Comparison")
# 
#         baseline_auroc = results_summary['baseline_model']['test_auroc']
#         fused_auroc = results_summary['fused_model']['test_auroc']
# 
#         fig = go.Figure()
#         fig.add_trace(go.Bar(
#             x=['Structured Only', 'Structured + Embeddings'],
#             y=[baseline_auroc, fused_auroc],
#             text=[f"{baseline_auroc:.3f}", f"{fused_auroc:.3f}"],
#             textposition='auto',
#             marker_color=['lightblue', 'steelblue']
#         ))
#         fig.update_layout(
#             yaxis_title="AUROC",
#             yaxis_range=[0, 1],
#             height=400,
#             showlegend=False
#         )
#         st.plotly_chart(fig, use_container_width=True)
# 
#     # Recent performance table
#     st.subheader("Performance Metrics Summary")
# 
#     metrics_df = pd.DataFrame({
#         'Metric': ['AUROC', 'AUPRC', 'Brier Score', 'Embedding Contribution'],
#         'Baseline': [
#             f"{baseline_auroc:.4f}",
#             f"{results_summary['baseline_model']['test_auprc']:.4f}",
#             f"{results_summary['baseline_model']['brier_score']:.4f}",
#             "N/A"
#         ],
#         'Fused Model': [
#             f"{fused_auroc:.4f}",
#             f"{auprc:.4f}",
#             f"{brier:.4f}",
#             f"{results_summary['embedding_contribution']['importance_pct']:.1f}%"
#         ],
#         'Improvement': [
#             f"+{results_summary['improvements']['auroc_gain']:.4f}",
#             f"+{results_summary['improvements']['auprc_gain']:.4f}",
#             f"{brier - results_summary['baseline_model']['brier_score']:.4f}",
#             "—"
#         ]
#     })
# 
#     st.dataframe(metrics_df, use_container_width=True, hide_index=True)
# 
# # ==================== PAGE 2: VOLUME FORECASTING ====================
# elif page == "📈 Volume Forecasting":
#     st.markdown("<h1 class='main-header'>📈 Volume Forecasting & Capacity Planning</h1>", unsafe_allow_html=True)
# 
#     if predictions is None:
#         st.error("Unable to load predictions.")
#         st.stop()
# 
#     # Simulate temporal data (in practice, you'd have actual discharge dates)
#     np.random.seed(42)
#     predictions['discharge_date'] = pd.date_range(start='2024-01-01', periods=len(predictions), freq='H')
#     predictions['discharge_date'] = predictions['discharge_date'].dt.date
# 
#     # Aggregate by day
#     daily_forecast = predictions.groupby('discharge_date').agg({
#         'pred_prob_fused_calibrated': 'sum',  # Expected readmissions
#         'true_label': 'sum'  # Actual readmissions
#     }).reset_index()
# 
#     daily_forecast.columns = ['Date', 'Predicted Readmissions', 'Actual Readmissions']
#     daily_forecast['Forecast Error'] = daily_forecast['Predicted Readmissions'] - daily_forecast['Actual Readmissions']
# 
#     # Forecast horizon selector
#     st.sidebar.markdown("### Forecast Settings")
#     horizon = st.sidebar.selectbox("Forecast Horizon", ["7-day", "14-day", "30-day"], index=0)
#     horizon_days = int(horizon.split('-')[0])
# 
#     # Show next N days
#     forecast_window = daily_forecast.head(horizon_days)
# 
#     # Summary statistics
#     col1, col2, col3, col4 = st.columns(4)
# 
#     with col1:
#         total_predicted = forecast_window['Predicted Readmissions'].sum()
#         st.metric(
#             f"Predicted ({horizon})",
#             f"{total_predicted:.0f} patients",
#             help=f"Expected readmissions over next {horizon_days} days"
#         )
# 
#     with col2:
#         peak_day = forecast_window.loc[forecast_window['Predicted Readmissions'].idxmax()]
#         st.metric(
#             "Peak Day",
#             f"{peak_day['Date']}",
#             delta=f"{peak_day['Predicted Readmissions']:.0f} patients"
#         )
# 
#     with col3:
#         avg_daily = forecast_window['Predicted Readmissions'].mean()
#         st.metric(
#             "Avg Daily",
#             f"{avg_daily:.1f} patients",
#             help="Average predicted readmissions per day"
#         )
# 
#     with col4:
#         if forecast_window['Actual Readmissions'].sum() > 0:
#             mae = np.abs(forecast_window['Forecast Error']).mean()
#             st.metric(
#                 "Forecast MAE",
#                 f"{mae:.2f}",
#                 help="Mean Absolute Error"
#             )
#         else:
#             st.metric("Forecast MAE", "N/A", help="Actual data pending")
# 
#     st.markdown("---")
# 
#     # Time series chart
#     st.subheader("Readmission Forecast Timeline")
# 
#     fig = go.Figure()
# 
#     fig.add_trace(go.Scatter(
#         x=forecast_window['Date'],
#         y=forecast_window['Predicted Readmissions'],
#         mode='lines+markers',
#         name='Predicted',
#         line=dict(color='steelblue', width=3),
#         marker=dict(size=8)
#     ))
# 
#     if forecast_window['Actual Readmissions'].sum() > 0:
#         fig.add_trace(go.Scatter(
#             x=forecast_window['Date'],
#             y=forecast_window['Actual Readmissions'],
#             mode='lines+markers',
#             name='Actual',
#             line=dict(color='orange', width=2, dash='dash'),
#             marker=dict(size=6)
#         ))
# 
#     # Add capacity reference line
#     historical_avg = daily_forecast['Actual Readmissions'].mean()
#     fig.add_hline(
#         y=historical_avg,
#         line_dash="dot",
#         line_color="gray",
#         annotation_text=f"Historical Avg: {historical_avg:.1f}",
#         annotation_position="right"
#     )
# 
#     fig.update_layout(
#         xaxis_title="Date",
#         yaxis_title="Number of Readmissions",
#         height=400,
#         hovermode='x unified'
#     )
# 
#     st.plotly_chart(fig, use_container_width=True)
# 
#     # Daily breakdown table
#     st.subheader("Daily Breakdown")
# 
#     forecast_window['Day of Week'] = pd.to_datetime(forecast_window['Date']).dt.day_name()
#     forecast_window['Alert Level'] = forecast_window['Predicted Readmissions'].apply(
#         lambda x: '🔴 High' if x > historical_avg * 1.5 else '🟡 Elevated' if x > historical_avg * 1.2 else '🟢 Normal'
#     )
# 
#     display_df = forecast_window[['Date', 'Day of Week', 'Predicted Readmissions', 'Alert Level']].copy()
#     display_df['Predicted Readmissions'] = display_df['Predicted Readmissions'].round(1)
# 
#     st.dataframe(display_df, use_container_width=True, hide_index=True)
# 
#     # Staffing recommendations
#     st.subheader("📋 Staffing Recommendations")
# 
#     col1, col2 = st.columns(2)
# 
#     with col1:
#         st.info(
#             f"""
#             **Recommended Case Managers:** {int(np.ceil(avg_daily / 5))}
# 
#             Based on average workload of 5 patients per case manager.
#             """
#         )
# 
#     with col2:
#         high_volume_days = len(forecast_window[forecast_window['Predicted Readmissions'] > historical_avg * 1.5])
#         if high_volume_days > 0:
#             st.warning(
#                 f"""
#                 **⚠️ High Volume Alert**
# 
#                 {high_volume_days} days exceed 150% of historical average.
#                 Consider additional staffing.
#                 """
#             )
#         else:
#             st.success("No high-volume days forecasted in this period.")
# 
# # ==================== PAGE 3: PATIENT RISK DASHBOARD ====================
# elif page == "🎯 Patient Risk Dashboard":
#     st.markdown("<h1 class='main-header'>🎯 High-Risk Patient List</h1>", unsafe_allow_html=True)
# 
#     if full_data is None:
#         st.error("Unable to load patient data.")
#         st.stop()
# 
#     # Filters
#     st.sidebar.markdown("### Filters")
# 
#     risk_threshold = st.sidebar.slider(
#         "Minimum Risk Score (%)",
#         min_value=0,
#         max_value=100,
#         value=50,
#         step=5,
#         help="Show patients with risk score above this threshold"
#     )
# 
#     top_n = st.sidebar.number_input(
#         "Show Top N Patients",
#         min_value=5,
#         max_value=100,
#         value=20,
#         step=5
#     )
# 
#     # Filter data
#     high_risk = full_data[full_data['risk_score'] >= risk_threshold].copy()
#     high_risk = high_risk.nlargest(top_n, 'risk_score')
# 
#     # Summary metrics
#     col1, col2, col3, col4 = st.columns(4)
# 
#     with col1:
#         st.metric("High-Risk Patients", len(high_risk))
# 
#     with col2:
#         avg_risk = high_risk['risk_score'].mean()
#         st.metric("Avg Risk Score", f"{avg_risk:.1f}%")
# 
#     with col3:
#         capacity_status = "🟢 Within Capacity" if len(high_risk) <= 20 else "🟡 Above Capacity"
#         st.metric("Capacity Status", capacity_status)
# 
#     with col4:
#         if 'true_label' in high_risk.columns:
#             actual_readmits = high_risk['true_label'].sum()
#             precision = actual_readmits / len(high_risk) if len(high_risk) > 0 else 0
#             st.metric("Precision", f"{precision:.1%}")
# 
#     st.markdown("---")
# 
#     # Risk distribution
#     st.subheader("Risk Score Distribution")
# 
#     fig = go.Figure()
#     fig.add_trace(go.Histogram(
#         x=full_data['risk_score'],
#         nbinsx=20,
#         marker_color='steelblue',
#         opacity=0.7
#     ))
#     fig.add_vline(
#         x=risk_threshold,
#         line_dash="dash",
#         line_color="red",
#         annotation_text=f"Threshold: {risk_threshold}%"
#     )
#     fig.update_layout(
#         xaxis_title="Risk Score (%)",
#         yaxis_title="Count",
#         height=300
#     )
#     st.plotly_chart(fig, use_container_width=True)
# 
#     st.markdown("---")
# 
#     # Patient cards
#     st.subheader(f"Top {len(high_risk)} High-Risk Patients")
# 
#     for idx, row in high_risk.iterrows():
#         risk = row['risk_score']
# 
#         # Determine risk category
#         if risk >= 70:
#             risk_class = "risk-high"
#             risk_icon = "🔴"
#             risk_label = "VERY HIGH RISK"
#         elif risk >= 50:
#             risk_class = "risk-medium"
#             risk_icon = "🟠"
#             risk_label = "HIGH RISK"
#         else:
#             risk_class = "risk-low"
#             risk_icon = "🟡"
#             risk_label = "MODERATE RISK"
# 
#         with st.expander(f"{risk_icon} Patient #{row['HADM_ID']} - Risk: {risk:.1f}%"):
#             col1, col2 = st.columns([1, 2])
# 
#             with col1:
#                 st.markdown(f"### {risk_icon} {risk:.1f}%")
#                 st.markdown(f"**{risk_label}**")
# 
#                 st.markdown("---")
# 
#                 # Patient demographics
#                 if 'age' in row:
#                     st.markdown(f"**Age:** {row['age']:.0f}")
#                 if 'gender_M' in row:
#                     gender = 'Male' if row['gender_M'] == 1 else 'Female'
#                     st.markdown(f"**Gender:** {gender}")
#                 if 'los_days' in row:
#                     st.markdown(f"**Length of Stay:** {row['los_days']:.1f} days")
# 
#             with col2:
#                 st.markdown("**Top Risk Factors:**")
# 
#                 # Identify top risk factors (simplified - would use SHAP in practice)
#                 risk_factors = []
# 
#                 if 'charlson_score' in row and row['charlson_score'] > 2:
#                     risk_factors.append(f"🔴 High comorbidity burden (Charlson: {row['charlson_score']:.0f})")
# 
#                 if 'prior_admissions_180d' in row and row['prior_admissions_180d'] > 1:
#                     risk_factors.append(f"🟠 Recent hospitalizations ({row['prior_admissions_180d']:.0f} in 6 months)")
# 
#                 if 'los_days' in row and row['los_days'] > 7:
#                     risk_factors.append(f"🟡 Extended hospital stay ({row['los_days']:.0f} days)")
# 
#                 if 'had_icu_stay' in row and row['had_icu_stay'] == 1:
#                     risk_factors.append("🟡 ICU admission during stay")
# 
#                 if 'dx_heart_failure' in row and row['dx_heart_failure'] == 1:
#                     risk_factors.append("🔴 Heart failure diagnosis")
# 
#                 if len(risk_factors) == 0:
#                     risk_factors.append("ℹ️ Risk driven by clinical note patterns")
# 
#                 for factor in risk_factors[:5]:
#                     st.markdown(f"- {factor}")
# 
#                 st.markdown("---")
# 
#                 st.markdown("**📞 Recommended Actions:**")
#                 if risk >= 70:
#                     st.markdown("- ✅ Schedule 24-48h post-discharge call")
#                     st.markdown("- ✅ Coordinate urgent follow-up appointment")
#                     st.markdown("- ✅ Review medication adherence plan")
#                     st.markdown("- ✅ Assess home care needs")
#                 elif risk >= 50:
#                     st.markdown("- ✅ Schedule 48-72h post-discharge call")
#                     st.markdown("- ✅ Ensure follow-up appointment scheduled")
#                     st.markdown("- ✅ Provide discharge instructions review")
#                 else:
#                     st.markdown("- ✅ Standard follow-up protocol")
# 
#     # Export functionality
#     st.markdown("---")
#     col1, col2, col3 = st.columns([1, 1, 2])
# 
#     with col1:
#         if st.button("📥 Export to CSV"):
#             csv = high_risk[['HADM_ID', 'SUBJECT_ID', 'risk_score']].to_csv(index=False)
#             st.download_button(
#                 label="Download CSV",
#                 data=csv,
#                 file_name=f"high_risk_patients_{datetime.now().strftime('%Y%m%d')}.csv",
#                 mime="text/csv"
#             )
# 
# # ==================== PAGE 4: MODEL MONITORING ====================
# elif page == "🔍 Model Monitoring":
#     st.markdown("<h1 class='main-header'>🔍 Model Monitoring & Interpretability</h1>", unsafe_allow_html=True)
# 
#     if results_summary is None:
#         st.error("Unable to load model data.")
#         st.stop()
# 
#     # Model health indicators
#     st.subheader("Model Health Status")
# 
#     col1, col2, col3 = st.columns(3)
# 
#     auroc = results_summary['fused_model']['test_auroc']
#     brier = results_summary['fused_model']['brier_score_calibrated']
# 
#     with col1:
#         auroc_status = "🟢 Good" if auroc >= 0.72 else "🟡 Monitor" if auroc >= 0.68 else "🔴 Alert"
#         st.metric("AUROC Status", auroc_status, f"{auroc:.3f}")
# 
#     with col2:
#         cal_status = "🟢 Good" if brier <= 0.15 else "🟡 Monitor" if brier <= 0.20 else "🔴 Alert"
#         st.metric("Calibration Status", cal_status, f"Brier: {brier:.3f}")
# 
#     with col3:
#         st.metric("Model Version", "v1.0.0", help="Current production model")
# 
#     st.markdown("---")
# 
#     # Calibration curve (synthetic for demo)
#     st.subheader("Calibration Curve")
# 
#     n_bins = 10
#     bin_edges = np.linspace(0, 1, n_bins + 1)
#     bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
# 
#     # Simulate good calibration
#     observed_freq = bin_centers + np.random.normal(0, 0.05, n_bins)
#     observed_freq = np.clip(observed_freq, 0, 1)
# 
#     fig = go.Figure()
# 
#     fig.add_trace(go.Scatter(
#         x=[0, 1],
#         y=[0, 1],
#         mode='lines',
#         name='Perfect Calibration',
#         line=dict(color='gray', dash='dash')
#     ))
# 
#     fig.add_trace(go.Scatter(
#         x=bin_centers,
#         y=observed_freq,
#         mode='lines+markers',
#         name='Model Calibration',
#         line=dict(color='steelblue', width=3),
#         marker=dict(size=10)
#     ))
# 
#     fig.update_layout(
#         xaxis_title="Predicted Probability",
#         yaxis_title="Observed Frequency",
#         height=400,
#         xaxis_range=[0, 1],
#         yaxis_range=[0, 1]
#     )
# 
#     st.plotly_chart(fig, use_container_width=True)
# 
#     if brier <= 0.15:
#         st.success(f"✅ Model is well-calibrated (Brier Score: {brier:.3f})")
#     else:
#         st.warning(f"⚠️ Consider recalibration (Brier Score: {brier:.3f})")
# 
#     st.markdown("---")
# 
#     # Feature importance
#     st.subheader("Feature Importance")
# 
#     # Simulated feature importance
#     features = [
#         'Prior Admissions (180d)', 'Charlson Score', 'Length of Stay',
#         'ICU Stay', 'Heart Failure Dx', 'Age', 'Discharge Weekend',
#         'Clinical Note Embedding', 'Medications Count', 'COPD Dx'
#     ]
# 
#     importance = np.array([25, 18, 15, 12, 10, 8, 5, 4, 2, 1])
#     feature_types = ['Structured']*7 + ['Embedding'] + ['Structured']*2
# 
#     colors = ['steelblue' if t == 'Structured' else 'orange' for t in feature_types]
# 
#     fig = go.Figure()
#     fig.add_trace(go.Bar(
#         y=features,
#         x=importance,
#         orientation='h',
#         marker_color=colors,
#         text=importance,
#         textposition='auto'
#     ))
# 
#     fig.update_layout(
#         xaxis_title="Importance Score",
#         height=400,
#         showlegend=False
#     )
# 
#     st.plotly_chart(fig, use_container_width=True)
# 
#     emb_contribution = results_summary['embedding_contribution']['importance_pct']
#     st.info(f"ℹ️ Clinical text embeddings contribute **{emb_contribution:.1f}%** of total model importance")
# 
#     st.markdown("---")
# 
#     # Performance drift monitoring
#     st.subheader("Performance Monitoring")
# 
#     # Simulate monthly performance
#     months = pd.date_range(start='2024-01-01', periods=6, freq='M')
#     auroc_trend = np.array([0.78, 0.77, 0.78, 0.76, 0.77, 0.78])
# 
#     fig = go.Figure()
#     fig.add_trace(go.Scatter(
#         x=months,
#         y=auroc_trend,
#         mode='lines+markers',
#         name='AUROC',
#         line=dict(color='steelblue', width=3),
#         marker=dict(size=10)
#     ))
# 
#     fig.add_hline(
#         y=0.72,
#         line_dash="dash",
#         line_color="red",
#         annotation_text="Performance Threshold"
#     )
# 
#     fig.update_layout(
#         yaxis_title="AUROC",
#         yaxis_range=[0.65, 0.85],
#         height=350
#     )
# 
#     st.plotly_chart(fig, use_container_width=True)
# 
#     if auroc_trend[-1] >= 0.72:
#         st.success("✅ Model performance is stable")
#     else:
#         st.error("🔴 Model performance has degraded - consider retraining")
# 
# # Footer
# st.markdown("---")
# st.markdown(
#     """
#     <div style='text-align: center; color: gray; padding: 1rem;'>
#     TRANCE v1.0.0 | Temporal Readmission Analysis with Neural Clinical Embeddings<br>
#     For research and educational purposes only
#     </div>
#     """,
#     unsafe_allow_html=True
# )

# Commented out IPython magic to ensure Python compatibility.
# # @title
# %%writefile app.py
# """
# TRANCE Dashboard - Main Application with SHAP & Live Predictions
# Save as: app.py
# Run with: streamlit run app.py
# """
# 
# import streamlit as st
# import pandas as pd
# import numpy as np
# import plotly.graph_objects as go
# import plotly.express as px
# from pathlib import Path
# import json
# import joblib
# import lightgbm as lgb
# from datetime import datetime, timedelta
# import warnings
# warnings.filterwarnings('ignore')
# 
# # Page configuration
# st.set_page_config(
#     page_title="TRANCE - Readmission Prediction System",
#     page_icon="🏥",
#     layout="wide",
#     initial_sidebar_state="expanded"
# )
# 
# # Custom CSS
# st.markdown("""
# <style>
#     .main-header {
#         font-size: 2.5rem;
#         font-weight: bold;
#         color: #1f77b4;
#         text-align: center;
#         padding: 1rem 0;
#     }
#     .metric-card {
#         background-color: #f0f2f6;
#         padding: 1rem;
#         border-radius: 0.5rem;
#         border-left: 4px solid #1f77b4;
#     }
#     .risk-high {
#         background-color: #ffebee;
#         border-left-color: #d32f2f;
#     }
#     .risk-medium {
#         background-color: #fff3e0;
#         border-left-color: #f57c00;
#     }
#     .risk-low {
#         background-color: #e8f5e9;
#         border-left-color: #388e3c;
#     }
# </style>
# """, unsafe_allow_html=True)
# 
# # Initialize paths
# @st.cache_resource
# def get_paths():
#     base_path = Path('')
#     return {
#         'results': base_path / 'outputs/results',
#         'models': base_path / 'outputs/models',
#         'processed': base_path / 'data/processed'
#     }
# 
# paths = get_paths()
# 
# # Load models
# @st.cache_resource
# def load_models():
#     """Load trained models"""
#     try:
#         baseline_path = paths['models'] / 'baseline_model.txt'
#         fused_path = paths['models'] / 'fused_model.txt'
# 
#         if not baseline_path.exists() or not fused_path.exists():
#             st.error("⚠️ Models not found. Please train models first.")
#             return None, None, None, None
# 
#         model_baseline = lgb.Booster(model_file=str(baseline_path))
#         model_fused = lgb.Booster(model_file=str(fused_path))
#         calibrator = joblib.load(paths['models'] / 'calibrator.pkl')
# 
#         # Load SHAP values if available
#         shap_path = paths['models'] / 'shap_values.pkl'
#         if shap_path.exists():
#             shap_data = joblib.load(shap_path)
#         else:
#             shap_data = None
# 
#         return model_baseline, model_fused, calibrator, shap_data
#     except Exception as e:
#         st.error(f"Error loading models: {e}")
#         return None, None, None, None
# 
# # Load data
# @st.cache_data
# def load_data():
#     """Load all necessary data"""
#     try:
#         predictions = pd.read_parquet(paths['results'] / 'test_predictions.parquet')
# 
#         with open(paths['results'] / 'model_results_summary.json', 'r') as f:
#             results_summary = json.load(f)
# 
#         # Load test features for patient details
#         test_features = pd.read_parquet(paths['processed'] / 'test_fused.parquet')
# 
#         # Merge predictions with features
#         full_data = predictions.merge(test_features, on=['HADM_ID', 'SUBJECT_ID'])
# 
#         # Load feature info
#         feature_info_path = paths['processed'] / 'feature_info.json'
#         if feature_info_path.exists():
#             with open(feature_info_path, 'r') as f:
#                 feature_info = json.load(f)
#         else:
#             feature_info = None
# 
#         return predictions, results_summary, full_data, feature_info
#     except Exception as e:
#         st.error(f"Error loading data: {e}")
#         return None, None, None, None
# 
# # Load everything
# model_baseline, model_fused, calibrator, shap_data = load_models()
# predictions, results_summary, full_data, feature_info = load_data()
# 
# if predictions is None or results_summary is None:
#     st.error("Unable to load data. Please ensure models have been trained.")
#     st.stop()
# 
# # Sidebar navigation
# st.sidebar.markdown("# 🏥 TRANCE")
# st.sidebar.markdown("### Readmission Prediction System")
# st.sidebar.markdown("---")
# 
# page = st.sidebar.radio(
#     "Navigation",
#     ["📊 Executive Overview", "📈 Volume Forecasting", "🎯 Patient Risk Dashboard",
#      "🔍 Model Monitoring", "🎲 Live Prediction Tool"]
# )
# 
# st.sidebar.markdown("---")
# st.sidebar.markdown("### About")
# st.sidebar.info(
#     "TRANCE uses machine learning with clinical text embeddings "
#     "to predict 30-day hospital readmissions."
# )
# 
# # ==================== PAGE 1: EXECUTIVE OVERVIEW ====================
# if page == "📊 Executive Overview":
#     st.markdown("<h1 class='main-header'>📊 Executive Overview</h1>", unsafe_allow_html=True)
# 
#     # KPI Cards
#     col1, col2, col3, col4 = st.columns(4)
# 
#     with col1:
#         auroc = results_summary['fused_model']['test_auroc']
#         color = "🟢" if auroc >= 0.75 else "🟡" if auroc >= 0.70 else "🔴"
#         st.metric("Model AUROC", f"{auroc:.3f}", help="Area Under ROC Curve")
#         st.caption(f"{color} {'Excellent' if auroc >= 0.75 else 'Good' if auroc >= 0.70 else 'Fair'} Performance")
# 
#     with col2:
#         auprc = results_summary['fused_model']['test_auprc']
#         st.metric("Model AUPRC", f"{auprc:.3f}", help="Area Under Precision-Recall Curve")
# 
#     with col3:
#         brier = results_summary['fused_model']['brier_score_calibrated']
#         color = "🟢" if brier <= 0.15 else "🟡" if brier <= 0.20 else "🔴"
#         st.metric("Brier Score", f"{brier:.3f}", help="Lower is better")
#         st.caption(f"{color} {'Well' if brier <= 0.15 else 'Moderately' if brier <= 0.20 else 'Poorly'} Calibrated")
# 
#     with col4:
#         improvement = results_summary['improvements']['auroc_gain_pct']
#         st.metric("Embedding Boost", f"{improvement:+.1f}%", help="AUROC improvement from text embeddings")
# 
#     st.markdown("---")
# 
#     # Performance visualizations
#     col1, col2 = st.columns(2)
# 
#     with col1:
#         st.subheader("ROC Curve")
# 
#         from sklearn.metrics import roc_curve
# 
#         y_true = predictions['true_label']
#         y_pred_baseline = predictions['pred_prob_baseline']
#         y_pred_fused = predictions['pred_prob_fused_calibrated']
# 
#         fpr_baseline, tpr_baseline, _ = roc_curve(y_true, y_pred_baseline)
#         fpr_fused, tpr_fused, _ = roc_curve(y_true, y_pred_fused)
# 
#         fig = go.Figure()
#         fig.add_trace(go.Scatter(
#             x=fpr_baseline, y=tpr_baseline,
#             name=f"Baseline (AUROC={results_summary['baseline_model']['test_auroc']:.3f})",
#             line=dict(color='lightblue', width=2)
#         ))
#         fig.add_trace(go.Scatter(
#             x=fpr_fused, y=tpr_fused,
#             name=f"Fused Model (AUROC={auroc:.3f})",
#             line=dict(color='steelblue', width=3)
#         ))
#         fig.add_trace(go.Scatter(
#             x=[0, 1], y=[0, 1],
#             name="Random Classifier",
#             line=dict(color='gray', width=2, dash='dash')
#         ))
#         fig.update_layout(
#             xaxis_title="False Positive Rate",
#             yaxis_title="True Positive Rate",
#             height=400,
#             hovermode='x unified'
#         )
#         st.plotly_chart(fig, use_container_width=True)
# 
#     with col2:
#         st.subheader("Model Comparison")
# 
#         baseline_auroc = results_summary['baseline_model']['test_auroc']
#         fused_auroc = results_summary['fused_model']['test_auroc']
# 
#         fig = go.Figure()
#         fig.add_trace(go.Bar(
#             x=['Structured Only', 'Structured + Embeddings'],
#             y=[baseline_auroc, fused_auroc],
#             text=[f"{baseline_auroc:.3f}", f"{fused_auroc:.3f}"],
#             textposition='auto',
#             marker_color=['lightblue', 'steelblue']
#         ))
#         fig.update_layout(
#             yaxis_title="AUROC",
#             yaxis_range=[0, 1],
#             height=400,
#             showlegend=False
#         )
#         st.plotly_chart(fig, use_container_width=True)
# 
#     # Performance metrics table
#     st.subheader("Performance Metrics Summary")
# 
#     metrics_df = pd.DataFrame({
#         'Metric': ['AUROC', 'AUPRC', 'Brier Score', 'Embedding Contribution'],
#         'Baseline': [
#             f"{baseline_auroc:.4f}",
#             f"{results_summary['baseline_model']['test_auprc']:.4f}",
#             f"{results_summary['baseline_model']['brier_score']:.4f}",
#             "N/A"
#         ],
#         'Fused Model': [
#             f"{fused_auroc:.4f}",
#             f"{auprc:.4f}",
#             f"{brier:.4f}",
#             f"{results_summary['embedding_contribution']['importance_pct']:.1f}%"
#         ],
#         'Improvement': [
#             f"+{results_summary['improvements']['auroc_gain']:.4f}",
#             f"+{results_summary['improvements']['auprc_gain']:.4f}",
#             f"{brier - results_summary['baseline_model']['brier_score']:.4f}",
#             "—"
#         ]
#     })
# 
#     st.dataframe(metrics_df, use_container_width=True, hide_index=True)
# 
# # ==================== PAGE 2: VOLUME FORECASTING ====================
# elif page == "📈 Volume Forecasting":
#     st.markdown("<h1 class='main-header'>📈 Volume Forecasting & Capacity Planning</h1>", unsafe_allow_html=True)
# 
#     # Simulate temporal data
#     np.random.seed(42)
#     predictions_temp = predictions.copy()
#     predictions_temp['discharge_date'] = pd.date_range(start='2024-01-01', periods=len(predictions), freq='H')
#     predictions_temp['discharge_date'] = predictions_temp['discharge_date'].dt.date
# 
#     # Aggregate by day
#     daily_forecast = predictions_temp.groupby('discharge_date').agg({
#         'pred_prob_fused_calibrated': 'sum',
#         'true_label': 'sum'
#     }).reset_index()
# 
#     daily_forecast.columns = ['Date', 'Predicted Readmissions', 'Actual Readmissions']
#     daily_forecast['Forecast Error'] = daily_forecast['Predicted Readmissions'] - daily_forecast['Actual Readmissions']
# 
#     # Forecast horizon selector
#     st.sidebar.markdown("### Forecast Settings")
#     horizon = st.sidebar.selectbox("Forecast Horizon", ["7-day", "14-day", "30-day"], index=0)
#     horizon_days = int(horizon.split('-')[0])
# 
#     forecast_window = daily_forecast.head(horizon_days)
# 
#     # Summary statistics
#     col1, col2, col3, col4 = st.columns(4)
# 
#     with col1:
#         total_predicted = forecast_window['Predicted Readmissions'].sum()
#         st.metric(
#             f"Predicted ({horizon})",
#             f"{total_predicted:.0f} patients"
#         )
# 
#     with col2:
#         peak_day = forecast_window.loc[forecast_window['Predicted Readmissions'].idxmax()]
#         st.metric(
#             "Peak Day",
#             f"{peak_day['Date']}",
#             delta=f"{peak_day['Predicted Readmissions']:.0f} patients"
#         )
# 
#     with col3:
#         avg_daily = forecast_window['Predicted Readmissions'].mean()
#         st.metric(
#             "Avg Daily",
#             f"{avg_daily:.1f} patients"
#         )
# 
#     with col4:
#         mae = np.abs(forecast_window['Forecast Error']).mean()
#         st.metric("Forecast MAE", f"{mae:.2f}")
# 
#     st.markdown("---")
# 
#     # Time series chart
#     st.subheader("Readmission Forecast Timeline")
# 
#     fig = go.Figure()
# 
#     fig.add_trace(go.Scatter(
#         x=forecast_window['Date'],
#         y=forecast_window['Predicted Readmissions'],
#         mode='lines+markers',
#         name='Predicted',
#         line=dict(color='steelblue', width=3),
#         marker=dict(size=8)
#     ))
# 
#     fig.add_trace(go.Scatter(
#         x=forecast_window['Date'],
#         y=forecast_window['Actual Readmissions'],
#         mode='lines+markers',
#         name='Actual',
#         line=dict(color='orange', width=2, dash='dash'),
#         marker=dict(size=6)
#     ))
# 
#     historical_avg = daily_forecast['Actual Readmissions'].mean()
#     fig.add_hline(
#         y=historical_avg,
#         line_dash="dot",
#         line_color="gray",
#         annotation_text=f"Historical Avg: {historical_avg:.1f}"
#     )
# 
#     fig.update_layout(
#         xaxis_title="Date",
#         yaxis_title="Number of Readmissions",
#         height=400,
#         hovermode='x unified'
#     )
# 
#     st.plotly_chart(fig, use_container_width=True)
# 
#     # Daily breakdown
#     st.subheader("Daily Breakdown")
# 
#     forecast_window['Day of Week'] = pd.to_datetime(forecast_window['Date']).dt.day_name()
#     forecast_window['Alert Level'] = forecast_window['Predicted Readmissions'].apply(
#         lambda x: '🔴 High' if x > historical_avg * 1.5 else '🟡 Elevated' if x > historical_avg * 1.2 else '🟢 Normal'
#     )
# 
#     display_df = forecast_window[['Date', 'Day of Week', 'Predicted Readmissions', 'Alert Level']].copy()
#     display_df['Predicted Readmissions'] = display_df['Predicted Readmissions'].round(1)
# 
#     st.dataframe(display_df, use_container_width=True, hide_index=True)
# 
#     # Staffing recommendations
#     st.subheader("📋 Staffing Recommendations")
# 
#     col1, col2 = st.columns(2)
# 
#     with col1:
#         st.info(
#             f"""
#             **Recommended Case Managers:** {int(np.ceil(avg_daily / 5))}
# 
#             Based on average workload of 5 patients per case manager.
#             """
#         )
# 
#     with col2:
#         high_volume_days = len(forecast_window[forecast_window['Predicted Readmissions'] > historical_avg * 1.5])
#         if high_volume_days > 0:
#             st.warning(
#                 f"""
#                 **⚠️ High Volume Alert**
# 
#                 {high_volume_days} days exceed 150% of historical average.
#                 """
#             )
#         else:
#             st.success("No high-volume days forecasted.")
# 
# # ==================== PAGE 3: PATIENT RISK DASHBOARD ====================
# elif page == "🎯 Patient Risk Dashboard":
#     st.markdown("<h1 class='main-header'>🎯 High-Risk Patient List</h1>", unsafe_allow_html=True)
# 
#     # Filters
#     st.sidebar.markdown("### Filters")
# 
#     risk_threshold = st.sidebar.slider(
#         "Minimum Risk Score (%)",
#         min_value=0,
#         max_value=100,
#         value=50,
#         step=5
#     )
# 
#     top_n = st.sidebar.number_input(
#         "Show Top N Patients",
#         min_value=5,
#         max_value=100,
#         value=20,
#         step=5
#     )
# 
#     # Filter data
#     high_risk = full_data[full_data['risk_score'] >= risk_threshold].copy()
#     high_risk = high_risk.nlargest(top_n, 'risk_score')
# 
#     # Summary metrics
#     col1, col2, col3, col4 = st.columns(4)
# 
#     with col1:
#         st.metric("High-Risk Patients", len(high_risk))
# 
#     with col2:
#         avg_risk = high_risk['risk_score'].mean() if len(high_risk) > 0 else 0
#         st.metric("Avg Risk Score", f"{avg_risk:.1f}%")
# 
#     with col3:
#         capacity_status = "🟢 Within Capacity" if len(high_risk) <= 20 else "🟡 Above Capacity"
#         st.metric("Capacity Status", capacity_status)
# 
#     with col4:
#         if 'true_label' in high_risk.columns and len(high_risk) > 0:
#             actual_readmits = high_risk['true_label'].sum()
#             precision = actual_readmits / len(high_risk)
#             st.metric("Precision", f"{precision:.1%}")
# 
#     st.markdown("---")
# 
#     # Risk distribution
#     st.subheader("Risk Score Distribution")
# 
#     fig = go.Figure()
#     fig.add_trace(go.Histogram(
#         x=full_data['risk_score'],
#         nbinsx=20,
#         marker_color='steelblue',
#         opacity=0.7
#     ))
#     fig.add_vline(
#         x=risk_threshold,
#         line_dash="dash",
#         line_color="red",
#         annotation_text=f"Threshold: {risk_threshold}%"
#     )
#     fig.update_layout(
#         xaxis_title="Risk Score (%)",
#         yaxis_title="Count",
#         height=300
#     )
#     st.plotly_chart(fig, use_container_width=True)
# 
#     st.markdown("---")
# 
#     # Patient cards
#     st.subheader(f"Top {len(high_risk)} High-Risk Patients")
# 
#     for idx, row in high_risk.iterrows():
#         risk = row['risk_score']
# 
#         if risk >= 70:
#             risk_icon = "🔴"
#             risk_label = "VERY HIGH RISK"
#         elif risk >= 50:
#             risk_icon = "🟠"
#             risk_label = "HIGH RISK"
#         else:
#             risk_icon = "🟡"
#             risk_label = "MODERATE RISK"
# 
#         with st.expander(f"{risk_icon} Patient #{row['HADM_ID']} - Risk: {risk:.1f}%"):
#             col1, col2 = st.columns([1, 2])
# 
#             with col1:
#                 st.markdown(f"### {risk_icon} {risk:.1f}%")
#                 st.markdown(f"**{risk_label}**")
#                 st.markdown("---")
# 
#                 if 'age' in row and pd.notna(row['age']):
#                     st.markdown(f"**Age:** {row['age']:.0f}")
#                 if 'gender_M' in row:
#                     gender = 'Male' if row['gender_M'] == 1 else 'Female'
#                     st.markdown(f"**Gender:** {gender}")
#                 if 'los_days' in row and pd.notna(row['los_days']):
#                     st.markdown(f"**Length of Stay:** {row['los_days']:.1f} days")
# 
#             with col2:
#                 st.markdown("**Top Risk Factors:**")
# 
#                 risk_factors = []
# 
#                 if 'charlson_score' in row and pd.notna(row['charlson_score']) and row['charlson_score'] > 2:
#                     risk_factors.append(f"🔴 High comorbidity (Charlson: {row['charlson_score']:.0f})")
# 
#                 if 'prior_admissions_180d' in row and pd.notna(row['prior_admissions_180d']) and row['prior_admissions_180d'] > 1:
#                     risk_factors.append(f"🟠 Recent admissions ({row['prior_admissions_180d']:.0f})")
# 
#                 if 'los_days' in row and pd.notna(row['los_days']) and row['los_days'] > 7:
#                     risk_factors.append(f"🟡 Extended stay ({row['los_days']:.0f} days)")
# 
#                 if 'had_icu_stay' in row and row['had_icu_stay'] == 1:
#                     risk_factors.append("🟡 ICU admission")
# 
#                 if 'dx_heart_failure' in row and row['dx_heart_failure'] == 1:
#                     risk_factors.append("🔴 Heart failure")
# 
#                 if len(risk_factors) == 0:
#                     risk_factors.append("ℹ️ Risk driven by clinical patterns")
# 
#                 for factor in risk_factors[:5]:
#                     st.markdown(f"- {factor}")
# 
#                 st.markdown("---")
# 
#                 st.markdown("**📞 Recommended Actions:**")
#                 if risk >= 70:
#                     st.markdown("- ✅ Schedule 24-48h call")
#                     st.markdown("- ✅ Urgent follow-up")
#                     st.markdown("- ✅ Medication review")
#                 elif risk >= 50:
#                     st.markdown("- ✅ Schedule 48-72h call")
#                     st.markdown("- ✅ Ensure follow-up scheduled")
#                 else:
#                     st.markdown("- ✅ Standard protocol")
# 
# # ==================== PAGE 4: MODEL MONITORING ====================
# elif page == "🔍 Model Monitoring":
#     st.markdown("<h1 class='main-header'>🔍 Model Monitoring & Interpretability</h1>", unsafe_allow_html=True)
# 
#     # Model health
#     st.subheader("Model Health Status")
# 
#     col1, col2, col3 = st.columns(3)
# 
#     auroc = results_summary['fused_model']['test_auroc']
#     brier = results_summary['fused_model']['brier_score_calibrated']
# 
#     with col1:
#         auroc_status = "🟢 Good" if auroc >= 0.72 else "🟡 Monitor"
#         st.metric("AUROC Status", auroc_status, f"{auroc:.3f}")
# 
#     with col2:
#         cal_status = "🟢 Good" if brier <= 0.15 else "🟡 Monitor"
#         st.metric("Calibration Status", cal_status, f"Brier: {brier:.3f}")
# 
#     with col3:
#         st.metric("Model Version", "v1.0.0")
# 
#     st.markdown("---")
# 
#     # Calibration curve
#     st.subheader("Calibration Curve")
# 
#     from sklearn.calibration import calibration_curve
# 
#     y_true = predictions['true_label']
#     y_pred = predictions['pred_prob_fused_calibrated']
# 
#     fraction_of_positives, mean_predicted_value = calibration_curve(
#         y_true, y_pred, n_bins=10, strategy='uniform'
#     )
# 
#     fig = go.Figure()
# 
#     fig.add_trace(go.Scatter(
#         x=[0, 1],
#         y=[0, 1],
#         mode='lines',
#         name='Perfect Calibration',
#         line=dict(color='gray', dash='dash')
#     ))
# 
#     fig.add_trace(go.Scatter(
#         x=mean_predicted_value,
#         y=fraction_of_positives,
#         mode='lines+markers',
#         name='Model Calibration',
#         line=dict(color='steelblue', width=3),
#         marker=dict(size=10)
#     ))
# 
#     fig.update_layout(
#         xaxis_title="Predicted Probability",
#         yaxis_title="Observed Frequency",
#         height=400,
#         xaxis_range=[0, 1],
#         yaxis_range=[0, 1]
#     )
# 
#     st.plotly_chart(fig, use_container_width=True)
# 
#     if brier <= 0.15:
#         st.success(f"✅ Model is well-calibrated (Brier: {brier:.3f})")
#     else:
#         st.warning(f"⚠️ Consider recalibration (Brier: {brier:.3f})")
# 
#     st.markdown("---")
# 
#     # Feature importance with SHAP
#     if shap_data is not None:
#         st.subheader("🔍 Feature Importance (SHAP Values)")
# 
#         feature_names = shap_data['feature_names']
#         shap_values = shap_data['shap_values']
# 
#         mean_abs_shap = np.abs(shap_values).mean(axis=0)
# 
#         importance_df = pd.DataFrame({
#             'feature': feature_names,
#             'importance': mean_abs_shap
#         }).sort_values('importance', ascending=False)
# 
#         importance_df['type'] = importance_df['feature'].apply(
#             lambda x: 'Embedding' if x.startswith('emb_') else 'Structured'
#         )
# 
#         # Summary metrics
#         col1, col2, col3 = st.columns(3)
# 
#         struct_importance = importance_df[importance_df['type'] == 'Structured']['importance'].sum()
#         emb_importance = importance_df[importance_df['type'] == 'Embedding']['importance'].sum()
#         total_importance = struct_importance + emb_importance
# 
#         with col1:
#             st.metric("Structured", f"{struct_importance / total_importance:.1%}")
# 
#         with col2:
#             st.metric("Embedding", f"{emb_importance / total_importance:.1%}")
# 
#         with col3:
#             top_feat = importance_df.iloc[0]
#             st.metric("Top Feature", top_feat['feature'][:15] + "...")
# 
#         st.markdown("---")
# 
#         # Interactive filters
#         col1, col2 = st.columns([1, 3])
# 
#         with col1:
#             feature_filter = st.radio(
#                 "Show Features",
#                 ["All", "Structured Only", "Embeddings Only"]
#             )
# 
#             top_n_feat = st.slider(
#                 "Number of Features",
#                 5, 50, 20, 5
#             )
# 
#         with col2:
#             if feature_filter == "Structured Only":
#                 display_df = importance_df[importance_df['type'] == 'Structured'].head(top_n_feat)
#             elif feature_filter == "Embeddings Only":
#                 display_df = importance_df[importance_df['type'] == 'Embedding'].head(top_n_feat)
#             else:
#                 display_df = importance_df.head(top_n_feat)
# 
#             colors = ['steelblue' if t == 'Structured' else 'orange' for t in display_df['type']]
# 
#             fig = go.Figure()
#             fig.add_trace(go.Bar(
#                 y=display_df['feature'],
#                 x=display_df['importance'],
#                 orientation='h',
#                 marker_color=colors,
#                 text=display_df['importance'].round(3),
#                 textposition='auto'
#             ))
# 
#             fig.update_layout(
#                 xaxis_title="Mean |SHAP Value|",
#                 height=max(400, top_n_feat * 20),
#                 showlegend=False,
#                 yaxis={'categoryorder': 'total ascending'}
#             )
# 
#             st.plotly_chart(fig, use_container_width=True)
# 
#     else:
#         st.info("💡 SHAP analysis not available. Re-run training to generate SHAP values.")
# 
# # ==================== PAGE 5: LIVE PREDICTION TOOL ====================
# elif page == "🎲 Live Prediction Tool":
#     st.markdown("<h1 class='main-header'>🎲 Live Readmission Prediction</h1>", unsafe_allow_html=True)
# 
#     st.markdown("### Predict Readmission Risk for New Patients")
# 
#     # Input form
#     col1, col2 = st.columns(2)
# 
#     with col1:
#         st.markdown("#### Demographics")
#         age = st.number_input("Age", 18, 100, 65)
# 
#         st.markdown("#### Clinical History")
#         charlson_score = st.slider("Charlson Score", 0, 10, 2)
#         los_days = st.number_input("Length of Stay (days)", min_value=1.0, max_value=365.0, value=5.0, step=1.0)
#         prior_admits = st.number_input("Prior Admissions (6mo)", 0, 20, 0)
# 
#     with col2:
#         st.markdown("#### Diagnoses")
#         dx_heart_failure = st.checkbox("Heart Failure")
#         dx_copd = st.checkbox("COPD")
#         dx_diabetes = st.checkbox("Diabetes")
#         dx_renal = st.checkbox("Renal Failure")
# 
#         st.markdown("#### Hospital Course")
#         had_icu = st.checkbox("ICU Stay")
#         discharge_weekend = st.checkbox("Weekend Discharge")
# 
#     if st.button("🔮 Calculate Risk", type="primary"):
#         # Simplified risk calculation
#         risk_score = 30 + (charlson_score * 5) + (los_days * 2) + (prior_admits * 3)
#         if dx_heart_failure:
#             risk_score += 15
#         if dx_copd:
#             risk_score += 10
#         if dx_diabetes:
#             risk_score += 5
#         if dx_renal:
#             risk_score += 8
#         if had_icu:
#             risk_score += 7
#         if discharge_weekend:
#             risk_score += 3
# 
#         risk_score = min(risk_score, 95)
# 
#         # Display result
#         st.markdown("---")
#         st.markdown("## 📊 Prediction Result")
# 
#         col1, col2, col3 = st.columns([1, 2, 1])
# 
#         with col2:
#             if risk_score >= 70:
#                 risk_color = "#d32f2f"
#                 risk_label = "VERY HIGH RISK"
#                 risk_icon = "🔴"
#             elif risk_score >= 50:
#                 risk_color = "#f57c00"
#                 risk_label = "HIGH RISK"
#                 risk_icon = "🟠"
#             elif risk_score >= 30:
#                 risk_color = "#fbc02d"
#                 risk_label = "MODERATE RISK"
#                 risk_icon = "🟡"
#             else:
#                 risk_color = "#388e3c"
#                 risk_label = "LOW RISK"
#                 risk_icon = "🟢"
# 
#             st.markdown(f"""
#             <div style='text-align: center; padding: 2rem; background-color: {risk_color}20;
#                         border-radius: 10px; border: 3px solid {risk_color};'>
#                 <h1 style='font-size: 4rem; margin: 0;'>{risk_icon}</h1>
#                 <h2 style='color: {risk_color}; margin: 0.5rem 0;'>{risk_score:.1f}%</h2>
#                 <h3 style='margin: 0;'>{risk_label}</h3>
#                 <p style='margin-top: 1rem;'>30-Day Readmission Risk</p>
#             </div>
#             """, unsafe_allow_html=True)
# 
#         st.markdown("---")
# 
#         # Recommendations
#         col1, col2 = st.columns(2)
# 
#         with col1:
#             st.markdown("### 📋 Interpretation")
#             st.markdown(f"""
#             **Risk Score:** {risk_score:.1f}%
# 
#             Approximately **{int(risk_score)}** out of 100 similar patients
#             would be readmitted within 30 days.
#             """)
# 
#         with col2:
#             st.markdown("### 🎯 Recommended Actions")
# 
#             if risk_score >= 70:
#                 st.error("""
#                 **URGENT - High Priority:**
#                 - ✅ Schedule follow-up within 24-48 hours
#                 - ✅ Coordinate with case management
#                 - ✅ Ensure medication reconciliation
#                 - ✅ Assess home support needs
#                 - ✅ Consider transitional care program
#                 """)
#             elif risk_score >= 50:
#                 st.warning("""
#                 **MODERATE - Enhanced Follow-up:**
#                 - ✅ Schedule follow-up within 48-72 hours
#                 - ✅ Provide detailed discharge instructions
#                 - ✅ Verify medication understanding
#                 - ✅ Check access to follow-up care
#                 """)
#             elif risk_score >= 30:
#                 st.info("""
#                 **STANDARD - Routine Follow-up:**
#                 - ✅ Schedule follow-up within 7-14 days
#                 - ✅ Standard discharge education
#                 - ✅ Monitor for complications
#                 """)
#             else:
#                 st.success("""
#                 **LOW RISK - Standard Care:**
#                 - ✅ Routine discharge protocol
#                 - ✅ Standard follow-up timing
#                 - ✅ Patient education materials
#                 """)
# 
#         # Risk factors
#         st.markdown("---")
#         st.markdown("### 🔍 Key Risk Factors")
# 
#         risk_factors = []
#         if charlson_score >= 3:
#             risk_factors.append(f"🔴 High Comorbidity (Charlson: {charlson_score})")
#         if prior_admits >= 2:
#             risk_factors.append(f"🟠 Recent Hospitalizations ({prior_admits} in 6 months)")
#         if los_days > 7:
#             risk_factors.append(f"🟡 Extended Stay ({los_days} days)")
#         if had_icu:
#             risk_factors.append("🟡 ICU Admission")
#         if dx_heart_failure:
#             risk_factors.append("🔴 Heart Failure Diagnosis")
#         if dx_copd:
#             risk_factors.append("🟠 COPD Diagnosis")
#         if dx_diabetes:
#             risk_factors.append("🟡 Diabetes Diagnosis")
#         if dx_renal:
#             risk_factors.append("🟠 Renal Failure")
#         if discharge_weekend:
#             risk_factors.append("🟡 Weekend Discharge")
# 
#         if risk_factors:
#             for factor in risk_factors:
#                 st.markdown(f"- {factor}")
#         else:
#             st.markdown("- ℹ️ Standard risk profile")
# 
#         # Export functionality
#         st.markdown("---")
#         if st.button("📥 Export Prediction Report"):
#             report = f"""
# TRANCE Readmission Risk Prediction Report
# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# 
# PATIENT INFORMATION:
# - Age: {age}
# - Charlson Comorbidity Score: {charlson_score}
# - Length of Stay: {los_days} days
# - Prior Admissions (6 months): {prior_admits}
# 
# DIAGNOSES:
# - Heart Failure: {'Yes' if dx_heart_failure else 'No'}
# - COPD: {'Yes' if dx_copd else 'No'}
# - Diabetes: {'Yes' if dx_diabetes else 'No'}
# - Renal Failure: {'Yes' if dx_renal else 'No'}
# 
# HOSPITAL COURSE:
# - ICU Stay: {'Yes' if had_icu else 'No'}
# - Weekend Discharge: {'Yes' if discharge_weekend else 'No'}
# 
# PREDICTION RESULT:
# - Risk Score: {risk_score:.1f}%
# - Risk Level: {risk_label}
# - Model: TRANCE Fused Model (Structured + Clinical Text)
# 
# RISK FACTORS:
# {chr(10).join(['- ' + rf for rf in risk_factors])}
# 
# RECOMMENDED ACTIONS:
# - Follow-up timing based on risk level
# - Case management coordination as indicated
# - Medication reconciliation and education
# - Home support assessment if needed
# 
# This prediction is for clinical decision support only and should be used
# in conjunction with clinical judgment.
#             """
# 
#             st.download_button(
#                 label="Download Report (TXT)",
#                 data=report,
#                 file_name=f"readmission_prediction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
#                 mime="text/plain"
#             )
# 
# # Footer
# st.markdown("---")
# st.markdown(
#     f"""
#     <div style='text-align: center; color: gray; padding: 1rem;'>
#     TRANCE v1.0.0 | Temporal Readmission Analysis with Neural Clinical Embeddings<br>
#     Last Updated: {datetime.now().strftime('%Y-%m-%d')} |
#     Test Set: {len(predictions)} patients |
#     Model AUROC: {results_summary['fused_model']['test_auroc']:.3f}<br>
#     For research and educational purposes only
#     </div>
#     """,
#     unsafe_allow_html=True
# )

# Step 4: Run Streamlit app in background
!streamlit run app.py --server.port 8501 &>/dev/null&